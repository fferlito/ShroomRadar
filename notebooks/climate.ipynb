{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generating file structure from CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5403/5403 [00:00<00:00, 200883.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Found 2744 unique observation dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building file list: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2744/2744 [00:00<00:00, 22422.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generated 140032 unique file entries\n",
      "‚úÖ File structure saved to: climate_files_from_csv.txt\n",
      "Step 2: Downloading climate data files...\n",
      "‚ñ∂Ô∏è Running command: ..//shroomradar//data//rclone.exe sync -v --filter-from climate_files_from_csv.txt --drive-shared-with-me google:/MSWX_V100 ..//climate_data\n"
     ]
    }
   ],
   "source": [
    "from shroomradar.src.climate import *\n",
    "\n",
    "csv_file_path = \"..//data/negative_samples_within_land_10k_with_coords_topography.csv\"\n",
    "\n",
    "download_climate_data_from_csv(csv_file_path,\n",
    "                               rclone_path=\"..//shroomradar//data//rclone.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append to cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3326 rows from ..//data//negative_samples_within_land_10k_with_coords_topography.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing observations:   1%|          | 30/3326 [00:00<00:11, 297.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preview row 0 ---\n",
      "P [nan, nan, nan, nan, nan] ...\n",
      "Pres [nan, nan, nan, nan, nan] ...\n",
      "RelHum [nan, nan, nan, nan, nan] ...\n",
      "SpecHum [nan, nan, nan, nan, nan] ...\n",
      "Temp [nan, nan, nan, nan, nan] ...\n",
      "Tmax [nan, nan, nan, nan, nan] ...\n",
      "Tmin [nan, nan, nan, nan, nan] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing observations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3326/3326 [00:07<00:00, 431.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data has been updated and saved to ..//data//negative_samples_within_land_10k_with_coords_topography_climate.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm   \n",
    "\n",
    "\n",
    "# --- Main pipeline ---\n",
    "# Read the CSV\n",
    "input_csv = \"..//data//negative_samples_within_land_10k_with_coords_topography.csv\"\n",
    "output_csv = \"..//data//negative_samples_within_land_10k_with_coords_topography_climate.csv\"\n",
    "\n",
    "climate_base = \"..//climate_data\"  # root folder containing NRT/ and Past/\n",
    "\n",
    "data = []\n",
    "with open(input_csv, \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if 'location' in row and row['location']:\n",
    "            data.append(row)\n",
    "\n",
    "print(f\"Loaded {len(data)} rows from {input_csv}\")\n",
    "\n",
    "variables = ['P', 'Pres', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "\n",
    "for idx, row in enumerate(tqdm(data, desc=\"Processing observations\")):\n",
    "    coords = extract_coordinates(row['location'])\n",
    "    if coords is None:\n",
    "        continue\n",
    "\n",
    "    obs_date = parse_datetime_with_timezone(row['observed_on'])\n",
    "    if obs_date is None:\n",
    "        continue  \n",
    "\n",
    "    for variable in variables:\n",
    "        env_data = get_environmental_data(coords, obs_date, climate_base, variable)\n",
    "        for i, val in enumerate(env_data, start=1):\n",
    "            row[f\"{variable}_{i}\"] = val\n",
    "\n",
    "    if idx == 0:\n",
    "        print(f\"--- Preview row {idx} ---\")\n",
    "        for variable in variables:\n",
    "            print(variable, [row[f\"{variable}_{i}\"] for i in range(1, 6)], \"...\")\n",
    "\n",
    "\n",
    "# Save enriched CSV\n",
    "fieldnames = list(data[0].keys())\n",
    "with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"‚úÖ Data has been updated and saved to\", output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append to geojson values (to be used in pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def sample_nc_point(file_path: str, lon: float, lat: float) -> float:\n",
    "    \"\"\"\n",
    "    Read a single NetCDF file and return the value at (lat, lon),\n",
    "    using xarray's CF decoding and spatial interpolation.\n",
    "\n",
    "    Returns float(np.nan) if the value cannot be obtained.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        return float('nan')\n",
    "\n",
    "    try:\n",
    "        # decode_cf=True applies scale_factor/add_offset and masks _FillValue/missing_value\n",
    "        with xr.open_dataset(file_path, engine=\"netcdf4\", decode_cf=True) as ds:\n",
    "            # pick the first real data variable (skip common coord names)\n",
    "            data_vars = list(ds.data_vars)\n",
    "            var_candidates = [v for v in data_vars\n",
    "                              if v.lower() not in (\"lat\", \"latitude\", \"lon\", \"longitude\", \"time\")]\n",
    "            if not var_candidates:\n",
    "                return float('nan')\n",
    "            vname = var_candidates[0]\n",
    "\n",
    "            da = ds[vname]\n",
    "\n",
    "            # Standardize spatial dim names to ('lat','lon') if needed\n",
    "            rename_map = {}\n",
    "            if \"latitude\" in da.dims: rename_map[\"latitude\"] = \"lat\"\n",
    "            if \"longitude\" in da.dims: rename_map[\"longitude\"] = \"lon\"\n",
    "            if rename_map:\n",
    "                da = da.rename(rename_map)\n",
    "\n",
    "            # If there is a time dimension, use the first (these are daily files)\n",
    "            if \"time\" in da.dims:\n",
    "                da = da.isel(time=0)\n",
    "\n",
    "            # Ensure lat/lon dims exist\n",
    "            if not ((\"lat\" in da.dims) and (\"lon\" in da.dims)):\n",
    "                return float('nan')\n",
    "\n",
    "            # Spatial interpolation (linear) with nearest fallback at edges\n",
    "            val = da.interp(lat=lat, lon=lon, method=\"linear\").values.item()\n",
    "            if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "                val = da.interp(lat=lat, lon=lon, method=\"nearest\").values.item()\n",
    "\n",
    "            # Final guard\n",
    "            if val is None:\n",
    "                return float('nan')\n",
    "            return float(val)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading/interpolating {file_path}: {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "\n",
    "def get_environmental_timeseries(polygon, date, data_dir, variables, num_days):\n",
    "    \"\"\"\n",
    "    For each variable, build a vector of daily values at the polygon centroid\n",
    "    using spatial interpolation (xarray.interp), then temporally interpolate\n",
    "    missing days (pandas), and return today's-first ordering.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    cx, cy = polygon.centroid.x, polygon.centroid.y  # lon, lat\n",
    "\n",
    "    for variable in variables:\n",
    "        values = []\n",
    "        for i in range(num_days):\n",
    "            current_date = date - timedelta(days=i)\n",
    "            file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "            data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "\n",
    "            val = sample_nc_point(data_file, cx, cy)\n",
    "            values.append(val)\n",
    "\n",
    "        # Temporal interpolation (fill gaps) while keeping Var_1 = \"today\"\n",
    "        s = pd.Series(values[::-1])  # oldest‚Üínewest for interpolation\n",
    "        s = s.interpolate(limit_direction=\"both\")\n",
    "        results[variable] = s[::-1].tolist()  # back to newest‚Üíoldest (today first)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# === Main script ===\n",
    "spain = gpd.read_file(\"crop.geojson\")\n",
    "# Ensure CRS is WGS84 lon/lat\n",
    "if spain.crs is None:\n",
    "    spain.set_crs(\"EPSG:4326\", inplace=True)\n",
    "else:\n",
    "    spain = spain.to_crs(\"EPSG:4326\")\n",
    "\n",
    "data_dir = \"new_data/NRT\"\n",
    "variables = [\"P\", \"Pres\", \"RelHum\", \"SpecHum\", \"Temp\", \"Tmax\", \"Tmin\"]\n",
    "num_days = 14\n",
    "test_date = datetime(2025, 9, 10)\n",
    "\n",
    "results = []\n",
    "for idx, row in tqdm(spain.iterrows(), total=len(spain), desc=\"Processing polygons\"):\n",
    "    ts_data = get_environmental_timeseries(row[\"geometry\"], test_date, data_dir, variables, num_days)\n",
    "    results.append((row.name, ts_data))\n",
    "\n",
    "# Merge back into GeoDataFrame\n",
    "for idx, ts_data in results:\n",
    "    for variable, values in ts_data.items():\n",
    "        # per-day values (Var_1 = today)\n",
    "        for day_index, value in enumerate(values, start=1):\n",
    "            spain.loc[idx, f\"{variable}_{day_index}\"] = value\n",
    "        # mean across the period\n",
    "        spain.loc[idx, f\"{variable}_mean\"] = float(np.nanmean(values))\n",
    "\n",
    "    # quick preview for first polygon\n",
    "    if idx == 0:\n",
    "        print(f\"--- Preview for polygon {idx} ---\")\n",
    "        for variable, values in ts_data.items():\n",
    "            print(variable, values[:5], \"...\")\n",
    "\n",
    "# Save output\n",
    "output_file = \"crop_new.geojson\"\n",
    "spain.to_file(output_file, driver=\"GeoJSON\")\n",
    "print(\"‚úÖ Data has been updated and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import rasterio\n",
    "from rasterio import mask\n",
    "\n",
    "# Function to calculate the average value of environmental data within a polygon area\n",
    "def get_average_environmental_data(polygon, date, data_dir, variable):\n",
    "    # Define the number of days to consider for the average (including today)\n",
    "    num_days = 5\n",
    "\n",
    "    # Initialize a list to store the environmental data values\n",
    "    data_values = []\n",
    "\n",
    "    # Iterate over the number of days to consider\n",
    "    for i in range(num_days):\n",
    "        # Calculate the date for the current iteration\n",
    "        current_date = date - timedelta(days=i)\n",
    "\n",
    "        # Construct the filename for the current date\n",
    "        file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "        data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "\n",
    "        # Check if the data file exists\n",
    "        if not os.path.isfile(data_file):\n",
    "            print(f\"File not found for {variable} on {file_date_str}\")\n",
    "            data_values.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Read environmental data from the file\n",
    "        nc = Dataset(data_file, 'r')\n",
    "\n",
    "        # Extract environmental data variable\n",
    "        var = None\n",
    "        for var_name in nc.variables.keys():\n",
    "            if var_name not in ['lon', 'lat', 'time']:\n",
    "                var = nc.variables[var_name]\n",
    "                break\n",
    "\n",
    "        if var is None:\n",
    "            print(f\"No environmental data variable found in {data_file}\")\n",
    "            data_values.append(np.nan)\n",
    "            nc.close()\n",
    "            continue  # Skip this variable and move to the next one\n",
    "\n",
    "        # Extract bounding box of the polygon\n",
    "        min_lon, min_lat, max_lon, max_lat = polygon.bounds\n",
    "\n",
    "        # Read the raster data and clip it to the polygon extent\n",
    "        with rasterio.open(data_file) as src:\n",
    "            out_image, out_transform = mask.mask(src, [polygon], crop=True)\n",
    "            out_image = np.squeeze(out_image)  # Remove singleton dimension\n",
    "\n",
    "            # Calculate the mean value for the current date and append to the list\n",
    "            data_values.append(np.nanmean(out_image))\n",
    "\n",
    "        # Close the netCDF file\n",
    "        nc.close()\n",
    "\n",
    "    # Calculate the average value\n",
    "    average_value = np.nanmean(data_values)\n",
    "\n",
    "    return average_value\n",
    "\n",
    "# Load the GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('spain_3km_ready.geojson')\n",
    "spain = spain.to_crs('4623')\n",
    "# Define the directory containing the data files\n",
    "data_dir = \"new_data/NRT\"\n",
    "\n",
    "# Define the environmental variables you want to include\n",
    "variables = ['P', 'Pres', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "\n",
    "# Create a list to store the updated data\n",
    "updated_data = []\n",
    "\n",
    "# Define a fixed date for testing\n",
    "test_date = datetime(2024, 5, 3)  # Replace with the desired date\n",
    "\n",
    "# Iterate over each polygon in the GeoDataFrame\n",
    "for index, row in spain.iterrows():\n",
    "    # Iterate over each variable\n",
    "    for variable in variables:\n",
    "        # Calculate the average environmental data for the current polygon, date, and variable\n",
    "        average_data = get_average_environmental_data(row['geometry'], test_date, data_dir, variable)\n",
    "\n",
    "        # Add the average environmental data to the row\n",
    "        row[f'{variable}_avg'] = average_data\n",
    "\n",
    "    # Append the updated row to the list\n",
    "    updated_data.append(row)\n",
    "\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_file = \"today_ready.csv\"\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=updated_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(updated_data)\n",
    "\n",
    "print(\"Data has been updated and saved to\", output_file)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
