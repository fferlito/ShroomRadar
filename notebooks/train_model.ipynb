{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(\"data/inaturalist_boletus_edulis_with_el_aspect_corine_weather.csv\")\n",
    "data2 = pd.read_csv(\"data/negative_samples_el_aspect_corine_weather.csv\")\n",
    "data2[\"species\"] = \"None\"\n",
    "\n",
    "del data[\"Unnamed: 0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([data, data2], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data, data2], sort=False)\n",
    "# Remove rows with any NaN values to avoid issues during model training\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "data = df\n",
    "\n",
    "# Selecting relevant features and target variable\n",
    "X = data[\n",
    "    ['tmin_P1', 'tmin_P2', 'tmin_P3', 'tmin_P4', 'tmin_P5', 'tmin_P6', 'tmin_P7', 'tmin_P8', 'tmin_P9', 'tmin_P10', 'tmin_P11', 'tmin_P12', 'tmin_P13', 'tmin_P14', 'tmin_P15',\n",
    "     'tmax_P1', 'tmax_P2', 'tmax_P3', 'tmax_P4', 'tmax_P5', 'tmax_P6', 'tmax_P7', 'tmax_P8', 'tmax_P9', 'tmax_P10', 'tmax_P11', 'tmax_P12', 'tmax_P13', 'tmax_P14', 'tmax_P15',\n",
    "     'temp_P1', 'temp_P2', 'temp_P3', 'temp_P4', 'temp_P5', 'temp_P6', 'temp_P7', 'temp_P8', 'temp_P9', 'temp_P10', 'temp_P11', 'temp_P12', 'temp_P13', 'temp_P14', 'temp_P15',\n",
    "     'rel_humidity_P1', 'rel_humidity_P2', 'rel_humidity_P3', 'rel_humidity_P4', 'rel_humidity_P5', 'rel_humidity_P6', 'rel_humidity_P7', 'rel_humidity_P8', 'rel_humidity_P9', 'rel_humidity_P10', 'rel_humidity_P11', 'rel_humidity_P12', 'rel_humidity_P13', 'rel_humidity_P14', 'rel_humidity_P15',\n",
    "     'precipitation_P1', 'precipitation_P2', 'precipitation_P3', 'precipitation_P4', 'precipitation_P5', 'precipitation_P6', 'precipitation_P7', 'precipitation_P8', 'precipitation_P9', 'precipitation_P10', 'precipitation_P11', 'precipitation_P12', 'precipitation_P13', 'precipitation_P14', 'precipitation_P15',\n",
    "     'wind_speed_P1', 'wind_speed_P2', 'wind_speed_P3', 'wind_speed_P4', 'wind_speed_P5', 'wind_speed_P6', 'wind_speed_P7', 'wind_speed_P8', 'wind_speed_P9', 'wind_speed_P10', 'wind_speed_P11', 'wind_speed_P12', 'wind_speed_P13', 'wind_speed_P14', 'wind_speed_P15',\n",
    "     'LC', 'elevation', 'aspect'\n",
    "    ]\n",
    "]\n",
    "y = data['species']\n",
    "\n",
    "# Balance the classes in the dataset by downsampling the majority class\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Combine X and y for easier resampling\n",
    "df_combined = X.copy()\n",
    "df_combined['species'] = y\n",
    "\n",
    "# Find the class counts\n",
    "class_counts = df_combined['species'].value_counts()\n",
    "min_class_count = class_counts.min()\n",
    "\n",
    "# Downsample each class to the size of the smallest class\n",
    "balanced_df = pd.concat([\n",
    "    resample(\n",
    "        df_combined[df_combined['species'] == label],\n",
    "        replace=False,\n",
    "        n_samples=min_class_count,\n",
    "        random_state=43\n",
    "    )\n",
    "    for label in class_counts.index\n",
    "])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=43).reset_index(drop=True)\n",
    "\n",
    "# Separate features and target again\n",
    "X_balanced = balanced_df.drop('species', axis=1)\n",
    "y_balanced = balanced_df['species']\n",
    "\n",
    "# Split the balanced data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.15, random_state=43, stratify=y_balanced)\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=43)\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(gb_clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_gb_clf = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_gb_clf.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = df\n",
    "\n",
    "required_variables = ['tmin_P1', 'tmin_P2', 'tmin_P3', 'tmin_P4', 'tmin_P5', 'tmin_P6', 'tmin_P7', 'tmin_P8', 'tmin_P9', 'tmin_P10', 'tmin_P11', 'tmin_P12', 'tmin_P13', 'tmin_P14',\n",
    "     'tmax_P1', 'tmax_P2', 'tmax_P3', 'tmax_P4', 'tmax_P5', 'tmax_P6', 'tmax_P7', 'tmax_P8', 'tmax_P9', 'tmax_P10', 'tmax_P11', 'tmax_P12', 'tmax_P13', 'tmax_P14',\n",
    "     'temp_P1', 'temp_P2', 'temp_P3', 'temp_P4', 'temp_P5', 'temp_P6', 'temp_P7', 'temp_P8', 'temp_P9', 'temp_P10', 'temp_P11', 'temp_P12', 'temp_P13', 'temp_P14', \n",
    "     'rel_humidity_P1', 'rel_humidity_P2', 'rel_humidity_P3', 'rel_humidity_P4', 'rel_humidity_P5', 'rel_humidity_P6', 'rel_humidity_P7', 'rel_humidity_P8', 'rel_humidity_P9', 'rel_humidity_P10', 'rel_humidity_P11', 'rel_humidity_P12', 'rel_humidity_P13', 'rel_humidity_P14',\n",
    "     'precipitation_P1', 'precipitation_P2', 'precipitation_P3', 'precipitation_P4', 'precipitation_P5', 'precipitation_P6', 'precipitation_P7', 'precipitation_P8', 'precipitation_P9', 'precipitation_P10', 'precipitation_P11', 'precipitation_P12', 'precipitation_P13', 'precipitation_P14', \n",
    "     'wind_speed_P1', 'wind_speed_P2', 'wind_speed_P3', 'wind_speed_P4', 'wind_speed_P5', 'wind_speed_P6', 'wind_speed_P7', 'wind_speed_P8', 'wind_speed_P9', 'wind_speed_P10', 'wind_speed_P11', 'wind_speed_P12', 'wind_speed_P13', 'wind_speed_P14', \n",
    "    'elevation', 'aspect'\n",
    "    ]\n",
    "# Selecting relevant features and target variable\n",
    "X = data[\n",
    "    required_variables\n",
    "]\n",
    "\n",
    "# Split the balanced data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=45)\n",
    "# Split the data into train and test sets\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=3, random_state=43)\n",
    "\n",
    "# Train the model\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(gb_clf, 'docker//data//models//gradient_boosting_model_v5.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logist regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "data = df   # make sure df is already defined\n",
    "target_col = \"species\"   # your target column\n",
    "\n",
    "# List of features\n",
    "required_variables = [\n",
    "    'tmin_P1','tmin_P2','tmin_P3','tmin_P4','tmin_P5','tmin_P6','tmin_P7','tmin_P8','tmin_P9','tmin_P10','tmin_P11','tmin_P12','tmin_P13','tmin_P14',\n",
    "    'tmax_P1','tmax_P2','tmax_P3','tmax_P4','tmax_P5','tmax_P6','tmax_P7','tmax_P8','tmax_P9','tmax_P10','tmax_P11','tmax_P12','tmax_P13','tmax_P14',\n",
    "    'temp_P1','temp_P2','temp_P3','temp_P4','temp_P5','temp_P6','temp_P7','temp_P8','temp_P9','temp_P10','temp_P11','temp_P12','temp_P13','temp_P14',\n",
    "    'rel_humidity_P1','rel_humidity_P2','rel_humidity_P3','rel_humidity_P4','rel_humidity_P5','rel_humidity_P6','rel_humidity_P7','rel_humidity_P8','rel_humidity_P9','rel_humidity_P10','rel_humidity_P11','rel_humidity_P12','rel_humidity_P13','rel_humidity_P14',\n",
    "    'precipitation_P1','precipitation_P2','precipitation_P3','precipitation_P4','precipitation_P5','precipitation_P6','precipitation_P7','precipitation_P8','precipitation_P9','precipitation_P10','precipitation_P11','precipitation_P12','precipitation_P13','precipitation_P14',\n",
    "    'wind_speed_P1','wind_speed_P2','wind_speed_P3','wind_speed_P4','wind_speed_P5','wind_speed_P6','wind_speed_P7','wind_speed_P8','wind_speed_P9','wind_speed_P10','wind_speed_P11','wind_speed_P12','wind_speed_P13','wind_speed_P14',\n",
    "    'elevation','aspect'\n",
    "]\n",
    "\n",
    "# Extract features\n",
    "X = data[required_variables]\n",
    "\n",
    "# --- Step 1: Encode categorical features (None -> 0, others -> 1) ---\n",
    "categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].apply(lambda x: 0 if pd.isnull(x) or x == \"None\" else 1)\n",
    "\n",
    "# --- Step 2: Encode target (None=0, rest=1) ---\n",
    "y = data[target_col].apply(lambda x: 0 if x == \"None\" else 1)\n",
    "\n",
    "# --- Step 3: Train-test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=45, stratify=y\n",
    ")\n",
    "\n",
    "# --- Step 4: Oversample minority with SMOTE ---\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# --- Step 5: Logistic Regression ---\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_res, y_res)\n",
    "\n",
    "\n",
    "# --- Step 6: Predictions ---\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Step 7: Evaluation ---\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"None\",\"Other\"]))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(log_reg, 'docker//data//models//lr_model_v5.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# ----------------------\n",
    "# Load and prepare data\n",
    "# ----------------------\n",
    "data = df   # make sure df is already defined\n",
    "target_col = \"species\"   # <-- your target column\n",
    "\n",
    "required_variables = [\n",
    "    'tmin_P1','tmin_P2','tmin_P3','tmin_P4','tmin_P5','tmin_P6','tmin_P7','tmin_P8','tmin_P9','tmin_P10','tmin_P11','tmin_P12','tmin_P13','tmin_P14',\n",
    "    'tmax_P1','tmax_P2','tmax_P3','tmax_P4','tmax_P5','tmax_P6','tmax_P7','tmax_P8','tmax_P9','tmax_P10','tmax_P11','tmax_P12','tmax_P13','tmax_P14',\n",
    "    'temp_P1','temp_P2','temp_P3','temp_P4','temp_P5','temp_P6','temp_P7','temp_P8','temp_P9','temp_P10','temp_P11','temp_P12','temp_P13','temp_P14',\n",
    "    'rel_humidity_P1','rel_humidity_P2','rel_humidity_P3','rel_humidity_P4','rel_humidity_P5','rel_humidity_P6','rel_humidity_P7','rel_humidity_P8','rel_humidity_P9','rel_humidity_P10','rel_humidity_P11','rel_humidity_P12','rel_humidity_P13','rel_humidity_P14',\n",
    "    'precipitation_P1','precipitation_P2','precipitation_P3','precipitation_P4','precipitation_P5','precipitation_P6','precipitation_P7','precipitation_P8','precipitation_P9','precipitation_P10','precipitation_P11','precipitation_P12','precipitation_P13','precipitation_P14',\n",
    "    'wind_speed_P1','wind_speed_P2','wind_speed_P3','wind_speed_P4','wind_speed_P5','wind_speed_P6','wind_speed_P7','wind_speed_P8','wind_speed_P9','wind_speed_P10','wind_speed_P11','wind_speed_P12','wind_speed_P13','wind_speed_P14',\n",
    "    'elevation','aspect'\n",
    "]\n",
    "\n",
    "# Features\n",
    "X = data[required_variables].copy()\n",
    "\n",
    "# Encode categorical features: None/NaN -> 0, else -> 1\n",
    "categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].apply(lambda x: 0 if pd.isnull(x) or x == \"None\" else 1)\n",
    "\n",
    "# Encode target: None=0, all other species=1\n",
    "y = data[target_col].apply(lambda x: 0 if x == \"None\" else 1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=45, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# Option 1: Logistic Regression with class_weight balanced\n",
    "# ----------------------\n",
    "log_reg_balanced = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "log_reg_balanced.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = log_reg_balanced.predict(X_test)\n",
    "y_prob1 = log_reg_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Option 1: Logistic Regression (class_weight balanced) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred1))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred1, target_names=[\"None\",\"Other\"]))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob1))\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Option 3: Logistic Regression + Probability Calibration\n",
    "# ----------------------\n",
    "log_reg_base = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "calibrated_clf = CalibratedClassifierCV(log_reg_base, method='isotonic', cv=5)\n",
    "calibrated_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = calibrated_clf.predict(X_test)\n",
    "y_prob3 = calibrated_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== Option 3: Logistic Regression (Calibrated) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred3))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred3, target_names=[\"None\",\"Other\"]))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "gb_clf = gb_clf\n",
    "\n",
    "# Load the GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('updated_spain.geojson')\n",
    "spain = spain.to_crs('EPSG:4326')  # Convert CRS if needed\n",
    "spain.rename(columns={'mode_value': 'LC', 'mean_elevation': 'elevation'}, inplace=True)\n",
    "\n",
    "# Define the directory containing the data files\n",
    "data_dir = \"MSWX_V100/Past\"\n",
    "\n",
    "# Define the list of variables required for prediction\n",
    "required_variables = ['Pres_1', 'Pres_2', 'Pres_3', 'Pres_4', 'Pres_5', 'Pres_6', 'Pres_7', 'Pres_8', 'Pres_9', 'Pres_10',\n",
    "                      'Pres_11', 'Pres_12', 'Pres_13', 'Pres_14', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5', 'P_6', 'P_7', 'P_8',\n",
    "                      'P_9', 'P_10', 'P_11', 'P_12', 'P_13', 'P_14', 'RelHum_1', 'RelHum_2', 'RelHum_3', 'RelHum_4',\n",
    "                      'RelHum_5', 'RelHum_6', 'RelHum_7', 'RelHum_8', 'RelHum_9', 'RelHum_10', 'RelHum_11', 'RelHum_12',\n",
    "                      'RelHum_13', 'RelHum_14', 'SpecHum_1', 'SpecHum_2', 'SpecHum_3', 'SpecHum_4', 'SpecHum_5',\n",
    "                      'SpecHum_6', 'SpecHum_7', 'SpecHum_8', 'SpecHum_9', 'SpecHum_10', 'SpecHum_11', 'SpecHum_12',\n",
    "                      'SpecHum_13', 'SpecHum_14', 'Temp_1', 'Temp_2', 'Temp_3', 'Temp_4', 'Temp_5', 'Temp_6', 'Temp_7',\n",
    "                      'Temp_8', 'Temp_9', 'Temp_10', 'Temp_11', 'Temp_12', 'Temp_13', 'Temp_14', 'Tmax_1', 'Tmax_2',\n",
    "                      'Tmax_3', 'Tmax_4', 'Tmax_5', 'Tmax_6', 'Tmax_7', 'Tmax_8', 'Tmax_9', 'Tmax_10', 'Tmax_11',\n",
    "                      'Tmax_12', 'Tmax_13', 'Tmax_14', 'Tmin_1', 'Tmin_2', 'Tmin_3', 'Tmin_4', 'Tmin_5', 'Tmin_6',\n",
    "                      'Tmin_7', 'Tmin_8', 'Tmin_9', 'Tmin_10', 'Tmin_11', 'Tmin_12', 'Tmin_13', 'Tmin_14', 'LC',\n",
    "                      'elevation']\n",
    "\n",
    "# Initialize an empty DataFrame to store predictions\n",
    "predictions_df = pd.DataFrame(columns=['geometry', 'species_prediction'])\n",
    "\n",
    "# Iterate over each polygon in the GeoDataFrame\n",
    "for index, row in spain.iterrows():\n",
    "    # Extract the required variables for prediction\n",
    "    variables_for_prediction = row[required_variables]\n",
    "\n",
    "\n",
    "    # Reshape the variables for prediction into a single-row DataFrame\n",
    "    variables_for_prediction_df = pd.DataFrame(variables_for_prediction).transpose()\n",
    "    \n",
    "\n",
    "\n",
    "    # Make the prediction\n",
    "    predicted_species = gb_clf.predict_proba(variables_for_prediction_df)\n",
    "    \n",
    "    print(\"Predicted species:\")\n",
    "    print(predicted_species[0][0])\n",
    "\n",
    "    # Append the prediction to the DataFrame\n",
    "    predictions_df = pd.concat([predictions_df, pd.DataFrame({'geometry': row['geometry'], 'species_prediction': predicted_species[0][0]}, index=[0])], ignore_index=True)\n",
    "\n",
    "# Merge the predictions with the original GeoDataFrame\n",
    "spain_with_predictions = pd.concat([spain, predictions_df['species_prediction']], axis=1)\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain_with_predictions.to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure you have a sample of your data to explain\n",
    "# We'll use the first 100 rows for speed, but you can adjust as needed\n",
    "X_sample = df[required_variables].iloc[:100]\n",
    "\n",
    "# Create a SHAP explainer for the trained model\n",
    "explainer = shap.Explainer(gb_clf, X_sample)\n",
    "\n",
    "# Calculate SHAP values for the sample\n",
    "shap_values = explainer(X_sample)\n",
    "\n",
    "# Plot a summary SHAP graph (beeswarm plot)\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"dot\", show=True)\n",
    "\n",
    "# Optionally, plot a bar chart of mean absolute SHAP values (feature importance)\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = gb_clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(feature_importance_df[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load your data\n",
    "data = df\n",
    "\n",
    "# Extract static features\n",
    "X_static = data[['LC', 'elevation']]  # Add other static variables here\n",
    "\n",
    "# Reshape the data\n",
    "temporal_vars = ['P', 'Pres', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "ordered_data = pd.concat([pd.concat([data[[f\"{var}_{i+1}\"]] for i in range(14)], axis=1).stack().reset_index(level=1, drop=True) for var in temporal_vars], axis=1)\n",
    "\n",
    "# Merge temporal and static features\n",
    "X_temporal = ordered_data.values\n",
    "X_static_repeated = np.repeat(X_static.values, 14, axis=0)\n",
    "X = np.concatenate([X_temporal, X_static_repeated], axis=1)\n",
    "\n",
    "# Reshape X to have the same number of samples as y\n",
    "X = X[:525]\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['species'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train[:, :-2].shape)\n",
    "\n",
    "# Reshape input data for LSTM (samples, timesteps, features)\n",
    "X_train_temporal = np.reshape(X_train[:, :-2], (X_train.shape[0], 14, 7))\n",
    "X_train_static = np.tile(X_train[:, -2:], (1, 14, 1))\n",
    "\n",
    "# Similarly, reshape the test data\n",
    "X_test_temporal = np.reshape(X_test[:, :-2], (X_test.shape[0], 14, -1))\n",
    "X_test_static = np.tile(X_test[:, -2:], (1, 14, 1))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(14, X_train.shape[1] // 14)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_temporal, X_train_static], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict([X_test_temporal, X_test_static])\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "\n",
    "def get_environmental_data(polygon, date, data_dir, variables, num_days):\n",
    "    data = {}\n",
    "    for variable in variables:\n",
    "        variable_values = []\n",
    "        for i in range(num_days):\n",
    "            current_date = date - timedelta(days=i)\n",
    "            file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "            data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "            if not os.path.isfile(data_file):\n",
    "                print(f\"File not found for {variable} on {file_date_str}\")\n",
    "                variable_values.append(np.nan)\n",
    "                continue\n",
    "            with rasterio.open(data_file, mode=\"r\") as src:\n",
    "                min_x, min_y, max_x, max_y = polygon.bounds\n",
    "                centroid_x = (min_x + max_x) / 2\n",
    "                centroid_y = (min_y + max_y) / 2\n",
    "                centroid = Point(centroid_x, centroid_y)\n",
    "                px, py = src.index(centroid.x, centroid.y)\n",
    "                value = src.read(1, window=((py, py+1), (px, px+1)))\n",
    "                variable_values.append(value[0, 0])\n",
    "        data[variable] = variable_values[::-1]  # Reverse the list to align with the order needed (recent to past)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_row(row_tuple):\n",
    "    index, row = row_tuple\n",
    "    average_data = get_environmental_data(row['geometry'], test_date, data_dir, variables, num_days)\n",
    "    variables_for_prediction = {}\n",
    "    for variable in variables:\n",
    "        for day_number, value in enumerate(average_data[variable], start=1):\n",
    "            variable_name = f\"{variable}_{day_number}\"\n",
    "            variables_for_prediction[variable_name] = value\n",
    "    variables_for_prediction['LC'] = row['mode_value']\n",
    "    variables_for_prediction['elevation'] = row['mean_elevation']\n",
    "    df = pd.DataFrame(variables_for_prediction, index=[0])\n",
    "    # Display all rows and columns without truncation\n",
    "    #with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #    print(df)    \n",
    "    predicted_species = gb_clf.predict_proba(df)[0][0]  # Make prediction\n",
    "    return predicted_species\n",
    "\n",
    "\n",
    "# Load GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('spain_3km_ready.geojson')\n",
    "spain = spain.to_crs('EPSG:4326')\n",
    "\n",
    "data_dir = \"new_data/NRT\"\n",
    "variables = ['Pres', 'P', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "num_days = 14\n",
    "test_date = datetime(2024, 5, 3)\n",
    "\n",
    "# Use ThreadPoolExecutor to run multiple threads\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    predictions = list(tqdm(executor.map(process_row, spain.iterrows()), total=len(spain), desc=\"Making predictions\"))\n",
    "\n",
    "# Add predictions to the GeoDataFrame\n",
    "spain['species_prediction'] = predictions\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain[['geometry', 'species_prediction']].to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def get_environmental_data(polygon, date, data_dir, variables, num_days):\n",
    "    data = {}\n",
    "    for variable in variables:\n",
    "        variable_values = []\n",
    "        for i in range(num_days):\n",
    "            current_date = date - timedelta(days=i)\n",
    "            file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "            data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "            if not os.path.isfile(data_file):\n",
    "                print(f\"File not found for {variable} on {file_date_str}\")\n",
    "                variable_values.append(np.nan)\n",
    "                continue\n",
    "            with rasterio.open(data_file, mode=\"r\") as src:\n",
    "                min_x, min_y, max_x, max_y = polygon.bounds\n",
    "                centroid_x = (min_x + max_x) / 2\n",
    "                centroid_y = (min_y + max_y) / 2\n",
    "                centroid = Point(centroid_x, centroid_y)\n",
    "                px, py = src.index(centroid.x, centroid.y)\n",
    "                value = src.read(1, window=((py, py+1), (px, px+1)))\n",
    "                variable_values.append(value[0, 0])\n",
    "        data[variable] = variable_values[::-1]  # Reverse the list to align with the order needed (recent to past)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_row(index, row):\n",
    "    average_data = get_environmental_data(row['geometry'], test_date, data_dir, variables, num_days)\n",
    "    variables_for_prediction = {}\n",
    "    for variable in variables:\n",
    "        for day_number, value in enumerate(average_data[variable], start=1):\n",
    "            variable_name = f\"{variable}_{day_number}\"\n",
    "            variables_for_prediction[variable_name] = value\n",
    "    variables_for_prediction['LC'] = row['mode_value']\n",
    "    variables_for_prediction['elevation'] = row['mean_elevation']\n",
    "    df = pd.DataFrame(variables_for_prediction, index=[0])\n",
    "    predicted_species = gb_clf.predict_proba(df)[0][0]  # Make prediction\n",
    "    return predicted_species\n",
    "\n",
    "# Load GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('spain_3km_ready.geojson')\n",
    "spain = spain.to_crs('EPSG:4326')\n",
    "\n",
    "data_dir = \"new_data/NRT\"\n",
    "variables = ['Pres', 'P', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "num_days = 14\n",
    "test_date = datetime(2024, 5, 3)\n",
    "\n",
    "# Sequentially process each row\n",
    "predictions = []\n",
    "for index, row in tqdm(spain.iterrows(), total=len(spain), desc=\"Making predictions\"):\n",
    "    prediction = process_row(index, row)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Add predictions to the GeoDataFrame\n",
    "spain['species_prediction'] = predictions\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain[['geometry', 'species_prediction']].to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def extract_data_from_raster(data_file, polygons):\n",
    "    values = []\n",
    "    if not os.path.isfile(data_file):\n",
    "        print(f\"File not found: {data_file}\")\n",
    "        return [np.nan] * len(polygons)\n",
    "    \n",
    "    with rasterio.open(data_file, mode=\"r\") as src:\n",
    "        for polygon in tqdm(polygons, desc=data_file):\n",
    "            min_x, min_y, max_x, max_y = polygon.bounds\n",
    "            centroid_x = (min_x + max_x) / 2\n",
    "            centroid_y = (min_y + max_y) / 2\n",
    "            centroid = Point(centroid_x, centroid_y)\n",
    "            px, py = src.index(centroid.x, centroid.y)\n",
    "            try:\n",
    "                value = src.read(1, window=((py, py+1), (px, px+1)))\n",
    "                values.append(value[0, 0])\n",
    "            except:\n",
    "                values.append(np.nan)\n",
    "    \n",
    "    return values\n",
    "\n",
    "def get_environmental_data(polygons, date, data_dir, variables, num_days):\n",
    "    data = {variable: [[] for _ in range(len(polygons))] for variable in variables}\n",
    "    \n",
    "    dates = [date - timedelta(days=i) for i in range(num_days)]\n",
    "    \n",
    "    for variable in tqdm(variables, desc=\"Variable\"):\n",
    "        print(f\"Processing variable: {variable}\")\n",
    "        for current_date in dates:\n",
    "            file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "            data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "            values = extract_data_from_raster(data_file, polygons)\n",
    "            for i, value in enumerate(values):\n",
    "                data[variable][i].append(value)\n",
    "    \n",
    "    for variable in variables:\n",
    "        for i in range(len(polygons)):\n",
    "            data[variable][i] = data[variable][i][::-1]  # Reverse the list to align with the order needed (recent to past)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepare_data_for_prediction(polygon_index, all_data, variables, num_days, row):\n",
    "    variables_for_prediction = {}\n",
    "    for variable in variables:\n",
    "        for day_number in range(num_days):\n",
    "            variable_name = f\"{variable}_{day_number+1}\"\n",
    "            variables_for_prediction[variable_name] = all_data[variable][polygon_index][day_number]\n",
    "    \n",
    "    variables_for_prediction['LC'] = row['mode_value']\n",
    "    variables_for_prediction['elevation'] = row['mean_elevation']\n",
    "    \n",
    "    return variables_for_prediction\n",
    "\n",
    "# Load GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('spain_3km_ready.geojson')\n",
    "spain = spain.to_crs('EPSG:4326')\n",
    "\n",
    "data_dir = \"new_data/NRT\"\n",
    "variables = ['Pres', 'P', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "num_days = 14\n",
    "test_date = datetime(2024, 5, 3)\n",
    "\n",
    "# Prepare polygon geometries\n",
    "polygon_geometries = spain['geometry']\n",
    "\n",
    "# Get environmental data for all polygons\n",
    "all_data = get_environmental_data(polygon_geometries, test_date, data_dir, variables, num_days)\n",
    "\n",
    "# Sequentially process each row for predictions\n",
    "predictions = []\n",
    "for index, row in tqdm(spain.iterrows(), total=len(spain), desc=\"Making predictions\"):\n",
    "    data_dict = prepare_data_for_prediction(index, all_data, variables, num_days, row)\n",
    "    df = pd.DataFrame(data_dict, index=[0])\n",
    "    predicted_species = gb_clf.predict_proba(df)[0][0]  # Make prediction\n",
    "    predictions.append(predicted_species)\n",
    "\n",
    "# Add predictions to the GeoDataFrame\n",
    "spain['species_prediction'] = predictions\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain[['geometry', 'species_prediction']].to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for all rows\n",
    "prepared_data = []\n",
    "for index, row in tqdm(spain.iterrows(), total=len(spain), desc=\"Preparing data\"):\n",
    "    data_dict = prepare_data_for_prediction(index, all_data, variables, num_days, row)\n",
    "    prepared_data.append(data_dict)\n",
    "\n",
    "# Concatenate all prepared data into a single DataFrame\n",
    "all_prepared_data = pd.DataFrame(prepared_data)\n",
    "\n",
    "# Fill NaN values with column means\n",
    "all_prepared_data.fillna(all_prepared_data.mean(), inplace=True)\n",
    "\n",
    "# Sequential processing for predictions\n",
    "predictions = []\n",
    "for index, row in tqdm(all_prepared_data.iterrows(), total=len(all_prepared_data), desc=\"Making predictions\"):\n",
    "    df = row.to_frame().T  # Convert the row to a DataFrame for prediction\n",
    "    predicted_species = gb_clf.predict_proba(df)[0][0]  # Make prediction\n",
    "    predictions.append(predicted_species)\n",
    "\n",
    "# Add predictions to the GeoDataFrame\n",
    "spain['species_prediction'] = predictions\n",
    "\n",
    "\n",
    "# Keep only 'geometry' and 'species_prediction' columns\n",
    "spain_smaller = spain[['geometry', 'species_prediction']]\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions_whole.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain_smaller.to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove polygons with a species_prediction value lower than 0.0001\n",
    "import geopandas as pd\n",
    "\n",
    "spain_smaller = pd.read_file(\"spain_with_species_predictions_whole.geojson\")\n",
    "threshold = 0.005\n",
    "initial_count = len(spain_smaller)\n",
    "spain_filtered = spain_smaller[spain_smaller['species_prediction'] >= threshold]\n",
    "removed_count = initial_count - len(spain_filtered)\n",
    "\n",
    "# Print the number of polygons removed\n",
    "print(f\"Number of polygons removed: {removed_count}\")\n",
    "print(f\"Number of polygons left: {len(spain_filtered)}\")\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions_smaller.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain_filtered.to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
