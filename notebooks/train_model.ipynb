{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(\"..//data//inaturalist_boletus_edulis_with_corine_climate_topography.csv\")\n",
    "data2 = pd.read_csv(\"..//data//negative_samples_within_land_10k_with_coords_topography_climate.csv\")\n",
    "data2[\"species\"] = \"None\"\n",
    "\n",
    "del data[\"Unnamed: 0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>location</th>\n",
       "      <th>observed_on</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>LC</th>\n",
       "      <th>P_1</th>\n",
       "      <th>P_2</th>\n",
       "      <th>P_3</th>\n",
       "      <th>P_4</th>\n",
       "      <th>...</th>\n",
       "      <th>dem</th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "      <th>geomorphon</th>\n",
       "      <th>dem_source</th>\n",
       "      <th>geomorphon_class</th>\n",
       "      <th>dem_failure</th>\n",
       "      <th>slope_failure</th>\n",
       "      <th>aspect_failure</th>\n",
       "      <th>geomorphon_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boletus edulis</td>\n",
       "      <td>(43.1119576925, 1.2274287587)</td>\n",
       "      <td>2022-12-09 11:31:00+01:00</td>\n",
       "      <td>43.111958</td>\n",
       "      <td>1.227429</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.966915</td>\n",
       "      <td>0.978948</td>\n",
       "      <td>1.146052</td>\n",
       "      <td>11.998103</td>\n",
       "      <td>...</td>\n",
       "      <td>461.0</td>\n",
       "      <td>11.997304</td>\n",
       "      <td>7.997421</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boletus edulis</td>\n",
       "      <td>(43.1121818071, 1.2249231333)</td>\n",
       "      <td>2022-12-09 11:02:00+01:00</td>\n",
       "      <td>43.112182</td>\n",
       "      <td>1.224923</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.960307</td>\n",
       "      <td>0.977968</td>\n",
       "      <td>1.147032</td>\n",
       "      <td>11.996561</td>\n",
       "      <td>...</td>\n",
       "      <td>469.0</td>\n",
       "      <td>6.478467</td>\n",
       "      <td>3.732119</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boletus edulis</td>\n",
       "      <td>(48.7929881052, 2.0979980752)</td>\n",
       "      <td>2023-11-18 16:37:00+01:00</td>\n",
       "      <td>48.792988</td>\n",
       "      <td>2.097998</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9.924997</td>\n",
       "      <td>3.664999</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>...</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2.357576</td>\n",
       "      <td>156.553619</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          species                       location                observed_on  \\\n",
       "0  Boletus edulis  (43.1119576925, 1.2274287587)  2022-12-09 11:31:00+01:00   \n",
       "1  Boletus edulis  (43.1121818071, 1.2249231333)  2022-12-09 11:02:00+01:00   \n",
       "2  Boletus edulis  (48.7929881052, 2.0979980752)  2023-11-18 16:37:00+01:00   \n",
       "\n",
       "           y         x    LC       P_1       P_2       P_3        P_4  ...  \\\n",
       "0  43.111958  1.227429  24.0  9.966915  0.978948  1.146052  11.998103  ...   \n",
       "1  43.112182  1.224923  24.0  9.960307  0.977968  1.147032  11.996561  ...   \n",
       "2  48.792988  2.097998  23.0  9.924997  3.664999  0.437500   0.747500  ...   \n",
       "\n",
       "     dem      slope      aspect  geomorphon  dem_source  geomorphon_class  \\\n",
       "0  461.0  11.997304    7.997421         7.0         NaN               NaN   \n",
       "1  469.0   6.478467    3.732119         7.0         NaN               NaN   \n",
       "2  183.0   2.357576  156.553619         6.0         NaN               NaN   \n",
       "\n",
       "   dem_failure  slope_failure  aspect_failure  geomorphon_failure  \n",
       "0          NaN            NaN             NaN                 NaN  \n",
       "1          NaN            NaN             NaN                 NaN  \n",
       "2          NaN            NaN             NaN                 NaN  \n",
       "\n",
       "[3 rows x 114 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([data, data2], sort=False)\n",
    "df.dropna(subset=['P_1', 'P_2', 'P_3', 'P_4', 'P_5', 'P_6', 'P_7', 'P_8', 'P_9', 'P_10', 'P_11', 'P_12', 'P_13', 'P_14',\n",
    "    'Pres_1', 'Pres_2', 'Pres_3', 'Pres_4', 'Pres_5', 'Pres_6', 'Pres_7', 'Pres_8', 'Pres_9', 'Pres_10', 'Pres_11', 'Pres_12', 'Pres_13', 'Pres_14',\n",
    "    'RelHum_1', 'RelHum_2', 'RelHum_3', 'RelHum_4', 'RelHum_5', 'RelHum_6', 'RelHum_7', 'RelHum_8', 'RelHum_9', 'RelHum_10', 'RelHum_11', 'RelHum_12', 'RelHum_13', 'RelHum_14',\n",
    "    'SpecHum_1', 'SpecHum_2', 'SpecHum_3', 'SpecHum_4', 'SpecHum_5', 'SpecHum_6', 'SpecHum_7', 'SpecHum_8', 'SpecHum_9', 'SpecHum_10', 'SpecHum_11', 'SpecHum_12', 'SpecHum_13', 'SpecHum_14',\n",
    "    'Temp_1', 'Temp_2', 'Temp_3', 'Temp_4', 'Temp_5', 'Temp_6', 'Temp_7', 'Temp_8', 'Temp_9', 'Temp_10', 'Temp_11', 'Temp_12', 'Temp_13', 'Temp_14',\n",
    "    'Tmax_1', 'Tmax_2', 'Tmax_3', 'Tmax_4', 'Tmax_5', 'Tmax_6', 'Tmax_7', 'Tmax_8', 'Tmax_9', 'Tmax_10', 'Tmax_11', 'Tmax_12', 'Tmax_13', 'Tmax_14',\n",
    "    'Tmin_1', 'Tmin_2', 'Tmin_3', 'Tmin_4', 'Tmin_5', 'Tmin_6', 'Tmin_7', 'Tmin_8', 'Tmin_9', 'Tmin_10', 'Tmin_11', 'Tmin_12', 'Tmin_13', 'Tmin_14',\n",
    "    'dem','slope'], inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([data, data2], sort=False)\n",
    "# Remove rows with any NaN values to avoid issues during model training\n",
    "#df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tmin_P1', 'tmin_P2', 'tmin_P3', 'tmin_P4', 'tmin_P5', 'tmin_P6', 'tmin_P7', 'tmin_P8', 'tmin_P9', 'tmin_P10', 'tmin_P11', 'tmin_P12', 'tmin_P13', 'tmin_P14', 'tmin_P15', 'tmax_P1', 'tmax_P2', 'tmax_P3', 'tmax_P4', 'tmax_P5', 'tmax_P6', 'tmax_P7', 'tmax_P8', 'tmax_P9', 'tmax_P10', 'tmax_P11', 'tmax_P12', 'tmax_P13', 'tmax_P14', 'tmax_P15', 'temp_P1', 'temp_P2', 'temp_P3', 'temp_P4', 'temp_P5', 'temp_P6', 'temp_P7', 'temp_P8', 'temp_P9', 'temp_P10', 'temp_P11', 'temp_P12', 'temp_P13', 'temp_P14', 'temp_P15', 'rel_humidity_P1', 'rel_humidity_P2', 'rel_humidity_P3', 'rel_humidity_P4', 'rel_humidity_P5', 'rel_humidity_P6', 'rel_humidity_P7', 'rel_humidity_P8', 'rel_humidity_P9', 'rel_humidity_P10', 'rel_humidity_P11', 'rel_humidity_P12', 'rel_humidity_P13', 'rel_humidity_P14', 'rel_humidity_P15', 'precipitation_P1', 'precipitation_P2', 'precipitation_P3', 'precipitation_P4', 'precipitation_P5', 'precipitation_P6', 'precipitation_P7', 'precipitation_P8', 'precipitation_P9', 'precipitation_P10', 'precipitation_P11', 'precipitation_P12', 'precipitation_P13', 'precipitation_P14', 'precipitation_P15', 'wind_speed_P1', 'wind_speed_P2', 'wind_speed_P3', 'wind_speed_P4', 'wind_speed_P5', 'wind_speed_P6', 'wind_speed_P7', 'wind_speed_P8', 'wind_speed_P9', 'wind_speed_P10', 'wind_speed_P11', 'wind_speed_P12', 'wind_speed_P13', 'wind_speed_P14', 'wind_speed_P15'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m data = df\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Selecting relevant features and target variable\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m X = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P6\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P7\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P9\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P11\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P13\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P14\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmin_P15\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P6\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P7\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P9\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P11\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P13\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P14\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtmax_P15\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P6\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P7\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P9\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P11\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P13\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P14\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemp_P15\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P6\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P7\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P9\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P11\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P13\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P14\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrel_humidity_P15\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P6\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P7\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P9\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P11\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P13\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P14\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprecipitation_P15\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P6\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P7\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P9\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P11\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P13\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P14\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwind_speed_P15\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdem\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maspect\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m]\u001b[49m\n\u001b[32m     21\u001b[39m y = data[\u001b[33m'\u001b[39m\u001b[33mspecies\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Balance the classes in the dataset by downsampling the majority class\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferli\\Documents\\ShroomRadar\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferli\\Documents\\ShroomRadar\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferli\\Documents\\ShroomRadar\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['tmin_P1', 'tmin_P2', 'tmin_P3', 'tmin_P4', 'tmin_P5', 'tmin_P6', 'tmin_P7', 'tmin_P8', 'tmin_P9', 'tmin_P10', 'tmin_P11', 'tmin_P12', 'tmin_P13', 'tmin_P14', 'tmin_P15', 'tmax_P1', 'tmax_P2', 'tmax_P3', 'tmax_P4', 'tmax_P5', 'tmax_P6', 'tmax_P7', 'tmax_P8', 'tmax_P9', 'tmax_P10', 'tmax_P11', 'tmax_P12', 'tmax_P13', 'tmax_P14', 'tmax_P15', 'temp_P1', 'temp_P2', 'temp_P3', 'temp_P4', 'temp_P5', 'temp_P6', 'temp_P7', 'temp_P8', 'temp_P9', 'temp_P10', 'temp_P11', 'temp_P12', 'temp_P13', 'temp_P14', 'temp_P15', 'rel_humidity_P1', 'rel_humidity_P2', 'rel_humidity_P3', 'rel_humidity_P4', 'rel_humidity_P5', 'rel_humidity_P6', 'rel_humidity_P7', 'rel_humidity_P8', 'rel_humidity_P9', 'rel_humidity_P10', 'rel_humidity_P11', 'rel_humidity_P12', 'rel_humidity_P13', 'rel_humidity_P14', 'rel_humidity_P15', 'precipitation_P1', 'precipitation_P2', 'precipitation_P3', 'precipitation_P4', 'precipitation_P5', 'precipitation_P6', 'precipitation_P7', 'precipitation_P8', 'precipitation_P9', 'precipitation_P10', 'precipitation_P11', 'precipitation_P12', 'precipitation_P13', 'precipitation_P14', 'precipitation_P15', 'wind_speed_P1', 'wind_speed_P2', 'wind_speed_P3', 'wind_speed_P4', 'wind_speed_P5', 'wind_speed_P6', 'wind_speed_P7', 'wind_speed_P8', 'wind_speed_P9', 'wind_speed_P10', 'wind_speed_P11', 'wind_speed_P12', 'wind_speed_P13', 'wind_speed_P14', 'wind_speed_P15'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "data = df\n",
    "\n",
    "# Selecting relevant features and target variable\n",
    "X = data[\n",
    "    ['tmin_P1', 'tmin_P2', 'tmin_P3', 'tmin_P4', 'tmin_P5', 'tmin_P6', 'tmin_P7', 'tmin_P8', 'tmin_P9', 'tmin_P10', 'tmin_P11', 'tmin_P12', 'tmin_P13', 'tmin_P14', 'tmin_P15',\n",
    "     'tmax_P1', 'tmax_P2', 'tmax_P3', 'tmax_P4', 'tmax_P5', 'tmax_P6', 'tmax_P7', 'tmax_P8', 'tmax_P9', 'tmax_P10', 'tmax_P11', 'tmax_P12', 'tmax_P13', 'tmax_P14', 'tmax_P15',\n",
    "     'temp_P1', 'temp_P2', 'temp_P3', 'temp_P4', 'temp_P5', 'temp_P6', 'temp_P7', 'temp_P8', 'temp_P9', 'temp_P10', 'temp_P11', 'temp_P12', 'temp_P13', 'temp_P14', 'temp_P15',\n",
    "     'rel_humidity_P1', 'rel_humidity_P2', 'rel_humidity_P3', 'rel_humidity_P4', 'rel_humidity_P5', 'rel_humidity_P6', 'rel_humidity_P7', 'rel_humidity_P8', 'rel_humidity_P9', 'rel_humidity_P10', 'rel_humidity_P11', 'rel_humidity_P12', 'rel_humidity_P13', 'rel_humidity_P14', 'rel_humidity_P15',\n",
    "     'precipitation_P1', 'precipitation_P2', 'precipitation_P3', 'precipitation_P4', 'precipitation_P5', 'precipitation_P6', 'precipitation_P7', 'precipitation_P8', 'precipitation_P9', 'precipitation_P10', 'precipitation_P11', 'precipitation_P12', 'precipitation_P13', 'precipitation_P14', 'precipitation_P15',\n",
    "     'wind_speed_P1', 'wind_speed_P2', 'wind_speed_P3', 'wind_speed_P4', 'wind_speed_P5', 'wind_speed_P6', 'wind_speed_P7', 'wind_speed_P8', 'wind_speed_P9', 'wind_speed_P10', 'wind_speed_P11', 'wind_speed_P12', 'wind_speed_P13', 'wind_speed_P14', 'wind_speed_P15',\n",
    "      'dem', 'aspect'\n",
    "    ]\n",
    "]\n",
    "y = data['species']\n",
    "\n",
    "# Balance the classes in the dataset by downsampling the majority class\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Combine X and y for easier resampling\n",
    "df_combined = X.copy()\n",
    "df_combined['species'] = y\n",
    "\n",
    "# Find the class counts\n",
    "class_counts = df_combined['species'].value_counts()\n",
    "min_class_count = class_counts.min()\n",
    "\n",
    "# Downsample each class to the size of the smallest class\n",
    "balanced_df = pd.concat([\n",
    "    resample(\n",
    "        df_combined[df_combined['species'] == label],\n",
    "        replace=False,\n",
    "        n_samples=min_class_count,\n",
    "        random_state=43\n",
    "    )\n",
    "    for label in class_counts.index\n",
    "])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=43).reset_index(drop=True)\n",
    "\n",
    "# Separate features and target again\n",
    "X_balanced = balanced_df.drop('species', axis=1)\n",
    "y_balanced = balanced_df['species']\n",
    "\n",
    "# Split the balanced data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.15, random_state=43, stratify=y_balanced)\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=43)\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(gb_clf, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_gb_clf = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_gb_clf.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9651972157772621\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Boletus edulis       0.98      0.95      0.97       673\n",
      "          None       0.95      0.98      0.96       620\n",
      "\n",
      "      accuracy                           0.97      1293\n",
      "     macro avg       0.97      0.97      0.97      1293\n",
      "  weighted avg       0.97      0.97      0.97      1293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = df\n",
    "\n",
    "required_variables = ['P_1', 'P_2', 'P_3', 'P_4', 'P_5', 'P_6', 'P_7', 'P_8', 'P_9', 'P_10', 'P_11', 'P_12', 'P_13', 'P_14',\n",
    "    'Pres_1', 'Pres_2', 'Pres_3', 'Pres_4', 'Pres_5', 'Pres_6', 'Pres_7', 'Pres_8', 'Pres_9', 'Pres_10', 'Pres_11', 'Pres_12', 'Pres_13', 'Pres_14',\n",
    "    'RelHum_1', 'RelHum_2', 'RelHum_3', 'RelHum_4', 'RelHum_5', 'RelHum_6', 'RelHum_7', 'RelHum_8', 'RelHum_9', 'RelHum_10', 'RelHum_11', 'RelHum_12', 'RelHum_13', 'RelHum_14',\n",
    "    'SpecHum_1', 'SpecHum_2', 'SpecHum_3', 'SpecHum_4', 'SpecHum_5', 'SpecHum_6', 'SpecHum_7', 'SpecHum_8', 'SpecHum_9', 'SpecHum_10', 'SpecHum_11', 'SpecHum_12', 'SpecHum_13', 'SpecHum_14',\n",
    "    'Temp_1', 'Temp_2', 'Temp_3', 'Temp_4', 'Temp_5', 'Temp_6', 'Temp_7', 'Temp_8', 'Temp_9', 'Temp_10', 'Temp_11', 'Temp_12', 'Temp_13', 'Temp_14',\n",
    "    'Tmax_1', 'Tmax_2', 'Tmax_3', 'Tmax_4', 'Tmax_5', 'Tmax_6', 'Tmax_7', 'Tmax_8', 'Tmax_9', 'Tmax_10', 'Tmax_11', 'Tmax_12', 'Tmax_13', 'Tmax_14',\n",
    "    'Tmin_1', 'Tmin_2', 'Tmin_3', 'Tmin_4', 'Tmin_5', 'Tmin_6', 'Tmin_7', 'Tmin_8', 'Tmin_9', 'Tmin_10', 'Tmin_11', 'Tmin_12', 'Tmin_13', 'Tmin_14',\n",
    "    'dem',\n",
    "    'slope'\n",
    "    ]\n",
    "\n",
    "y = data['species']\n",
    "\n",
    "# Selecting relevant features and target variable\n",
    "X = data[\n",
    "    required_variables\n",
    "]\n",
    "\n",
    "# Split the balanced data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=45)\n",
    "# Split the data into train and test sets\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=3, random_state=43)\n",
    "\n",
    "# Train the model\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..//shroomradar//data//models//gradient_boosting_model_new.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(gb_clf, '..//shroomradar//data//models//gradient_boosting_model_new.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final input shape: (12929, 14, 9)\n",
      "Labels shape: (12929,)\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferli\\Documents\\ShroomRadar\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8335 - loss: 0.3729 - val_accuracy: 0.8879 - val_loss: 0.2625\n",
      "Epoch 2/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9045 - loss: 0.2345 - val_accuracy: 0.9296 - val_loss: 0.1801\n",
      "Epoch 3/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9225 - loss: 0.1975 - val_accuracy: 0.9335 - val_loss: 0.1837\n",
      "Epoch 4/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9319 - loss: 0.1736 - val_accuracy: 0.9350 - val_loss: 0.1708\n",
      "Epoch 5/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9286 - loss: 0.1704 - val_accuracy: 0.9412 - val_loss: 0.1535\n",
      "Epoch 6/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9373 - loss: 0.1525 - val_accuracy: 0.9435 - val_loss: 0.1429\n",
      "Epoch 7/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9399 - loss: 0.1515 - val_accuracy: 0.9497 - val_loss: 0.1443\n",
      "Epoch 8/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9441 - loss: 0.1429 - val_accuracy: 0.9490 - val_loss: 0.1320\n",
      "Epoch 9/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9505 - loss: 0.1323 - val_accuracy: 0.9466 - val_loss: 0.1402\n",
      "Epoch 10/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9470 - loss: 0.1373 - val_accuracy: 0.9505 - val_loss: 0.1340\n",
      "Epoch 11/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9515 - loss: 0.1241 - val_accuracy: 0.9559 - val_loss: 0.1219\n",
      "Epoch 12/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9522 - loss: 0.1222 - val_accuracy: 0.9513 - val_loss: 0.1338\n",
      "Epoch 13/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9513 - loss: 0.1257 - val_accuracy: 0.9590 - val_loss: 0.1224\n",
      "Epoch 14/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9571 - loss: 0.1147 - val_accuracy: 0.9559 - val_loss: 0.1220\n",
      "Epoch 15/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9561 - loss: 0.1124 - val_accuracy: 0.9443 - val_loss: 0.1375\n",
      "Epoch 16/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9569 - loss: 0.1103 - val_accuracy: 0.9466 - val_loss: 0.1340\n",
      "Epoch 17/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9563 - loss: 0.1155 - val_accuracy: 0.9590 - val_loss: 0.1278\n",
      "Epoch 18/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9607 - loss: 0.1036 - val_accuracy: 0.9606 - val_loss: 0.1293\n",
      "Epoch 19/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9578 - loss: 0.1001 - val_accuracy: 0.9451 - val_loss: 0.1399\n",
      "Epoch 20/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9602 - loss: 0.1025 - val_accuracy: 0.9598 - val_loss: 0.1151\n",
      "Epoch 21/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9611 - loss: 0.0997 - val_accuracy: 0.9536 - val_loss: 0.1197\n",
      "Epoch 22/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9656 - loss: 0.0910 - val_accuracy: 0.9590 - val_loss: 0.1177\n",
      "Epoch 23/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9633 - loss: 0.1025 - val_accuracy: 0.9582 - val_loss: 0.1165\n",
      "Epoch 24/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9671 - loss: 0.0912 - val_accuracy: 0.9598 - val_loss: 0.1220\n",
      "Epoch 25/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9660 - loss: 0.0889 - val_accuracy: 0.9644 - val_loss: 0.1059\n",
      "Epoch 26/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9679 - loss: 0.0867 - val_accuracy: 0.9613 - val_loss: 0.1258\n",
      "Epoch 27/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9679 - loss: 0.0808 - val_accuracy: 0.9644 - val_loss: 0.1108\n",
      "Epoch 28/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9646 - loss: 0.0836 - val_accuracy: 0.9559 - val_loss: 0.1280\n",
      "Epoch 29/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9677 - loss: 0.0811 - val_accuracy: 0.9590 - val_loss: 0.1294\n",
      "Epoch 30/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9689 - loss: 0.0806 - val_accuracy: 0.9637 - val_loss: 0.1138\n",
      "Test Accuracy: 0.959938108921051\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# ===============================\n",
    "# 1. Data Preparation\n",
    "# ===============================\n",
    "\n",
    "# Assume df is your dataset\n",
    "data = df.copy()\n",
    "\n",
    "# Create binary target: sprout (1) or not (0)\n",
    "# If species is None => 0, otherwise => 1\n",
    "y = (data['species'] != \"None\").astype(int).values\n",
    "\n",
    "# Sequential variables\n",
    "variables = ['P', 'Pres', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "timesteps = 14\n",
    "\n",
    "X_sequences = []\n",
    "for _, row in data.iterrows():\n",
    "    seq = []\n",
    "    for t in range(1, timesteps+1):\n",
    "        timestep_features = []\n",
    "        for var in variables:\n",
    "            timestep_features.append(row[f\"{var}_{t}\"])\n",
    "        # Add DEM and slope to every timestep\n",
    "        timestep_features.extend([row['dem'], row['slope']])\n",
    "        seq.append(timestep_features)\n",
    "    X_sequences.append(seq)\n",
    "\n",
    "X = np.array(X_sequences)   # shape: [samples, timesteps, features]\n",
    "\n",
    "print(\"Final input shape:\", X.shape)  # [samples, timesteps, features]\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n",
    "# ===============================\n",
    "# 2. Feature Scaling (optional but recommended)\n",
    "# ===============================\n",
    "\n",
    "# Flatten to 2D, scale, reshape back\n",
    "nsamples, ntimesteps, nfeatures = X.shape\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X.reshape(-1, nfeatures)).reshape(nsamples, ntimesteps, nfeatures)\n",
    "\n",
    "# ===============================\n",
    "# 3. Train-Test Split\n",
    "# ===============================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.5, random_state=45\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 4. Build LSTM Model (binary output)\n",
    "# ===============================\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # binary output\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ===============================\n",
    "# 5. Train\n",
    "# ===============================\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 6. Evaluate\n",
    "# ===============================\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy:\", acc)\n",
    "\n",
    "# Predictions (threshold at 0.5)\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.959938128383604\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9587    0.9608    0.9598      3215\n",
      "           1     0.9611    0.9591    0.9601      3250\n",
      "\n",
      "    accuracy                         0.9599      6465\n",
      "   macro avg     0.9599    0.9599    0.9599      6465\n",
      "weighted avg     0.9599    0.9599    0.9599      6465\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3089  126]\n",
      " [ 133 3117]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ferli\\Documents\\ShroomRadar\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 9 variables whereas the saved optimizer has 16 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "model.save(\"..//shroomradar//data//models//lstm_model.keras\")\n",
    "\n",
    "# Load\n",
    "from tensorflow.keras.models import load_model\n",
    "lstm_model = load_model(\"..//shroomradar//data//models//lstm_model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logist regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "data = df   # make sure df is already defined\n",
    "target_col = \"species\"   # your target column\n",
    "\n",
    "# List of features\n",
    "required_variables = [\n",
    "    'tmin_P1','tmin_P2','tmin_P3','tmin_P4','tmin_P5','tmin_P6','tmin_P7','tmin_P8','tmin_P9','tmin_P10','tmin_P11','tmin_P12','tmin_P13','tmin_P14',\n",
    "    'tmax_P1','tmax_P2','tmax_P3','tmax_P4','tmax_P5','tmax_P6','tmax_P7','tmax_P8','tmax_P9','tmax_P10','tmax_P11','tmax_P12','tmax_P13','tmax_P14',\n",
    "    'temp_P1','temp_P2','temp_P3','temp_P4','temp_P5','temp_P6','temp_P7','temp_P8','temp_P9','temp_P10','temp_P11','temp_P12','temp_P13','temp_P14',\n",
    "    'rel_humidity_P1','rel_humidity_P2','rel_humidity_P3','rel_humidity_P4','rel_humidity_P5','rel_humidity_P6','rel_humidity_P7','rel_humidity_P8','rel_humidity_P9','rel_humidity_P10','rel_humidity_P11','rel_humidity_P12','rel_humidity_P13','rel_humidity_P14',\n",
    "    'precipitation_P1','precipitation_P2','precipitation_P3','precipitation_P4','precipitation_P5','precipitation_P6','precipitation_P7','precipitation_P8','precipitation_P9','precipitation_P10','precipitation_P11','precipitation_P12','precipitation_P13','precipitation_P14',\n",
    "    'wind_speed_P1','wind_speed_P2','wind_speed_P3','wind_speed_P4','wind_speed_P5','wind_speed_P6','wind_speed_P7','wind_speed_P8','wind_speed_P9','wind_speed_P10','wind_speed_P11','wind_speed_P12','wind_speed_P13','wind_speed_P14',\n",
    "    'elevation','aspect'\n",
    "]\n",
    "\n",
    "# Extract features\n",
    "X = data[required_variables]\n",
    "\n",
    "# --- Step 1: Encode categorical features (None -> 0, others -> 1) ---\n",
    "categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].apply(lambda x: 0 if pd.isnull(x) or x == \"None\" else 1)\n",
    "\n",
    "# --- Step 2: Encode target (None=0, rest=1) ---\n",
    "y = data[target_col].apply(lambda x: 0 if x == \"None\" else 1)\n",
    "\n",
    "# --- Step 3: Train-test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=45, stratify=y\n",
    ")\n",
    "\n",
    "# --- Step 4: Oversample minority with SMOTE ---\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# --- Step 5: Logistic Regression ---\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_res, y_res)\n",
    "\n",
    "\n",
    "# --- Step 6: Predictions ---\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Step 7: Evaluation ---\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"None\",\"Other\"]))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(log_reg, 'docker//data//models//lr_model_v5.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# ----------------------\n",
    "# Load and prepare data\n",
    "# ----------------------\n",
    "data = df   # make sure df is already defined\n",
    "target_col = \"species\"   # <-- your target column\n",
    "\n",
    "required_variables = [\n",
    "    'tmin_P1','tmin_P2','tmin_P3','tmin_P4','tmin_P5','tmin_P6','tmin_P7','tmin_P8','tmin_P9','tmin_P10','tmin_P11','tmin_P12','tmin_P13','tmin_P14',\n",
    "    'tmax_P1','tmax_P2','tmax_P3','tmax_P4','tmax_P5','tmax_P6','tmax_P7','tmax_P8','tmax_P9','tmax_P10','tmax_P11','tmax_P12','tmax_P13','tmax_P14',\n",
    "    'temp_P1','temp_P2','temp_P3','temp_P4','temp_P5','temp_P6','temp_P7','temp_P8','temp_P9','temp_P10','temp_P11','temp_P12','temp_P13','temp_P14',\n",
    "    'rel_humidity_P1','rel_humidity_P2','rel_humidity_P3','rel_humidity_P4','rel_humidity_P5','rel_humidity_P6','rel_humidity_P7','rel_humidity_P8','rel_humidity_P9','rel_humidity_P10','rel_humidity_P11','rel_humidity_P12','rel_humidity_P13','rel_humidity_P14',\n",
    "    'precipitation_P1','precipitation_P2','precipitation_P3','precipitation_P4','precipitation_P5','precipitation_P6','precipitation_P7','precipitation_P8','precipitation_P9','precipitation_P10','precipitation_P11','precipitation_P12','precipitation_P13','precipitation_P14',\n",
    "    'wind_speed_P1','wind_speed_P2','wind_speed_P3','wind_speed_P4','wind_speed_P5','wind_speed_P6','wind_speed_P7','wind_speed_P8','wind_speed_P9','wind_speed_P10','wind_speed_P11','wind_speed_P12','wind_speed_P13','wind_speed_P14',\n",
    "    'elevation','aspect'\n",
    "]\n",
    "\n",
    "# Features\n",
    "X = data[required_variables].copy()\n",
    "\n",
    "# Encode categorical features: None/NaN -> 0, else -> 1\n",
    "categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].apply(lambda x: 0 if pd.isnull(x) or x == \"None\" else 1)\n",
    "\n",
    "# Encode target: None=0, all other species=1\n",
    "y = data[target_col].apply(lambda x: 0 if x == \"None\" else 1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=45, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# Option 1: Logistic Regression with class_weight balanced\n",
    "# ----------------------\n",
    "log_reg_balanced = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "log_reg_balanced.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = log_reg_balanced.predict(X_test)\n",
    "y_prob1 = log_reg_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Option 1: Logistic Regression (class_weight balanced) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred1))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred1, target_names=[\"None\",\"Other\"]))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob1))\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Option 3: Logistic Regression + Probability Calibration\n",
    "# ----------------------\n",
    "log_reg_base = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "calibrated_clf = CalibratedClassifierCV(log_reg_base, method='isotonic', cv=5)\n",
    "calibrated_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = calibrated_clf.predict(X_test)\n",
    "y_prob3 = calibrated_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== Option 3: Logistic Regression (Calibrated) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred3))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred3, target_names=[\"None\",\"Other\"]))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "gb_clf = gb_clf\n",
    "\n",
    "# Load the GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('updated_spain.geojson')\n",
    "spain = spain.to_crs('EPSG:4326')  # Convert CRS if needed\n",
    "spain.rename(columns={'mode_value': 'LC', 'mean_elevation': 'elevation'}, inplace=True)\n",
    "\n",
    "# Define the directory containing the data files\n",
    "data_dir = \"MSWX_V100/Past\"\n",
    "\n",
    "# Define the list of variables required for prediction\n",
    "required_variables = ['Pres_1', 'Pres_2', 'Pres_3', 'Pres_4', 'Pres_5', 'Pres_6', 'Pres_7', 'Pres_8', 'Pres_9', 'Pres_10',\n",
    "                      'Pres_11', 'Pres_12', 'Pres_13', 'Pres_14', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5', 'P_6', 'P_7', 'P_8',\n",
    "                      'P_9', 'P_10', 'P_11', 'P_12', 'P_13', 'P_14', 'RelHum_1', 'RelHum_2', 'RelHum_3', 'RelHum_4',\n",
    "                      'RelHum_5', 'RelHum_6', 'RelHum_7', 'RelHum_8', 'RelHum_9', 'RelHum_10', 'RelHum_11', 'RelHum_12',\n",
    "                      'RelHum_13', 'RelHum_14', 'SpecHum_1', 'SpecHum_2', 'SpecHum_3', 'SpecHum_4', 'SpecHum_5',\n",
    "                      'SpecHum_6', 'SpecHum_7', 'SpecHum_8', 'SpecHum_9', 'SpecHum_10', 'SpecHum_11', 'SpecHum_12',\n",
    "                      'SpecHum_13', 'SpecHum_14', 'Temp_1', 'Temp_2', 'Temp_3', 'Temp_4', 'Temp_5', 'Temp_6', 'Temp_7',\n",
    "                      'Temp_8', 'Temp_9', 'Temp_10', 'Temp_11', 'Temp_12', 'Temp_13', 'Temp_14', 'Tmax_1', 'Tmax_2',\n",
    "                      'Tmax_3', 'Tmax_4', 'Tmax_5', 'Tmax_6', 'Tmax_7', 'Tmax_8', 'Tmax_9', 'Tmax_10', 'Tmax_11',\n",
    "                      'Tmax_12', 'Tmax_13', 'Tmax_14', 'Tmin_1', 'Tmin_2', 'Tmin_3', 'Tmin_4', 'Tmin_5', 'Tmin_6',\n",
    "                      'Tmin_7', 'Tmin_8', 'Tmin_9', 'Tmin_10', 'Tmin_11', 'Tmin_12', 'Tmin_13', 'Tmin_14', 'LC',\n",
    "                      'elevation']\n",
    "\n",
    "# Initialize an empty DataFrame to store predictions\n",
    "predictions_df = pd.DataFrame(columns=['geometry', 'species_prediction'])\n",
    "\n",
    "# Iterate over each polygon in the GeoDataFrame\n",
    "for index, row in spain.iterrows():\n",
    "    # Extract the required variables for prediction\n",
    "    variables_for_prediction = row[required_variables]\n",
    "\n",
    "\n",
    "    # Reshape the variables for prediction into a single-row DataFrame\n",
    "    variables_for_prediction_df = pd.DataFrame(variables_for_prediction).transpose()\n",
    "    \n",
    "\n",
    "\n",
    "    # Make the prediction\n",
    "    predicted_species = gb_clf.predict_proba(variables_for_prediction_df)\n",
    "    \n",
    "    print(\"Predicted species:\")\n",
    "    print(predicted_species[0][0])\n",
    "\n",
    "    # Append the prediction to the DataFrame\n",
    "    predictions_df = pd.concat([predictions_df, pd.DataFrame({'geometry': row['geometry'], 'species_prediction': predicted_species[0][0]}, index=[0])], ignore_index=True)\n",
    "\n",
    "# Merge the predictions with the original GeoDataFrame\n",
    "spain_with_predictions = pd.concat([spain, predictions_df['species_prediction']], axis=1)\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain_with_predictions.to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure you have a sample of your data to explain\n",
    "# We'll use the first 100 rows for speed, but you can adjust as needed\n",
    "X_sample = df[required_variables].iloc[:100]\n",
    "\n",
    "# Create a SHAP explainer for the trained model\n",
    "explainer = shap.Explainer(gb_clf, X_sample)\n",
    "\n",
    "# Calculate SHAP values for the sample\n",
    "shap_values = explainer(X_sample)\n",
    "\n",
    "# Plot a summary SHAP graph (beeswarm plot)\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"dot\", show=True)\n",
    "\n",
    "# Optionally, plot a bar chart of mean absolute SHAP values (feature importance)\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_importance = gb_clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(feature_importance_df[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load your data\n",
    "data = df\n",
    "\n",
    "# Extract static features\n",
    "X_static = data[['LC', 'elevation']]  # Add other static variables here\n",
    "\n",
    "# Reshape the data\n",
    "temporal_vars = ['P', 'Pres', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "ordered_data = pd.concat([pd.concat([data[[f\"{var}_{i+1}\"]] for i in range(14)], axis=1).stack().reset_index(level=1, drop=True) for var in temporal_vars], axis=1)\n",
    "\n",
    "# Merge temporal and static features\n",
    "X_temporal = ordered_data.values\n",
    "X_static_repeated = np.repeat(X_static.values, 14, axis=0)\n",
    "X = np.concatenate([X_temporal, X_static_repeated], axis=1)\n",
    "\n",
    "# Reshape X to have the same number of samples as y\n",
    "X = X[:525]\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['species'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train[:, :-2].shape)\n",
    "\n",
    "# Reshape input data for LSTM (samples, timesteps, features)\n",
    "X_train_temporal = np.reshape(X_train[:, :-2], (X_train.shape[0], 14, 7))\n",
    "X_train_static = np.tile(X_train[:, -2:], (1, 14, 1))\n",
    "\n",
    "# Similarly, reshape the test data\n",
    "X_test_temporal = np.reshape(X_test[:, :-2], (X_test.shape[0], 14, -1))\n",
    "X_test_static = np.tile(X_test[:, -2:], (1, 14, 1))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(14, X_train.shape[1] // 14)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_temporal, X_train_static], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict([X_test_temporal, X_test_static])\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "\n",
    "def get_environmental_data(polygon, date, data_dir, variables, num_days):\n",
    "    data = {}\n",
    "    for variable in variables:\n",
    "        variable_values = []\n",
    "        for i in range(num_days):\n",
    "            current_date = date - timedelta(days=i)\n",
    "            file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "            data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "            if not os.path.isfile(data_file):\n",
    "                print(f\"File not found for {variable} on {file_date_str}\")\n",
    "                variable_values.append(np.nan)\n",
    "                continue\n",
    "            with rasterio.open(data_file, mode=\"r\") as src:\n",
    "                min_x, min_y, max_x, max_y = polygon.bounds\n",
    "                centroid_x = (min_x + max_x) / 2\n",
    "                centroid_y = (min_y + max_y) / 2\n",
    "                centroid = Point(centroid_x, centroid_y)\n",
    "                px, py = src.index(centroid.x, centroid.y)\n",
    "                value = src.read(1, window=((py, py+1), (px, px+1)))\n",
    "                variable_values.append(value[0, 0])\n",
    "        data[variable] = variable_values[::-1]  # Reverse the list to align with the order needed (recent to past)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_row(row_tuple):\n",
    "    index, row = row_tuple\n",
    "    average_data = get_environmental_data(row['geometry'], test_date, data_dir, variables, num_days)\n",
    "    variables_for_prediction = {}\n",
    "    for variable in variables:\n",
    "        for day_number, value in enumerate(average_data[variable], start=1):\n",
    "            variable_name = f\"{variable}_{day_number}\"\n",
    "            variables_for_prediction[variable_name] = value\n",
    "    variables_for_prediction['LC'] = row['mode_value']\n",
    "    variables_for_prediction['elevation'] = row['mean_elevation']\n",
    "    df = pd.DataFrame(variables_for_prediction, index=[0])\n",
    "    # Display all rows and columns without truncation\n",
    "    #with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #    print(df)    \n",
    "    predicted_species = gb_clf.predict_proba(df)[0][0]  # Make prediction\n",
    "    return predicted_species\n",
    "\n",
    "\n",
    "# Load GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('spain_3km_ready.geojson')\n",
    "spain = spain.to_crs('EPSG:4326')\n",
    "\n",
    "data_dir = \"new_data/NRT\"\n",
    "variables = ['Pres', 'P', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "num_days = 14\n",
    "test_date = datetime(2024, 5, 3)\n",
    "\n",
    "# Use ThreadPoolExecutor to run multiple threads\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    predictions = list(tqdm(executor.map(process_row, spain.iterrows()), total=len(spain), desc=\"Making predictions\"))\n",
    "\n",
    "# Add predictions to the GeoDataFrame\n",
    "spain['species_prediction'] = predictions\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain[['geometry', 'species_prediction']].to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def get_environmental_data(polygon, date, data_dir, variables, num_days):\n",
    "    data = {}\n",
    "    for variable in variables:\n",
    "        variable_values = []\n",
    "        for i in range(num_days):\n",
    "            current_date = date - timedelta(days=i)\n",
    "            file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "            data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "            if not os.path.isfile(data_file):\n",
    "                print(f\"File not found for {variable} on {file_date_str}\")\n",
    "                variable_values.append(np.nan)\n",
    "                continue\n",
    "            with rasterio.open(data_file, mode=\"r\") as src:\n",
    "                min_x, min_y, max_x, max_y = polygon.bounds\n",
    "                centroid_x = (min_x + max_x) / 2\n",
    "                centroid_y = (min_y + max_y) / 2\n",
    "                centroid = Point(centroid_x, centroid_y)\n",
    "                px, py = src.index(centroid.x, centroid.y)\n",
    "                value = src.read(1, window=((py, py+1), (px, px+1)))\n",
    "                variable_values.append(value[0, 0])\n",
    "        data[variable] = variable_values[::-1]  # Reverse the list to align with the order needed (recent to past)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_row(index, row):\n",
    "    average_data = get_environmental_data(row['geometry'], test_date, data_dir, variables, num_days)\n",
    "    variables_for_prediction = {}\n",
    "    for variable in variables:\n",
    "        for day_number, value in enumerate(average_data[variable], start=1):\n",
    "            variable_name = f\"{variable}_{day_number}\"\n",
    "            variables_for_prediction[variable_name] = value\n",
    "    variables_for_prediction['LC'] = row['mode_value']\n",
    "    variables_for_prediction['elevation'] = row['mean_elevation']\n",
    "    df = pd.DataFrame(variables_for_prediction, index=[0])\n",
    "    predicted_species = gb_clf.predict_proba(df)[0][0]  # Make prediction\n",
    "    return predicted_species\n",
    "\n",
    "# Load GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('spain_3km_ready.geojson')\n",
    "spain = spain.to_crs('EPSG:4326')\n",
    "\n",
    "data_dir = \"new_data/NRT\"\n",
    "variables = ['Pres', 'P', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "num_days = 14\n",
    "test_date = datetime(2024, 5, 3)\n",
    "\n",
    "# Sequentially process each row\n",
    "predictions = []\n",
    "for index, row in tqdm(spain.iterrows(), total=len(spain), desc=\"Making predictions\"):\n",
    "    prediction = process_row(index, row)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Add predictions to the GeoDataFrame\n",
    "spain['species_prediction'] = predictions\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain[['geometry', 'species_prediction']].to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def extract_data_from_raster(data_file, polygons):\n",
    "    values = []\n",
    "    if not os.path.isfile(data_file):\n",
    "        print(f\"File not found: {data_file}\")\n",
    "        return [np.nan] * len(polygons)\n",
    "    \n",
    "    with rasterio.open(data_file, mode=\"r\") as src:\n",
    "        for polygon in tqdm(polygons, desc=data_file):\n",
    "            min_x, min_y, max_x, max_y = polygon.bounds\n",
    "            centroid_x = (min_x + max_x) / 2\n",
    "            centroid_y = (min_y + max_y) / 2\n",
    "            centroid = Point(centroid_x, centroid_y)\n",
    "            px, py = src.index(centroid.x, centroid.y)\n",
    "            try:\n",
    "                value = src.read(1, window=((py, py+1), (px, px+1)))\n",
    "                values.append(value[0, 0])\n",
    "            except:\n",
    "                values.append(np.nan)\n",
    "    \n",
    "    return values\n",
    "\n",
    "def get_environmental_data(polygons, date, data_dir, variables, num_days):\n",
    "    data = {variable: [[] for _ in range(len(polygons))] for variable in variables}\n",
    "    \n",
    "    dates = [date - timedelta(days=i) for i in range(num_days)]\n",
    "    \n",
    "    for variable in tqdm(variables, desc=\"Variable\"):\n",
    "        print(f\"Processing variable: {variable}\")\n",
    "        for current_date in dates:\n",
    "            file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "            data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "            values = extract_data_from_raster(data_file, polygons)\n",
    "            for i, value in enumerate(values):\n",
    "                data[variable][i].append(value)\n",
    "    \n",
    "    for variable in variables:\n",
    "        for i in range(len(polygons)):\n",
    "            data[variable][i] = data[variable][i][::-1]  # Reverse the list to align with the order needed (recent to past)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepare_data_for_prediction(polygon_index, all_data, variables, num_days, row):\n",
    "    variables_for_prediction = {}\n",
    "    for variable in variables:\n",
    "        for day_number in range(num_days):\n",
    "            variable_name = f\"{variable}_{day_number+1}\"\n",
    "            variables_for_prediction[variable_name] = all_data[variable][polygon_index][day_number]\n",
    "    \n",
    "    variables_for_prediction['LC'] = row['mode_value']\n",
    "    variables_for_prediction['elevation'] = row['mean_elevation']\n",
    "    \n",
    "    return variables_for_prediction\n",
    "\n",
    "# Load GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('spain_3km_ready.geojson')\n",
    "spain = spain.to_crs('EPSG:4326')\n",
    "\n",
    "data_dir = \"new_data/NRT\"\n",
    "variables = ['Pres', 'P', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "num_days = 14\n",
    "test_date = datetime(2024, 5, 3)\n",
    "\n",
    "# Prepare polygon geometries\n",
    "polygon_geometries = spain['geometry']\n",
    "\n",
    "# Get environmental data for all polygons\n",
    "all_data = get_environmental_data(polygon_geometries, test_date, data_dir, variables, num_days)\n",
    "\n",
    "# Sequentially process each row for predictions\n",
    "predictions = []\n",
    "for index, row in tqdm(spain.iterrows(), total=len(spain), desc=\"Making predictions\"):\n",
    "    data_dict = prepare_data_for_prediction(index, all_data, variables, num_days, row)\n",
    "    df = pd.DataFrame(data_dict, index=[0])\n",
    "    predicted_species = gb_clf.predict_proba(df)[0][0]  # Make prediction\n",
    "    predictions.append(predicted_species)\n",
    "\n",
    "# Add predictions to the GeoDataFrame\n",
    "spain['species_prediction'] = predictions\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain[['geometry', 'species_prediction']].to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for all rows\n",
    "prepared_data = []\n",
    "for index, row in tqdm(spain.iterrows(), total=len(spain), desc=\"Preparing data\"):\n",
    "    data_dict = prepare_data_for_prediction(index, all_data, variables, num_days, row)\n",
    "    prepared_data.append(data_dict)\n",
    "\n",
    "# Concatenate all prepared data into a single DataFrame\n",
    "all_prepared_data = pd.DataFrame(prepared_data)\n",
    "\n",
    "# Fill NaN values with column means\n",
    "all_prepared_data.fillna(all_prepared_data.mean(), inplace=True)\n",
    "\n",
    "# Sequential processing for predictions\n",
    "predictions = []\n",
    "for index, row in tqdm(all_prepared_data.iterrows(), total=len(all_prepared_data), desc=\"Making predictions\"):\n",
    "    df = row.to_frame().T  # Convert the row to a DataFrame for prediction\n",
    "    predicted_species = gb_clf.predict_proba(df)[0][0]  # Make prediction\n",
    "    predictions.append(predicted_species)\n",
    "\n",
    "# Add predictions to the GeoDataFrame\n",
    "spain['species_prediction'] = predictions\n",
    "\n",
    "\n",
    "# Keep only 'geometry' and 'species_prediction' columns\n",
    "spain_smaller = spain[['geometry', 'species_prediction']]\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions_whole.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain_smaller.to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove polygons with a species_prediction value lower than 0.0001\n",
    "import geopandas as pd\n",
    "\n",
    "spain_smaller = pd.read_file(\"spain_with_species_predictions_whole.geojson\")\n",
    "threshold = 0.005\n",
    "initial_count = len(spain_smaller)\n",
    "spain_filtered = spain_smaller[spain_smaller['species_prediction'] >= threshold]\n",
    "removed_count = initial_count - len(spain_filtered)\n",
    "\n",
    "# Print the number of polygons removed\n",
    "print(f\"Number of polygons removed: {removed_count}\")\n",
    "print(f\"Number of polygons left: {len(spain_filtered)}\")\n",
    "\n",
    "# Define the output GeoJSON file path\n",
    "output_file = \"spain_with_species_predictions_smaller.geojson\"\n",
    "\n",
    "# Write the GeoDataFrame with predictions to a new GeoJSON file\n",
    "spain_filtered.to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "print(\"Predictions have been made and saved to\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
