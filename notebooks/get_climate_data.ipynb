{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generating file structure from CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5403/5403 [00:00<00:00, 173094.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Found 2744 unique observation dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building file list: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2744/2744 [00:00<00:00, 20204.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generated 140032 unique file entries\n",
      "‚úÖ File structure saved to: climate_files_from_csv.txt\n",
      "Step 2: Downloading climate data files...\n",
      "‚ñ∂Ô∏è Running command: ..\\docker\\rclone.exe sync -v --filter-from climate_files_from_csv.txt --drive-shared-with-me google:/MSWX_V100 ..//climate_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_unique_dates_from_csv(csv_file_path, limit_rows=None):\n",
    "    \"\"\"Extract unique observation dates from the CSV file, with optional row limit.\"\"\"\n",
    "    unique_dates = set()\n",
    "\n",
    "    with open(csv_file_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        total_rows = sum(1 for _ in open(csv_file_path)) - 1  # minus header\n",
    "        csvfile.seek(0)\n",
    "\n",
    "        for i, row in enumerate(tqdm(reader, total=limit_rows or total_rows, desc=\"Reading CSV\")):\n",
    "            if limit_rows is not None and i >= limit_rows:\n",
    "                break\n",
    "\n",
    "            if 'observed_on' in row and row['observed_on']:\n",
    "                date_str = row['observed_on'].split(\" \")[0]\n",
    "                date_obj = None\n",
    "                \n",
    "                # Try different date formats\n",
    "                date_formats = [\n",
    "                    \"%Y-%m-%d\",      # 2022-03-02\n",
    "                    \"%m/%d/%Y\",      # 2/3/2022\n",
    "                    \"%d/%m/%Y\",      # 3/2/2022\n",
    "                    \"%Y/%m/%d\",      # 2022/03/02\n",
    "                    \"%m-%d-%Y\",      # 2-3-2022\n",
    "                    \"%d-%m-%Y\",      # 3-2-2022\n",
    "                ]\n",
    "                \n",
    "                for fmt in date_formats:\n",
    "                    try:\n",
    "                        date_obj = datetime.strptime(date_str, fmt)\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                \n",
    "                if date_obj:\n",
    "                    unique_dates.add(date_obj)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Could not parse date: {row['observed_on']}\")\n",
    "                    continue\n",
    "    return sorted(list(unique_dates))\n",
    "\n",
    "\n",
    "def generate_file_structure_from_csv(csv_file_path, output_file_path, limit_rows=None):\n",
    "    \"\"\"\n",
    "    Generate file structure for all dates in CSV with 15 days before each observation,\n",
    "    for both MSWX_V100/PAST and MSWX_V100/NRT.\n",
    "    \"\"\"\n",
    "    unique_dates = get_unique_dates_from_csv(csv_file_path, limit_rows=limit_rows)\n",
    "    print(f\"üìÖ Found {len(unique_dates)} unique observation dates\")\n",
    "\n",
    "    all_files = set()\n",
    "    variables = ['Wind', 'P', 'Pres', 'RelHum', 'SpecHum', 'Tmin', 'Tmax', 'Temp']\n",
    "\n",
    "    for obs_date in tqdm(unique_dates, desc=\"Building file list\"):\n",
    "        for i in range(15):\n",
    "            target_date = obs_date - timedelta(days=i)\n",
    "            date_string = target_date.strftime(\"%Y%j\")\n",
    "\n",
    "            for variable in variables:\n",
    "                all_files.add(f\"+ /Past/{variable}/Daily/{date_string}.nc\")\n",
    "                all_files.add(f\"+ /NRT/{variable}/Daily/{date_string}.nc\")\n",
    "\n",
    "    file_list = sorted(list(all_files))\n",
    "    file_list.append(\"- *\")  # exclude everything else\n",
    "\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for line in file_list:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "    print(f\"üìù Generated {len(file_list)-1} unique file entries\")\n",
    "    print(f\"‚úÖ File structure saved to: {output_file_path}\")\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def run_rclone_sync_fixed(filter_file_path, dest_folder=\"climate_data\"):\n",
    "    \"\"\"Run rclone sync for both PAST and NRT using one filter file\"\"\"\n",
    "    rclone_path = os.path.join(\"..\", \"docker\", \"rclone.exe\")\n",
    "\n",
    "    if not os.path.exists(rclone_path):\n",
    "        print(f\"‚ùå Error: rclone not found at {rclone_path}\")\n",
    "        return\n",
    "\n",
    "    command = [\n",
    "        rclone_path, \"sync\", \"-v\",\n",
    "        \"--filter-from\", filter_file_path,\n",
    "        \"--drive-shared-with-me\",\n",
    "        \"google:/MSWX_V100\",  # root folder\n",
    "        dest_folder\n",
    "    ]\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è Running command: {' '.join(command)}\")\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(\"‚úÖ rclone sync command executed successfully.\")\n",
    "        print(f\"üìÇ Climate data downloaded to: {dest_folder}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error running rclone: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: rclone executable not found.\")\n",
    "\n",
    "\n",
    "def download_climate_data_from_csv_fixed(csv_file_path, dest_folder=\"..//climate_data\", limit_rows=None):\n",
    "    \"\"\"Complete workflow with filter file + rclone sync\"\"\"\n",
    "    filter_file_path = \"climate_files_from_csv.txt\"\n",
    "\n",
    "    print(\"Step 1: Generating file structure from CSV...\")\n",
    "    generate_file_structure_from_csv(csv_file_path, filter_file_path, limit_rows=limit_rows)\n",
    "\n",
    "    print(\"Step 2: Downloading climate data files...\")\n",
    "    run_rclone_sync_fixed(filter_file_path, dest_folder)\n",
    "\n",
    "    return filter_file_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = \"..//data/negative_samples_within_land_10k_with_coords_topography.csv\"\n",
    "    download_climate_data_from_csv_fixed(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
