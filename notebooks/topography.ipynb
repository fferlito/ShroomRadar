{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import earthaccess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Login using environment variables\n",
    "auth = earthaccess.login(strategy=\"environment\")\n",
    "\n",
    "# Search for Rome area\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"SRTMGL1\",\n",
    "    version=\"003\",\n",
    "    bounding_box=(12.35, 41.8, 12.65, 42.0)  # min lon, min lat, max lon, max lat\n",
    ")\n",
    "\n",
    "# Download tiles\n",
    "paths = earthaccess.download(results, \"./srtm_tiles\")\n",
    "print(paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append variables to CVS for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 19:25:02,937 - INFO - Loading CSV: data/negative_samples_within_land_10k_with_coords.csv\n",
      "2025-09-15 19:25:02,948 - INFO - Loaded 5403 rows\n",
      "2025-09-15 19:25:02,950 - INFO - Step 1: Scanning CSV for tile requirements...\n",
      "Scanning CSV:  14%|█▍        | 754/5403 [00:05<00:30, 154.43it/s]2025-09-15 19:25:08,709 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\S28E016_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:08,710 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  15%|█▍        | 803/5403 [00:06<00:32, 142.96it/s]2025-09-15 19:25:09,012 - INFO - GDAL signalled an error: err_no=1, msg='N53E039_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\N53E039_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:09,014 - INFO - GDAL signalled an error: err_no=1, msg='N53E039_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  15%|█▌        | 818/5403 [00:06<00:31, 143.79it/s]2025-09-15 19:25:09,106 - INFO - GDAL signalled an error: err_no=1, msg='N32W009_slope.tif: TIFFFetchDirectory:dem_tiles\\\\slope\\\\N32W009_slope.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:09,108 - INFO - GDAL signalled an error: err_no=1, msg='N32W009_slope.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  46%|████▋     | 2509/5403 [00:19<00:15, 185.53it/s]2025-09-15 19:25:22,604 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\S28E016_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:22,605 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  71%|███████   | 3822/5403 [00:26<00:06, 242.52it/s]2025-09-15 19:25:29,599 - INFO - GDAL signalled an error: err_no=1, msg='S28E122_slope.tif: TIFFFetchDirectory:dem_tiles\\\\slope\\\\S28E122_slope.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:29,600 - INFO - GDAL signalled an error: err_no=1, msg='S28E122_slope.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  75%|███████▌  | 4054/5403 [00:27<00:04, 273.83it/s]2025-09-15 19:25:30,528 - INFO - GDAL signalled an error: err_no=1, msg='N49E110_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\N49E110_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:30,529 - INFO - GDAL signalled an error: err_no=1, msg='N49E110_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV: 100%|██████████| 5403/5403 [00:32<00:00, 165.16it/s]\n",
      "2025-09-15 19:25:35,668 - INFO - Found 4310 tiles that need processing\n",
      "2025-09-15 19:25:35,668 - INFO - Step 4: Extracting values from rasters...\n",
      "Extracting values:  14%|█▍        | 763/5403 [00:17<01:26, 53.91it/s]2025-09-15 19:25:53,495 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\S28E016_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:53,496 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  15%|█▍        | 807/5403 [00:18<01:20, 56.99it/s]2025-09-15 19:25:54,215 - INFO - GDAL signalled an error: err_no=1, msg='N53E039_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\N53E039_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:54,216 - INFO - GDAL signalled an error: err_no=1, msg='N53E039_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  15%|█▌        | 820/5403 [00:18<01:24, 54.51it/s]2025-09-15 19:25:54,473 - INFO - GDAL signalled an error: err_no=1, msg='N32W009_slope.tif: TIFFFetchDirectory:dem_tiles\\\\slope\\\\N32W009_slope.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:54,473 - INFO - GDAL signalled an error: err_no=1, msg='N32W009_slope.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  47%|████▋     | 2517/5403 [00:49<00:54, 53.11it/s]2025-09-15 19:26:25,583 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\S28E016_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:26:25,583 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  71%|███████   | 3819/5403 [01:20<00:20, 77.12it/s]2025-09-15 19:26:55,854 - INFO - GDAL signalled an error: err_no=1, msg='S28E122_slope.tif: TIFFFetchDirectory:dem_tiles\\\\slope\\\\S28E122_slope.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:26:55,855 - INFO - GDAL signalled an error: err_no=1, msg='S28E122_slope.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  75%|███████▌  | 4071/5403 [01:23<00:18, 72.75it/s]2025-09-15 19:26:58,926 - INFO - GDAL signalled an error: err_no=1, msg='N49E110_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\N49E110_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:26:58,926 - INFO - GDAL signalled an error: err_no=1, msg='N49E110_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values: 100%|██████████| 5403/5403 [01:51<00:00, 48.48it/s]\n",
      "2025-09-15 19:27:27,108 - INFO - Step 5: Decoding geomorphon classes...\n",
      "2025-09-15 19:27:27,146 - INFO - ==================================================\n",
      "2025-09-15 19:27:27,148 - INFO - PROCESSING SUMMARY\n",
      "2025-09-15 19:27:27,148 - INFO - ==================================================\n",
      "2025-09-15 19:27:27,149 - INFO - Total rows processed: 5403\n",
      "2025-09-15 19:27:27,149 - INFO - Invalid coordinates: 0\n",
      "2025-09-15 19:27:27,150 - INFO - Missing tiles: 0\n",
      "2025-09-15 19:27:27,151 - INFO - Extraction failures: 0\n",
      "2025-09-15 19:27:27,153 - INFO - Successful DEM extractions: 5126\n",
      "2025-09-15 19:27:27,153 - INFO - Successful slope extractions: 3326\n",
      "2025-09-15 19:27:27,153 - INFO - Successful aspect extractions: 3323\n",
      "2025-09-15 19:27:27,155 - INFO - Successful geomorphon extractions: 0\n",
      "2025-09-15 19:27:27,155 - INFO - ✅ Done! Saved data/negative_samples_within_land_10k_with_coords_topography.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from whitebox.whitebox_tools import WhiteboxTools\n",
    "import earthaccess\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# ---------------------------\n",
    "# Tile ID utilities\n",
    "# ---------------------------\n",
    "def tile_id_from_coords(lat, lon):\n",
    "    \"\"\"Convert coords to tile ID (e.g. N40W106).\"\"\"\n",
    "    import math\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return None\n",
    "    ns = \"N\" if lat >= 0 else \"S\"\n",
    "    ew = \"E\" if lon >= 0 else \"W\"\n",
    "    # Use floor for both positive and negative coordinates to handle edge cases properly\n",
    "    lat_tile = math.floor(lat)\n",
    "    lon_tile = math.floor(lon)\n",
    "    return f\"{ns}{abs(lat_tile):02d}{ew}{abs(lon_tile):03d}\"\n",
    "\n",
    "# ---------------------------\n",
    "# DEM Download\n",
    "# ---------------------------\n",
    "def download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=\"dem_tiles\", prefer=\"SRTMGL1\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    earthaccess.login(strategy=\"environment\", persist=True)\n",
    "    dataset = (\"SRTMGL1\", \"003\") if prefer == \"SRTMGL1\" else (\"COPDEM_GLO_30\", \"001\")\n",
    "    try:\n",
    "        results = earthaccess.search_data(\n",
    "            short_name=dataset[0],\n",
    "            version=dataset[1],\n",
    "            bounding_box=(min_lon, min_lat, max_lon, max_lat),\n",
    "            count=10\n",
    "        )\n",
    "    except IndexError:\n",
    "        return []\n",
    "    if not results or len(results) == 0:\n",
    "        return []\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
    "        paths = earthaccess.download(results, out_dir)\n",
    "    return paths\n",
    "\n",
    "def download_dem_point(lat, lon, out_dir=\"dem_tiles\", buffer=0.1):\n",
    "    min_lon = max(-180.0, lon - buffer)\n",
    "    max_lon = min(180.0, lon + buffer)\n",
    "    min_lat = max(-90.0, lat - buffer)\n",
    "    max_lat = min(90.0, lat + buffer)\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"SRTMGL1\")\n",
    "    if paths:\n",
    "        return paths, \"SRTM\"\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"COPDEM\")\n",
    "    if paths:\n",
    "        return paths, \"Copernicus\"\n",
    "    return [], \"None\"\n",
    "\n",
    "# ---------------------------\n",
    "# HGT → GeoTIFF\n",
    "# ---------------------------\n",
    "def parse_hgt_bounds(hgt_path):\n",
    "    name = os.path.splitext(os.path.basename(hgt_path))[0]\n",
    "    m = re.match(r'([NS])(\\d{1,2})([EW])(\\d{1,3})', name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse HGT name: {hgt_path}\")\n",
    "    lat_sign = 1 if m.group(1).upper() == 'N' else -1\n",
    "    lon_sign = 1 if m.group(3).upper() == 'E' else -1\n",
    "    import math\n",
    "    lat0 = lat_sign * math.floor(int(m.group(2)))\n",
    "    lon0 = lon_sign * math.floor(int(m.group(4)))\n",
    "    west, south = float(lon0), float(lat0)\n",
    "    east, north = west + 1.0, south + 1.0\n",
    "    return west, south, east, north\n",
    "\n",
    "def hgt_to_gtiff(hgt_path, tif_path):\n",
    "    west, south, east, north = parse_hgt_bounds(hgt_path)\n",
    "    nbytes = os.path.getsize(hgt_path)\n",
    "    side = int(np.sqrt(nbytes // 2))\n",
    "    if side not in (3601, 1201):\n",
    "        raise ValueError(f\"Unexpected HGT side length: {side}\")\n",
    "    data = np.fromfile(hgt_path, dtype=\">i2\").reshape((side, side))\n",
    "    data = data[:-1, :-1]\n",
    "    res = 1.0 / (side - 1)\n",
    "    transform = from_origin(west, north, res, res)\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": data.shape[0],\n",
    "        \"width\": data.shape[1],\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"int16\",\n",
    "        \"crs\": \"EPSG:4326\",\n",
    "        \"transform\": transform,\n",
    "        \"nodata\": -32768,\n",
    "        \"tiled\": True,\n",
    "        \"compress\": \"LZW\"\n",
    "    }\n",
    "    with rasterio.open(tif_path, \"w\", **profile) as dst:\n",
    "        dst.write(data, 1)\n",
    "\n",
    "def prepare_tif(path):\n",
    "    \"\"\"Unpack zip/HGT and convert to GeoTIFF. Remove raw files after processing.\"\"\"\n",
    "    if path.lower().endswith(\".tif\"):\n",
    "        return os.path.abspath(path)\n",
    "    if path.lower().endswith(\".zip\"):\n",
    "        tif_out, hgt_out = None, None\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            tifs = [m for m in z.namelist() if m.lower().endswith(\".tif\")]\n",
    "            if tifs:\n",
    "                tif_out = os.path.join(os.path.dirname(path), os.path.basename(tifs[0]))\n",
    "                if not os.path.exists(tif_out):\n",
    "                    z.extract(tifs[0], os.path.dirname(path))\n",
    "                tif_out = os.path.abspath(tif_out)\n",
    "            else:\n",
    "                hgts = [m for m in z.namelist() if m.lower().endswith(\".hgt\")]\n",
    "                if hgts:\n",
    "                    hgt_out = os.path.join(os.path.dirname(path), os.path.basename(hgts[0]))\n",
    "                    if not os.path.exists(hgt_out):\n",
    "                        z.extract(hgts[0], os.path.dirname(path))\n",
    "                    tif_out = hgt_out.replace(\".hgt\", \".tif\")\n",
    "                    if not os.path.exists(tif_out):\n",
    "                        hgt_to_gtiff(hgt_out, tif_out)\n",
    "                    try:\n",
    "                        os.remove(hgt_out)\n",
    "                    except PermissionError:\n",
    "                        pass\n",
    "                    tif_out = os.path.abspath(tif_out)\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        if tif_out:\n",
    "            return tif_out\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .tif or .hgt in {path}\")\n",
    "    raise FileNotFoundError(f\"Unsupported DEM format: {path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Whitebox + helpers\n",
    "# ---------------------------\n",
    "wbt = WhiteboxTools()\n",
    "wbt.verbose = False\n",
    "\n",
    "def valid_raster(path):\n",
    "    \"\"\"Check if a raster exists, non-empty, and can be opened by rasterio.\"\"\"\n",
    "    if not path or not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        return False\n",
    "    try:\n",
    "        with rasterio.open(path) as src:\n",
    "            _ = src.count\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def run_whitebox(tif_file, need_slope=False, need_aspect=False, need_geomorph=False, slope_dir=None, aspect_dir=None, geomorph_dir=None):\n",
    "    tif_file = os.path.abspath(tif_file).replace(\"\\\\\", \"/\")\n",
    "    base_name = os.path.splitext(os.path.basename(tif_file))[0]\n",
    "    \n",
    "    # Create output paths in subfolders\n",
    "    slope_tif = os.path.join(slope_dir, f\"{base_name}_slope.tif\") if slope_dir else f\"{os.path.splitext(tif_file)[0]}_slope.tif\"\n",
    "    aspect_tif = os.path.join(aspect_dir, f\"{base_name}_aspect.tif\") if aspect_dir else f\"{os.path.splitext(tif_file)[0]}_aspect.tif\"\n",
    "    geomorph_tif = os.path.join(geomorph_dir, f\"{base_name}_geomorph.tif\") if geomorph_dir else f\"{os.path.splitext(tif_file)[0]}_geomorph.tif\"\n",
    "    \n",
    "    if need_slope and not valid_raster(slope_tif):\n",
    "        wbt.slope(dem=tif_file, output=slope_tif, zfactor=1.0, units=\"degrees\")\n",
    "    if need_aspect and not valid_raster(aspect_tif):\n",
    "        wbt.aspect(dem=tif_file, output=aspect_tif)\n",
    "    if need_geomorph and not valid_raster(geomorph_tif):\n",
    "        wbt.geomorphons(dem=tif_file, output=geomorph_tif, search=50, threshold=0.0, forms=True)\n",
    "    return (\n",
    "        tif_file,\n",
    "        slope_tif if valid_raster(slope_tif) else None,\n",
    "        aspect_tif if valid_raster(aspect_tif) else None,\n",
    "        geomorph_tif if valid_raster(geomorph_tif) else None\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# Extract raster value\n",
    "# ---------------------------\n",
    "def extract_value(raster, lat, lon):\n",
    "    if not valid_raster(raster):\n",
    "        return None\n",
    "    try:\n",
    "        with rasterio.open(raster) as src:\n",
    "            nd = src.nodata\n",
    "            for val in src.sample([(lon, lat)]):\n",
    "                v = float(val[0])\n",
    "                if np.isnan(v) or (nd is not None and v == nd):\n",
    "                    return None\n",
    "                return v\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------\n",
    "def enrich_csv(input_csv, output_csv, out_dir=\"dem_tiles\", download_tiles=True, variables=[\"dem\", \"slope\", \"aspect\", \"geomorphons\"], verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced version with detailed error logging and failure tracking.\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Input CSV file path\n",
    "        output_csv: Output CSV file path\n",
    "        out_dir: Directory for DEM tiles\n",
    "        download_tiles: Whether to download DEM tiles (if False, only use existing files)\n",
    "        variables: List of variables to append. Options: [\"dem\", \"slope\", \"aspect\", \"geomorphons\"]\n",
    "        verbose: Print detailed progress and error information\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Setup logging\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        logger = logging.getLogger(__name__)\n",
    "    else:\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.WARNING)\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subfolders for different file types\n",
    "    dem_dir = os.path.join(out_dir, \"dem\")\n",
    "    slope_dir = os.path.join(out_dir, \"slope\")\n",
    "    aspect_dir = os.path.join(out_dir, \"aspect\")\n",
    "    geomorph_dir = os.path.join(out_dir, \"geomorphons\")\n",
    "    \n",
    "    os.makedirs(dem_dir, exist_ok=True)\n",
    "    os.makedirs(slope_dir, exist_ok=True)\n",
    "    os.makedirs(aspect_dir, exist_ok=True)\n",
    "    os.makedirs(geomorph_dir, exist_ok=True)\n",
    "    \n",
    "    # Derive individual boolean flags from variables list\n",
    "    generate_dem = \"dem\" in variables\n",
    "    generate_slope = \"slope\" in variables\n",
    "    generate_aspect = \"aspect\" in variables\n",
    "    generate_geomorphons = \"geomorphons\" in variables\n",
    "    \n",
    "    # Validate variables list\n",
    "    valid_variables = [\"dem\", \"slope\", \"aspect\", \"geomorphons\"]\n",
    "    invalid_vars = [var for var in variables if var not in valid_variables]\n",
    "    if invalid_vars:\n",
    "        raise ValueError(f\"Invalid variables: {invalid_vars}. Valid options are: {valid_variables}\")\n",
    "    \n",
    "    # Validate input file\n",
    "    if not os.path.exists(input_csv):\n",
    "        raise FileNotFoundError(f\"Input CSV file not found: {input_csv}\")\n",
    "    \n",
    "    logger.info(f\"Loading CSV: {input_csv}\")\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df = df[df[\"y\"].between(-56, 60)]\n",
    "\n",
    "    logger.info(f\"Loaded {len(df)} rows\")\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = [\"x\", \"y\"]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Initialize columns based on requested variables\n",
    "    base_cols = []\n",
    "    if generate_dem:\n",
    "        base_cols.extend([\"dem\", \"dem_source\"])\n",
    "    if generate_slope:\n",
    "        base_cols.append(\"slope\")\n",
    "    if generate_aspect:\n",
    "        base_cols.append(\"aspect\")\n",
    "    if generate_geomorphons:\n",
    "        base_cols.extend([\"geomorphon\", \"geomorphon_class\"])\n",
    "    \n",
    "    for col in base_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Add failure tracking columns for requested variables\n",
    "    failure_cols = []\n",
    "    if generate_dem:\n",
    "        failure_cols.append(\"dem_failure\")\n",
    "    if generate_slope:\n",
    "        failure_cols.append(\"slope_failure\")\n",
    "    if generate_aspect:\n",
    "        failure_cols.append(\"aspect_failure\")\n",
    "    if generate_geomorphons:\n",
    "        failure_cols.append(\"geomorphon_failure\")\n",
    "    \n",
    "    for col in failure_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"invalid_coords\": 0,\n",
    "        \"missing_tiles\": 0,\n",
    "        \"extraction_failures\": 0\n",
    "    }\n",
    "    \n",
    "    # Add variable-specific stats based on requested variables\n",
    "    if generate_dem:\n",
    "        stats[\"successful_dem\"] = 0\n",
    "    if generate_slope:\n",
    "        stats[\"successful_slope\"] = 0\n",
    "    if generate_aspect:\n",
    "        stats[\"successful_aspect\"] = 0\n",
    "    if generate_geomorphons:\n",
    "        stats[\"successful_geomorphon\"] = 0\n",
    "\n",
    "    # Step 1: Collect per-tile needs\n",
    "    logger.info(\"Step 1: Scanning CSV for tile requirements...\")\n",
    "    tile_needs = {}\n",
    "    invalid_coord_rows = []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Scanning CSV\"):\n",
    "        lat, lon = row[\"y\"], row[\"x\"]\n",
    "        \n",
    "        # Check for invalid coordinates\n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            invalid_coord_rows.append(i)\n",
    "            df.at[i, \"dem_failure\"] = \"Invalid coordinates: NaN values\"\n",
    "            stats[\"invalid_coords\"] += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            tid = tile_id_from_coords(lat, lon)\n",
    "            if tid is None:\n",
    "                invalid_coord_rows.append(i)\n",
    "                df.at[i, \"dem_failure\"] = f\"Invalid coordinates: lat={lat}, lon={lon}\"\n",
    "                stats[\"invalid_coords\"] += 1\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            invalid_coord_rows.append(i)\n",
    "            df.at[i, \"dem_failure\"] = f\"Coordinate conversion error: {str(e)}\"\n",
    "            stats[\"invalid_coords\"] += 1\n",
    "            continue\n",
    "            \n",
    "        base = os.path.join(dem_dir, tid)\n",
    "        slope_file = os.path.join(slope_dir, f\"{tid}_slope.tif\")\n",
    "        aspect_file = os.path.join(aspect_dir, f\"{tid}_aspect.tif\")\n",
    "        geomorph_file = os.path.join(geomorph_dir, f\"{tid}_geomorph.tif\")\n",
    "        \n",
    "        if tid not in tile_needs:\n",
    "            tile_needs[tid] = {\"dem\": False, \"slope\": False, \"aspect\": False, \"geomorphon\": False}\n",
    "            \n",
    "        if generate_dem and pd.isna(row.get(\"dem\")) and not valid_raster(f\"{base}.tif\"):\n",
    "            tile_needs[tid][\"dem\"] = True\n",
    "        if generate_slope and pd.isna(row.get(\"slope\")) and not valid_raster(slope_file):\n",
    "            tile_needs[tid][\"slope\"] = True\n",
    "        if generate_aspect and pd.isna(row.get(\"aspect\")) and not valid_raster(aspect_file):\n",
    "            tile_needs[tid][\"aspect\"] = True\n",
    "        if generate_geomorphons and pd.isna(row.get(\"geomorphon\")) and not valid_raster(geomorph_file):\n",
    "            tile_needs[tid][\"geomorphon\"] = True\n",
    "            \n",
    "    tile_needs = {tid: needs for tid, needs in tile_needs.items() if any(needs.values())}\n",
    "    logger.info(f\"Found {len(tile_needs)} tiles that need processing\")\n",
    "\n",
    "    # Step 2 & 3: Only run if download_tiles is True\n",
    "    downloaded = {}\n",
    "    tile_results = {}\n",
    "    if download_tiles:\n",
    "        logger.info(\"Step 2: Downloading and preparing tiles...\")\n",
    "        for tid, needs in tqdm(tile_needs.items(), desc=\"Preparing tiles\"):\n",
    "            local_tif = os.path.join(dem_dir, f\"{tid}.tif\")\n",
    "            if valid_raster(local_tif):\n",
    "                downloaded[tid] = ([local_tif], \"Local\")\n",
    "                continue\n",
    "                \n",
    "            m = re.match(r'([NS])(\\d{2})([EW])(\\d{3})', tid)\n",
    "            if not m:\n",
    "                logger.warning(f\"Could not parse tile ID: {tid}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                lat0 = int(m.group(2)) * (1 if m.group(1) == \"N\" else -1)\n",
    "                lon0 = int(m.group(4)) * (1 if m.group(3) == \"E\" else -1)\n",
    "                zip_paths, source = download_dem_point(lat0 + 0.5, lon0 + 0.5, out_dir=out_dir)\n",
    "                if zip_paths:\n",
    "                    tifs = [prepare_tif(zp) for zp in zip_paths]\n",
    "                    # Move processed files to dem subfolder\n",
    "                    moved_tifs = []\n",
    "                    for tif in tifs:\n",
    "                        target_path = os.path.join(dem_dir, f\"{tid}.tif\")\n",
    "                        if tif != target_path:\n",
    "                            import shutil\n",
    "                            shutil.move(tif, target_path)\n",
    "                        moved_tifs.append(target_path)\n",
    "                    downloaded[tid] = (moved_tifs, source)\n",
    "                else:\n",
    "                    logger.warning(f\"No DEM data available for tile {tid}\")\n",
    "                    stats[\"missing_tiles\"] += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error downloading tile {tid}: {str(e)}\")\n",
    "                stats[\"missing_tiles\"] += 1\n",
    "\n",
    "        logger.info(\"Step 3: Running Whitebox processing...\")\n",
    "        for tid, (tifs, source) in tqdm(downloaded.items(), desc=\"Running Whitebox\"):\n",
    "            needs = tile_needs.get(tid, {})\n",
    "            for tif in tifs:\n",
    "                try:\n",
    "                    tif_path, slope_path, aspect_path, geomorph_path = run_whitebox(\n",
    "                        tif,\n",
    "                        need_slope=generate_slope and needs.get(\"slope\", False),\n",
    "                        need_aspect=generate_aspect and needs.get(\"aspect\", False),\n",
    "                        need_geomorph=generate_geomorphons and needs.get(\"geomorphon\", False),\n",
    "                        slope_dir=slope_dir,\n",
    "                        aspect_dir=aspect_dir,\n",
    "                        geomorph_dir=geomorph_dir\n",
    "                    )\n",
    "                    tile_results[tid] = {\n",
    "                        \"tif\": tif_path,\n",
    "                        \"slope\": slope_path,\n",
    "                        \"aspect\": aspect_path,\n",
    "                        \"geomorphon\": geomorph_path,\n",
    "                        \"source\": source\n",
    "                    }\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Whitebox processing failed for tile {tid}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    # Step 4: Extract values from whatever exists\n",
    "    logger.info(\"Step 4: Extracting values from rasters...\")\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting values\"):\n",
    "        # Skip rows with invalid coordinates\n",
    "        if i in invalid_coord_rows:\n",
    "            continue\n",
    "            \n",
    "        lat, lon = row[\"y\"], row[\"x\"]\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid is None:\n",
    "            continue\n",
    "            \n",
    "        dem_base = os.path.join(dem_dir, tid)\n",
    "        slope_base = os.path.join(slope_dir, f\"{tid}_slope.tif\")\n",
    "        aspect_base = os.path.join(aspect_dir, f\"{tid}_aspect.tif\")\n",
    "        geomorph_base = os.path.join(geomorph_dir, f\"{tid}_geomorph.tif\")\n",
    "        \n",
    "        tr = tile_results.get(tid, {\n",
    "            \"tif\": f\"{dem_base}.tif\" if valid_raster(f\"{dem_base}.tif\") else None,\n",
    "            \"slope\": slope_base if valid_raster(slope_base) else None,\n",
    "            \"aspect\": aspect_base if valid_raster(aspect_base) else None,\n",
    "            \"geomorphon\": geomorph_base if valid_raster(geomorph_base) else None,\n",
    "            \"source\": \"Local\"\n",
    "        })\n",
    "        \n",
    "        # Extract DEM value\n",
    "        if generate_dem and pd.isna(row.get(\"dem\")) and tr[\"tif\"]:\n",
    "            try:\n",
    "                dem_value = extract_value(tr[\"tif\"], lat, lon)\n",
    "                if dem_value is not None:\n",
    "                    df.at[i, \"dem\"] = dem_value\n",
    "                    df.at[i, \"dem_source\"] = tr[\"source\"]\n",
    "                    stats[\"successful_dem\"] += 1\n",
    "                else:\n",
    "                    df.at[i, \"dem_failure\"] = \"No data value or out of bounds\"\n",
    "            except Exception as e:\n",
    "                df.at[i, \"dem_failure\"] = f\"Extraction error: {str(e)}\"\n",
    "                stats[\"extraction_failures\"] += 1\n",
    "        elif generate_dem and pd.isna(row.get(\"dem\")):\n",
    "            df.at[i, \"dem_failure\"] = \"No DEM raster available\"\n",
    "        \n",
    "        # Extract slope value\n",
    "        if generate_slope and pd.isna(row.get(\"slope\")) and tr[\"slope\"]:\n",
    "            try:\n",
    "                slope_value = extract_value(tr[\"slope\"], lat, lon)\n",
    "                if slope_value is not None:\n",
    "                    df.at[i, \"slope\"] = slope_value\n",
    "                    stats[\"successful_slope\"] += 1\n",
    "                else:\n",
    "                    df.at[i, \"slope_failure\"] = \"No data value or out of bounds\"\n",
    "            except Exception as e:\n",
    "                df.at[i, \"slope_failure\"] = f\"Extraction error: {str(e)}\"\n",
    "                stats[\"extraction_failures\"] += 1\n",
    "        elif generate_slope and pd.isna(row.get(\"slope\")):\n",
    "            df.at[i, \"slope_failure\"] = \"No slope raster available\"\n",
    "        \n",
    "        # Extract aspect value\n",
    "        if generate_aspect and pd.isna(row.get(\"aspect\")) and tr[\"aspect\"]:\n",
    "            try:\n",
    "                aspect_value = extract_value(tr[\"aspect\"], lat, lon)\n",
    "                if aspect_value is not None:\n",
    "                    df.at[i, \"aspect\"] = aspect_value\n",
    "                    stats[\"successful_aspect\"] += 1\n",
    "                else:\n",
    "                    df.at[i, \"aspect_failure\"] = \"No data value or out of bounds\"\n",
    "            except Exception as e:\n",
    "                df.at[i, \"aspect_failure\"] = f\"Extraction error: {str(e)}\"\n",
    "                stats[\"extraction_failures\"] += 1\n",
    "        elif generate_aspect and pd.isna(row.get(\"aspect\")):\n",
    "            df.at[i, \"aspect_failure\"] = \"No aspect raster available\"\n",
    "        \n",
    "        # Extract geomorphon value\n",
    "        if generate_geomorphons and pd.isna(row.get(\"geomorphon\")) and tr[\"geomorphon\"]:\n",
    "            try:\n",
    "                geomorph_value = extract_value(tr[\"geomorphon\"], lat, lon)\n",
    "                if geomorph_value is not None:\n",
    "                    df.at[i, \"geomorphon\"] = geomorph_value\n",
    "                    stats[\"successful_geomorphon\"] += 1\n",
    "                else:\n",
    "                    df.at[i, \"geomorphon_failure\"] = \"No data value or out of bounds\"\n",
    "            except Exception as e:\n",
    "                df.at[i, \"geomorphon_failure\"] = f\"Extraction error: {str(e)}\"\n",
    "                stats[\"extraction_failures\"] += 1\n",
    "        elif generate_geomorphons and pd.isna(row.get(\"geomorphon\")):\n",
    "            df.at[i, \"geomorphon_failure\"] = \"No geomorphon raster available\"\n",
    "\n",
    "    # Step 5: Decode geomorphons (only if geomorphons were generated)\n",
    "    if generate_geomorphons:\n",
    "        logger.info(\"Step 5: Decoding geomorphon classes...\")\n",
    "        geomorph_classes = {\n",
    "            1: \"flat\", 2: \"summit\", 3: \"ridge\", 4: \"shoulder\", 5: \"spur\",\n",
    "            6: \"slope\", 7: \"hollow\", 8: \"footslope\", 9: \"valley\", 10: \"pit\"\n",
    "        }\n",
    "        df[\"geomorphon_class\"] = df[\"geomorphon\"].map(geomorph_classes)\n",
    "\n",
    "    # Save results\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(\"PROCESSING SUMMARY\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"Total rows processed: {stats['total_rows']}\")\n",
    "    logger.info(f\"Invalid coordinates: {stats['invalid_coords']}\")\n",
    "    logger.info(f\"Missing tiles: {stats['missing_tiles']}\")\n",
    "    logger.info(f\"Extraction failures: {stats['extraction_failures']}\")\n",
    "    \n",
    "    # Show stats only for requested variables\n",
    "    if generate_dem:\n",
    "        logger.info(f\"Successful DEM extractions: {stats['successful_dem']}\")\n",
    "    if generate_slope:\n",
    "        logger.info(f\"Successful slope extractions: {stats['successful_slope']}\")\n",
    "    if generate_aspect:\n",
    "        logger.info(f\"Successful aspect extractions: {stats['successful_aspect']}\")\n",
    "    if generate_geomorphons:\n",
    "        logger.info(f\"Successful geomorphon extractions: {stats['successful_geomorphon']}\")\n",
    "    \n",
    "    logger.info(f\"✅ Done! Saved {output_csv}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    enrich_csv(\n",
    "        \"data/negative_samples_within_land_10k_with_coords.csv\",\n",
    "        \"data/negative_samples_within_land_10k_with_coords_topography.csv\",\n",
    "        out_dir=\"dem_tiles\",  # Files will be organized in subfolders: dem/, slope/, aspect/, geomorphons/\n",
    "        download_tiles=False,  # 🚨 Set to True if you want to download new tiles\n",
    "        variables=[\"dem\", \"slope\", \"aspect\", \"geomorphons\"]  # Specify which variables to append\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append to vector file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from whitebox.whitebox_tools import WhiteboxTools\n",
    "import earthaccess\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "import io\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "# ---------------------------\n",
    "# Tile ID utilities\n",
    "# ---------------------------\n",
    "def tile_id_from_coords(lat, lon):\n",
    "    \"\"\"Convert coords to tile ID (e.g. N40W106).\"\"\"\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return None\n",
    "    ns = \"N\" if lat >= 0 else \"S\"\n",
    "    ew = \"E\" if lon >= 0 else \"W\"\n",
    "    lat_tile = math.floor(lat)\n",
    "    lon_tile = math.floor(lon)\n",
    "    return f\"{ns}{abs(lat_tile):02d}{ew}{abs(lon_tile):03d}\"\n",
    "\n",
    "# ---------------------------\n",
    "# DEM Download\n",
    "# ---------------------------\n",
    "def download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=\"dem_tiles\", prefer=\"SRTMGL1\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    earthaccess.login(strategy=\"environment\", persist=True)\n",
    "    dataset = (\"SRTMGL1\", \"003\") if prefer == \"SRTMGL1\" else (\"COPDEM_GLO_30\", \"001\")\n",
    "    try:\n",
    "        results = earthaccess.search_data(\n",
    "            short_name=dataset[0],\n",
    "            version=dataset[1],\n",
    "            bounding_box=(min_lon, min_lat, max_lon, max_lat),\n",
    "            count=10\n",
    "        )\n",
    "    except IndexError:\n",
    "        return []\n",
    "    if not results or len(results) == 0:\n",
    "        return []\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
    "        paths = earthaccess.download(results, out_dir)\n",
    "    return paths\n",
    "\n",
    "def download_dem_point(lat, lon, out_dir=\"dem_tiles\", buffer=0.1):\n",
    "    # Clamp bbox\n",
    "    min_lon = max(-180.0, lon - buffer)\n",
    "    max_lon = min(180.0, lon + buffer)\n",
    "    min_lat = max(-90.0, lat - buffer)\n",
    "    max_lat = min(90.0, lat + buffer)\n",
    "\n",
    "    # Try SRTM first (only valid between -56 and +60 lat)\n",
    "    if -56 <= lat <= 60:\n",
    "        paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"SRTMGL1\")\n",
    "        if paths:\n",
    "            return paths, \"SRTM\"\n",
    "\n",
    "    # Fallback: Copernicus global DEM\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"COPDEM\")\n",
    "    if paths:\n",
    "        return paths, \"Copernicus\"\n",
    "    return [], \"None\"\n",
    "\n",
    "# ---------------------------\n",
    "# HGT → GeoTIFF\n",
    "# ---------------------------\n",
    "def parse_hgt_bounds(hgt_path):\n",
    "    name = os.path.splitext(os.path.basename(hgt_path))[0]\n",
    "    m = re.match(r'([NS])(\\d{1,2})([EW])(\\d{1,3})', name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse HGT name: {hgt_path}\")\n",
    "    lat_sign = 1 if m.group(1).upper() == 'N' else -1\n",
    "    lon_sign = 1 if m.group(3).upper() == 'E' else -1\n",
    "    lat0 = lat_sign * int(m.group(2))\n",
    "    lon0 = lon_sign * int(m.group(4))\n",
    "    west, south = float(lon0), float(lat0)\n",
    "    east, north = west + 1.0, south + 1.0\n",
    "    return west, south, east, north\n",
    "\n",
    "def hgt_to_gtiff(hgt_path, tif_path):\n",
    "    west, south, east, north = parse_hgt_bounds(hgt_path)\n",
    "    nbytes = os.path.getsize(hgt_path)\n",
    "    side = int(np.sqrt(nbytes // 2))\n",
    "    if side not in (3601, 1201):\n",
    "        raise ValueError(f\"Unexpected HGT side length: {side}\")\n",
    "    data = np.fromfile(hgt_path, dtype=\">i2\").reshape((side, side))\n",
    "    data = data[:-1, :-1]\n",
    "    res = 1.0 / (side - 1)\n",
    "    transform = from_origin(west, north, res, res)\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": data.shape[0],\n",
    "        \"width\": data.shape[1],\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"int16\",\n",
    "        \"crs\": \"EPSG:4326\",\n",
    "        \"transform\": transform,\n",
    "        \"nodata\": -32768,\n",
    "        \"tiled\": True,\n",
    "        \"compress\": \"LZW\"\n",
    "    }\n",
    "    with rasterio.open(tif_path, \"w\", **profile) as dst:\n",
    "        dst.write(data, 1)\n",
    "\n",
    "def prepare_tif(path):\n",
    "    \"\"\"Unpack zip/HGT and convert to GeoTIFF. Remove raw files after processing.\"\"\"\n",
    "    if path.lower().endswith(\".tif\"):\n",
    "        return os.path.abspath(path)\n",
    "    if path.lower().endswith(\".zip\"):\n",
    "        tif_out, hgt_out = None, None\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            tifs = [m for m in z.namelist() if m.lower().endswith(\".tif\")]\n",
    "            if tifs:\n",
    "                tif_out = os.path.join(os.path.dirname(path), os.path.basename(tifs[0]))\n",
    "                if not os.path.exists(tif_out):\n",
    "                    z.extract(tifs[0], os.path.dirname(path))\n",
    "                tif_out = os.path.abspath(tif_out)\n",
    "            else:\n",
    "                hgts = [m for m in z.namelist() if m.lower().endswith(\".hgt\")]\n",
    "                if hgts:\n",
    "                    hgt_out = os.path.join(os.path.dirname(path), os.path.basename(hgts[0]))\n",
    "                    if not os.path.exists(hgt_out):\n",
    "                        z.extract(hgts[0], os.path.dirname(path))\n",
    "                    tif_out = hgt_out.replace(\".hgt\", \".tif\")\n",
    "                    if not os.path.exists(tif_out):\n",
    "                        hgt_to_gtiff(hgt_out, tif_out)\n",
    "                    try:\n",
    "                        os.remove(hgt_out)\n",
    "                    except PermissionError:\n",
    "                        pass\n",
    "                    tif_out = os.path.abspath(tif_out)\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        if tif_out:\n",
    "            return tif_out\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .tif or .hgt in {path}\")\n",
    "    raise FileNotFoundError(f\"Unsupported DEM format: {path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Whitebox + helpers\n",
    "# ---------------------------\n",
    "wbt = WhiteboxTools()\n",
    "wbt.verbose = False\n",
    "\n",
    "def valid_raster(path):\n",
    "    if not path or not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        return False\n",
    "    try:\n",
    "        with rasterio.open(path) as src:\n",
    "            _ = src.count\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def run_whitebox(tif_file, slope_dir, aspect_dir, geomorph_dir,\n",
    "                 need_slope=True, need_aspect=True, need_geomorph=True):\n",
    "    tif_file = os.path.abspath(tif_file).replace(\"\\\\\", \"/\")\n",
    "    base_name = os.path.splitext(os.path.basename(tif_file))[0]\n",
    "\n",
    "    slope_tif = os.path.join(slope_dir, f\"{base_name}_slope.tif\")\n",
    "    aspect_tif = os.path.join(aspect_dir, f\"{base_name}_aspect.tif\")\n",
    "    geomorph_tif = os.path.join(geomorph_dir, f\"{base_name}_geomorph.tif\")\n",
    "\n",
    "    if need_slope and not valid_raster(slope_tif):\n",
    "        wbt.slope(dem=tif_file, output=slope_tif, zfactor=1.0, units=\"degrees\")\n",
    "    if need_aspect and not valid_raster(aspect_tif):\n",
    "        wbt.aspect(dem=tif_file, output=aspect_tif)\n",
    "    if need_geomorph and not valid_raster(geomorph_tif):\n",
    "        wbt.geomorphons(dem=tif_file, output=geomorph_tif, search=50, threshold=0.0, forms=True)\n",
    "\n",
    "    return tif_file, slope_tif, aspect_tif, geomorph_tif\n",
    "\n",
    "# ---------------------------\n",
    "# Extract raster value\n",
    "# ---------------------------\n",
    "def extract_value(raster, lat, lon):\n",
    "    if not valid_raster(raster):\n",
    "        return None\n",
    "    try:\n",
    "        with rasterio.open(raster) as src:\n",
    "            nd = src.nodata\n",
    "            for val in src.sample([(lon, lat)]):\n",
    "                v = float(val[0])\n",
    "                if np.isnan(v) or (nd is not None and v == nd):\n",
    "                    return None\n",
    "                return v\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline for GeoJSON\n",
    "# ---------------------------\n",
    "def enrich_geojson(input_geojson, output_geojson, out_dir=\"dem_tiles\", download_tiles=True):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Create subfolders\n",
    "    dem_dir = os.path.join(out_dir, \"dem\")\n",
    "    slope_dir = os.path.join(out_dir, \"slope\")\n",
    "    aspect_dir = os.path.join(out_dir, \"aspect\")\n",
    "    geomorph_dir = os.path.join(out_dir, \"geomorphons\")\n",
    "    for d in [dem_dir, slope_dir, aspect_dir, geomorph_dir]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    gdf = gpd.read_file(input_geojson)\n",
    "\n",
    "    # Ensure WGS84\n",
    "    if gdf.crs is None:\n",
    "        print(\"⚠️ No CRS found, assuming EPSG:4326\")\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "    else:\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # Filter SRTM coverage (lat -56 to 60)\n",
    "    gdf = gdf[gdf.geometry.centroid.y.between(-56, 60)]\n",
    "\n",
    "    # Add expected cols\n",
    "    for col in [\"dem\", \"slope\", \"aspect\", \"geomorphon\", \"dem_source\", \"geomorphon_class\"]:\n",
    "        if col not in gdf.columns:\n",
    "            gdf[col] = None\n",
    "\n",
    "    # Collect centroids\n",
    "    centroids = gdf.geometry.centroid\n",
    "    coords = [(pt.y, pt.x) for pt in centroids]\n",
    "\n",
    "    # Step 1: collect tiles\n",
    "    needed_tiles = {}\n",
    "    for (lat, lon) in tqdm(coords, desc=\"Collecting tiles\"):\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid and tid not in needed_tiles:\n",
    "            needed_tiles[tid] = (lat, lon)\n",
    "\n",
    "    # Step 2: download & prepare tiles\n",
    "    downloaded = {}\n",
    "    for tid, (lat, lon) in tqdm(needed_tiles.items(), desc=\"Preparing tiles\"):\n",
    "        tif_path = os.path.join(dem_dir, f\"{tid}.tif\")\n",
    "        if valid_raster(tif_path):\n",
    "            downloaded[tid] = ([tif_path], \"Local\")\n",
    "        elif download_tiles:\n",
    "            zip_paths, source = download_dem_point(lat, lon, out_dir=out_dir)\n",
    "            if zip_paths:\n",
    "                tifs = [prepare_tif(zp) for zp in zip_paths]\n",
    "                moved_tifs = []\n",
    "                for tif in tifs:\n",
    "                    target = os.path.join(dem_dir, f\"{tid}.tif\")\n",
    "                    if tif != target:\n",
    "                        shutil.move(tif, target)\n",
    "                    moved_tifs.append(target)\n",
    "                downloaded[tid] = (moved_tifs, source)\n",
    "\n",
    "    # Step 3: run Whitebox\n",
    "    tile_results = {}\n",
    "    for tid, (tifs, source) in tqdm(downloaded.items(), desc=\"Running Whitebox\"):\n",
    "        for tif in tifs:\n",
    "            tif_path, slope_tif, aspect_tif, geomorph_tif = run_whitebox(\n",
    "                tif, slope_dir, aspect_dir, geomorph_dir)\n",
    "            tile_results[tid] = (tif_path, slope_tif, aspect_tif, geomorph_tif, source)\n",
    "\n",
    "    # Step 4: extract values\n",
    "    geomorph_classes = {\n",
    "        1: \"flat\", 2: \"summit\", 3: \"ridge\", 4: \"shoulder\", 5: \"spur\",\n",
    "        6: \"slope\", 7: \"hollow\", 8: \"footslope\", 9: \"valley\", 10: \"pit\"\n",
    "    }\n",
    "\n",
    "    for idx, (lat, lon) in enumerate(tqdm(coords, desc=\"Extracting values\")):\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid is None or tid not in tile_results:\n",
    "            continue\n",
    "        tif, slope_tif, aspect_tif, geomorph_tif, source = tile_results[tid]\n",
    "        gdf.at[idx, \"dem\"] = extract_value(tif, lat, lon)\n",
    "        gdf.at[idx, \"dem_source\"] = source\n",
    "        gdf.at[idx, \"slope\"] = extract_value(slope_tif, lat, lon)\n",
    "        gdf.at[idx, \"aspect\"] = extract_value(aspect_tif, lat, lon)\n",
    "        gdf.at[idx, \"geomorphon\"] = extract_value(geomorph_tif, lat, lon)\n",
    "        gdf.at[idx, \"geomorphon_class\"] = geomorph_classes.get(gdf.at[idx, \"geomorphon\"], None)\n",
    "\n",
    "    # Save enriched GeoJSON\n",
    "    gdf.to_file(output_geojson, driver=\"GeoJSON\")\n",
    "    print(f\"✅ Done! Saved {output_geojson}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    enrich_geojson(\n",
    "        \"data/grid_tuscany_forest.geojson\",\n",
    "        \"data/grid_tuscany_with_topography.geojson\",\n",
    "        out_dir=\"dem_tiles\",\n",
    "        download_tiles=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading DEM tiles to Google Cloud Storage...\n",
      "Found 15375 .tif files to upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 1/15375 [00:00<2:44:39,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E009.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 2/15375 [00:01<2:23:46,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E010.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 3/15375 [00:01<2:35:37,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E011.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 4/15375 [00:03<4:12:19,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E011_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 5/15375 [00:04<4:51:33,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E011_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 6/15375 [00:05<4:10:28,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E012.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 7/15375 [00:05<3:33:35,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E013.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 8/15375 [00:07<4:54:20,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E013_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 9/15375 [00:09<6:05:45,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E013_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 10/15375 [00:10<4:56:59,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E014.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 11/15375 [00:10<4:07:15,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E015.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 12/15375 [00:11<3:20:43,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E016.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 13/15375 [00:13<4:41:31,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E016_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 14/15375 [00:14<5:19:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E016_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 15/15375 [00:15<4:11:11,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E017.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 16/15375 [00:15<3:21:53,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E018.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 17/15375 [00:16<4:12:59,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E018_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 18/15375 [00:18<4:50:52,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E018_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 19/15375 [00:18<3:49:28,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E019.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 20/15375 [00:20<4:27:03,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E019_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 21/15375 [00:21<4:51:52,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E019_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 22/15375 [00:21<3:57:52,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E020.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 23/15375 [00:22<3:20:27,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E021.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 24/15375 [00:23<3:59:58,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E021_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 25/15375 [00:24<4:24:55,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E021_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 26/15375 [00:25<3:37:54,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E023.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 27/15375 [00:26<4:03:55,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E023_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 28/15375 [00:28<4:52:39,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E023_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 29/15375 [00:28<3:58:20,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E024.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 30/15375 [00:29<4:34:43,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E024_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 31/15375 [00:31<4:55:34,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E024_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 32/15375 [00:31<4:01:24,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E025.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 33/15375 [00:33<4:32:05,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E025_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 34/15375 [00:34<4:52:09,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E025_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 35/15375 [00:34<4:02:11,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E026.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 36/15375 [00:35<3:23:41,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E027.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 37/15375 [00:37<4:32:58,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E027_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 38/15375 [00:38<5:06:08,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E027_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 39/15375 [00:39<4:15:28,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E028.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 40/15375 [00:39<3:39:20,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E030.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 41/15375 [00:40<3:11:55,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E031.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 42/15375 [00:40<2:48:52,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E032.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 43/15375 [00:42<3:58:16,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E032_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 44/15375 [00:43<4:53:57,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E032_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 45/15375 [00:44<3:55:07,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E033.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 46/15375 [00:44<3:25:49,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E037.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 47/15375 [00:46<4:20:17,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E037_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 48/15375 [00:47<5:05:44,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E037_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 49/15375 [00:48<4:08:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E038.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 50/15375 [00:48<3:18:26,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E039.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 51/15375 [00:50<4:07:13,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E039_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 52/15375 [00:51<4:38:48,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E039_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 53/15375 [00:51<3:52:08,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E101.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 54/15375 [00:53<4:31:56,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E101_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 55/15375 [00:54<4:47:54,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E101_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 56/15375 [00:55<3:58:59,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E110.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 57/15375 [00:56<4:41:17,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E110_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 58/15375 [00:57<5:07:06,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E110_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 59/15375 [00:58<4:12:52,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E112.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 60/15375 [00:59<4:40:33,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E112_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 61/15375 [01:01<4:55:51,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E112_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 62/15375 [01:01<4:13:38,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E113.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 63/15375 [01:03<4:41:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E113_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 64/15375 [01:04<5:01:47,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E113_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 65/15375 [01:04<4:04:17,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E116.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 66/15375 [01:06<4:30:49,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E116_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 67/15375 [01:07<5:12:05,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E116_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 68/15375 [01:08<4:20:26,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E117.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 69/15375 [01:09<5:06:13,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E117_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 70/15375 [01:11<5:31:29,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E117_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 71/15375 [01:11<4:14:02,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E119.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 72/15375 [01:13<4:52:23,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E119_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 73/15375 [01:14<5:12:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E119_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 74/15375 [01:14<4:00:39,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E127.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 75/15375 [01:16<4:36:22,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E127_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 76/15375 [01:17<5:05:27,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E127_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 77/15375 [01:18<3:58:15,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E128.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 78/15375 [01:19<4:29:30,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E128_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 79/15375 [01:20<4:54:28,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E128_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 80/15375 [01:21<4:08:22,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W053.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 81/15375 [01:22<4:41:40,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W053_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 82/15375 [01:24<4:59:17,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W053_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 83/15375 [01:24<4:10:08,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W054.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 84/15375 [01:26<4:38:33,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W054_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 85/15375 [01:27<5:24:46,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W054_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 86/15375 [01:28<4:35:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W055.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 87/15375 [01:29<4:07:19,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W056.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 88/15375 [01:30<5:06:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W056_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 89/15375 [01:32<5:42:27,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W056_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 90/15375 [01:33<4:42:03,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W059.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 91/15375 [01:34<5:16:20,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W059_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 91/15375 [01:35<4:26:09,  1.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 157\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Upload DEM tiles to GCS\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading DEM tiles to Google Cloud Storage...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m \u001b[43mupload_dem_tiles_to_gcs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdem_tiles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBUCKET_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdem_tiles/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# List files in bucket\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mListing files in bucket...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mupload_dem_tiles_to_gcs\u001b[1;34m(local_dir, bucket_name, gcs_prefix)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Create blob and upload\u001b[39;00m\n\u001b[0;32m     43\u001b[0m blob \u001b[38;5;241m=\u001b[39m bucket\u001b[38;5;241m.\u001b[39mblob(blob_name)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_from_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m uploaded_files\u001b[38;5;241m.\u001b[39mappend(blob_name)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Uploaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:3013\u001b[0m, in \u001b[0;36mBlob.upload_from_filename\u001b[1;34m(self, filename, content_type, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Upload this blob's contents from the content of a named file.\u001b[39;00m\n\u001b[0;32m   2919\u001b[0m \n\u001b[0;32m   2920\u001b[0m \u001b[38;5;124;03mThe content type of the upload will be determined in order\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3010\u001b[0m \u001b[38;5;124;03m    to configure them.\u001b[39;00m\n\u001b[0;32m   3011\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m create_trace_span(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorage.Blob.uploadFromFilename\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 3013\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_filename_and_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:2896\u001b[0m, in \u001b[0;36mBlob._handle_filename_and_upload\u001b[1;34m(self, filename, content_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2894\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[0;32m   2895\u001b[0m     total_bytes \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfstat(file_obj\u001b[38;5;241m.\u001b[39mfileno())\u001b[38;5;241m.\u001b[39mst_size\n\u001b[1;32m-> 2896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_and_do_upload(\n\u001b[0;32m   2897\u001b[0m         file_obj,\n\u001b[0;32m   2898\u001b[0m         content_type\u001b[38;5;241m=\u001b[39mcontent_type,\n\u001b[0;32m   2899\u001b[0m         size\u001b[38;5;241m=\u001b[39mtotal_bytes,\n\u001b[0;32m   2900\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   2901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2902\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:2718\u001b[0m, in \u001b[0;36mBlob._prep_and_do_upload\u001b[1;34m(self, file_obj, rewind, size, content_type, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[0;32m   2715\u001b[0m predefined_acl \u001b[38;5;241m=\u001b[39m ACL\u001b[38;5;241m.\u001b[39mvalidate_predefined(predefined_acl)\n\u001b[0;32m   2717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2718\u001b[0m     created_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2722\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2730\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2732\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(created_json)\n\u001b[0;32m   2734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:2563\u001b[0m, in \u001b[0;36mBlob._do_upload\u001b[1;34m(self, client, stream, content_type, size, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[0;32m   2547\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_multipart_upload(\n\u001b[0;32m   2548\u001b[0m         client,\n\u001b[0;32m   2549\u001b[0m         stream,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2560\u001b[0m         command\u001b[38;5;241m=\u001b[39mcommand,\n\u001b[0;32m   2561\u001b[0m     )\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2563\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_resumable_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2567\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:2413\u001b[0m, in \u001b[0;36mBlob._do_resumable_upload\u001b[1;34m(self, client, stream, content_type, size, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[0;32m   2411\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m upload\u001b[38;5;241m.\u001b[39mfinished:\n\u001b[0;32m   2412\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2413\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mupload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransmit_next_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DataCorruption:\n\u001b[0;32m   2415\u001b[0m         \u001b[38;5;66;03m# Attempt to delete the corrupted object.\u001b[39;00m\n\u001b[0;32m   2416\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete()\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\upload.py:529\u001b[0m, in \u001b[0;36mResumableUpload.transmit_next_chunk\u001b[1;34m(self, transport, timeout)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_resumable_response(result, \u001b[38;5;28mlen\u001b[39m(payload))\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_strategy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\_request_helpers.py:107\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[1;34m(func, retry_strategy)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retry_strategy:\n\u001b[0;32m    106\u001b[0m     func \u001b[38;5;241m=\u001b[39m retry_strategy(func)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\upload.py:521\u001b[0m, in \u001b[0;36mResumableUpload.transmit_next_chunk.<locals>.retriable_request\u001b[1;34m()\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretriable_request\u001b[39m():\n\u001b[1;32m--> 521\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_resumable_response(result, \u001b[38;5;28mlen\u001b[39m(payload))\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\auth\\transport\\requests.py:540\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[1;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[0;32m    539\u001b[0m     _helpers\u001b[38;5;241m.\u001b[39mrequest_log(_LOGGER, method, url, data, headers)\n\u001b[1;32m--> 540\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(AuthorizedSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    541\u001b[0m         method,\n\u001b[0;32m    542\u001b[0m         url,\n\u001b[0;32m    543\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    544\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[0;32m    545\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    547\u001b[0m     )\n\u001b[0;32m    548\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\urllib3\\connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\urllib3\\connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    414\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest_chunked(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\urllib3\\connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (six\u001b[38;5;241m.\u001b[39mensure_str(k\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m headers):\n\u001b[0;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1283\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1328\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1079\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encode_chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_vsn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m11\u001b[39m:\n\u001b[0;32m   1076\u001b[0m         \u001b[38;5;66;03m# chunked encoding\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124mX\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m chunk \\\n\u001b[0;32m   1078\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1079\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encode_chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_vsn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m11\u001b[39m:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# end chunked transfer\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1001\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\ssl.py:1238\u001b[0m, in \u001b[0;36mSSLSocket.sendall\u001b[1;34m(self, data, flags)\u001b[0m\n\u001b[0;32m   1236\u001b[0m         amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(byte_view)\n\u001b[0;32m   1237\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m amount:\n\u001b[1;32m-> 1238\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m             count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\ssl.py:1207\u001b[0m, in \u001b[0;36mSSLSocket.send\u001b[1;34m(self, data, flags)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1205\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to send() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1206\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(data, flags)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Upload DEM tiles to Google Cloud Storage\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "def upload_dem_tiles_to_gcs(local_dir=\"dem_tiles\", bucket_name=\"your-bucket-name\", gcs_prefix=\"dem_tiles/\"):\n",
    "    \"\"\"\n",
    "    Upload all DEM tiles from local directory to Google Cloud Storage bucket.\n",
    "    \n",
    "    Args:\n",
    "        local_dir: Local directory containing DEM tiles\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        gcs_prefix: Prefix for files in the bucket (e.g., \"dem_tiles/\")\n",
    "    \n",
    "    Returns:\n",
    "        List of uploaded file names\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    # Find all .tif files in the local directory\n",
    "    pattern = os.path.join(local_dir, \"**\", \"*.tif\")\n",
    "    local_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    if not local_files:\n",
    "        print(f\"No .tif files found in {local_dir}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(local_files)} .tif files to upload\")\n",
    "    \n",
    "    uploaded_files = []\n",
    "    failed_uploads = []\n",
    "    \n",
    "    for local_file in tqdm(local_files, desc=\"Uploading to GCS\"):\n",
    "        try:\n",
    "            # Create the destination blob name\n",
    "            relative_path = os.path.relpath(local_file, local_dir)\n",
    "            blob_name = gcs_prefix + relative_path.replace(\"\\\\\", \"/\")  # Ensure forward slashes\n",
    "            \n",
    "            # Create blob and upload\n",
    "            blob = bucket.blob(blob_name)\n",
    "            blob.upload_from_filename(local_file)\n",
    "            \n",
    "            uploaded_files.append(blob_name)\n",
    "            print(f\"✅ Uploaded: {blob_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_uploads.append((local_file, str(e)))\n",
    "            print(f\"❌ Failed to upload {local_file}: {str(e)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n📊 Upload Summary:\")\n",
    "    print(f\"✅ Successfully uploaded: {len(uploaded_files)} files\")\n",
    "    print(f\"❌ Failed uploads: {len(failed_uploads)} files\")\n",
    "    \n",
    "    if failed_uploads:\n",
    "        print(\"\\nFailed uploads:\")\n",
    "        for file_path, error in failed_uploads:\n",
    "            print(f\"  - {file_path}: {error}\")\n",
    "    \n",
    "    return uploaded_files\n",
    "\n",
    "def download_dem_tiles_from_gcs(bucket_name=\"your-bucket-name\", gcs_prefix=\"dem_tiles/\", local_dir=\"dem_tiles\"):\n",
    "    \"\"\"\n",
    "    Download DEM tiles from Google Cloud Storage bucket to local directory.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        gcs_prefix: Prefix for files in the bucket (e.g., \"dem_tiles/\")\n",
    "        local_dir: Local directory to download files to\n",
    "    \n",
    "    Returns:\n",
    "        List of downloaded file names\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    # List all blobs with the given prefix\n",
    "    blobs = bucket.list_blobs(prefix=gcs_prefix)\n",
    "    \n",
    "    # Filter for .tif files\n",
    "    tif_blobs = [blob for blob in blobs if blob.name.endswith('.tif')]\n",
    "    \n",
    "    if not tif_blobs:\n",
    "        print(f\"No .tif files found in bucket {bucket_name} with prefix {gcs_prefix}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(tif_blobs)} .tif files to download\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    failed_downloads = []\n",
    "    \n",
    "    for blob in tqdm(tif_blobs, desc=\"Downloading from GCS\"):\n",
    "        try:\n",
    "            # Create local file path\n",
    "            relative_path = blob.name[len(gcs_prefix):]  # Remove prefix\n",
    "            local_file = os.path.join(local_dir, relative_path)\n",
    "            \n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(local_file), exist_ok=True)\n",
    "            \n",
    "            # Download the file\n",
    "            blob.download_to_filename(local_file)\n",
    "            \n",
    "            downloaded_files.append(local_file)\n",
    "            print(f\"✅ Downloaded: {blob.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_downloads.append((blob.name, str(e)))\n",
    "            print(f\"❌ Failed to download {blob.name}: {str(e)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n📊 Download Summary:\")\n",
    "    print(f\"✅ Successfully downloaded: {len(downloaded_files)} files\")\n",
    "    print(f\"❌ Failed downloads: {len(failed_downloads)} files\")\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(\"\\nFailed downloads:\")\n",
    "        for blob_name, error in failed_downloads:\n",
    "            print(f\"  - {blob_name}: {error}\")\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "def list_gcs_dem_tiles(bucket_name=\"your-bucket-name\", gcs_prefix=\"dem_tiles/\"):\n",
    "    \"\"\"\n",
    "    List all DEM tiles in the GCS bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        gcs_prefix: Prefix for files in the bucket (e.g., \"dem_tiles/\")\n",
    "    \n",
    "    Returns:\n",
    "        List of blob names\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    blobs = bucket.list_blobs(prefix=gcs_prefix)\n",
    "    tif_blobs = [blob.name for blob in blobs if blob.name.endswith('.tif')]\n",
    "    \n",
    "    print(f\"Found {len(tif_blobs)} .tif files in bucket {bucket_name}\")\n",
    "    for blob_name in tif_blobs:\n",
    "        print(f\"  - {blob_name}\")\n",
    "    \n",
    "    return tif_blobs\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your bucket name\n",
    "    BUCKET_NAME = \"mushroom-radar\"\n",
    "    \n",
    "    # Upload DEM tiles to GCS\n",
    "    print(\"Uploading DEM tiles to Google Cloud Storage...\")\n",
    "    uploaded = upload_dem_tiles_to_gcs(\n",
    "        local_dir=\"dem_tiles\",\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        gcs_prefix=\"dem_tiles/\"\n",
    "    )\n",
    "    \n",
    "    # List files in bucket\n",
    "    print(\"\\nListing files in bucket...\")\n",
    "    list_gcs_dem_tiles(bucket_name=BUCKET_NAME, gcs_prefix=\"dem_tiles/\")\n",
    "    \n",
    "    # Download DEM tiles from GCS (example)\n",
    "    # print(\"\\nDownloading DEM tiles from Google Cloud Storage...\")\n",
    "    # downloaded = download_dem_tiles_from_gcs(\n",
    "    #     bucket_name=BUCKET_NAME,\n",
    "    #     gcs_prefix=\"dem_tiles/\",\n",
    "    #     local_dir=\"dem_tiles_downloaded\"\n",
    "    # )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
