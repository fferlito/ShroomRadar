{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import earthaccess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Login using environment variables\n",
    "auth = earthaccess.login(strategy=\"environment\")\n",
    "\n",
    "# Search for Rome area\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"SRTMGL1\",\n",
    "    version=\"003\",\n",
    "    bounding_box=(12.35, 41.8, 12.65, 42.0)  # min lon, min lat, max lon, max lat\n",
    ")\n",
    "\n",
    "# Download tiles\n",
    "paths = earthaccess.download(results, \"./srtm_tiles\")\n",
    "print(paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append variables for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 19:25:02,937 - INFO - Loading CSV: data/negative_samples_within_land_10k_with_coords.csv\n",
      "2025-09-15 19:25:02,948 - INFO - Loaded 5403 rows\n",
      "2025-09-15 19:25:02,950 - INFO - Step 1: Scanning CSV for tile requirements...\n",
      "Scanning CSV:  14%|█▍        | 754/5403 [00:05<00:30, 154.43it/s]2025-09-15 19:25:08,709 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\S28E016_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:08,710 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  15%|█▍        | 803/5403 [00:06<00:32, 142.96it/s]2025-09-15 19:25:09,012 - INFO - GDAL signalled an error: err_no=1, msg='N53E039_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\N53E039_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:09,014 - INFO - GDAL signalled an error: err_no=1, msg='N53E039_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  15%|█▌        | 818/5403 [00:06<00:31, 143.79it/s]2025-09-15 19:25:09,106 - INFO - GDAL signalled an error: err_no=1, msg='N32W009_slope.tif: TIFFFetchDirectory:dem_tiles\\\\slope\\\\N32W009_slope.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:09,108 - INFO - GDAL signalled an error: err_no=1, msg='N32W009_slope.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  46%|████▋     | 2509/5403 [00:19<00:15, 185.53it/s]2025-09-15 19:25:22,604 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\S28E016_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:22,605 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  71%|███████   | 3822/5403 [00:26<00:06, 242.52it/s]2025-09-15 19:25:29,599 - INFO - GDAL signalled an error: err_no=1, msg='S28E122_slope.tif: TIFFFetchDirectory:dem_tiles\\\\slope\\\\S28E122_slope.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:29,600 - INFO - GDAL signalled an error: err_no=1, msg='S28E122_slope.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV:  75%|███████▌  | 4054/5403 [00:27<00:04, 273.83it/s]2025-09-15 19:25:30,528 - INFO - GDAL signalled an error: err_no=1, msg='N49E110_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\N49E110_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:30,529 - INFO - GDAL signalled an error: err_no=1, msg='N49E110_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Scanning CSV: 100%|██████████| 5403/5403 [00:32<00:00, 165.16it/s]\n",
      "2025-09-15 19:25:35,668 - INFO - Found 4310 tiles that need processing\n",
      "2025-09-15 19:25:35,668 - INFO - Step 4: Extracting values from rasters...\n",
      "Extracting values:  14%|█▍        | 763/5403 [00:17<01:26, 53.91it/s]2025-09-15 19:25:53,495 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\S28E016_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:53,496 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  15%|█▍        | 807/5403 [00:18<01:20, 56.99it/s]2025-09-15 19:25:54,215 - INFO - GDAL signalled an error: err_no=1, msg='N53E039_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\N53E039_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:54,216 - INFO - GDAL signalled an error: err_no=1, msg='N53E039_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  15%|█▌        | 820/5403 [00:18<01:24, 54.51it/s]2025-09-15 19:25:54,473 - INFO - GDAL signalled an error: err_no=1, msg='N32W009_slope.tif: TIFFFetchDirectory:dem_tiles\\\\slope\\\\N32W009_slope.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:25:54,473 - INFO - GDAL signalled an error: err_no=1, msg='N32W009_slope.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  47%|████▋     | 2517/5403 [00:49<00:54, 53.11it/s]2025-09-15 19:26:25,583 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\S28E016_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:26:25,583 - INFO - GDAL signalled an error: err_no=1, msg='S28E016_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  71%|███████   | 3819/5403 [01:20<00:20, 77.12it/s]2025-09-15 19:26:55,854 - INFO - GDAL signalled an error: err_no=1, msg='S28E122_slope.tif: TIFFFetchDirectory:dem_tiles\\\\slope\\\\S28E122_slope.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:26:55,855 - INFO - GDAL signalled an error: err_no=1, msg='S28E122_slope.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values:  75%|███████▌  | 4071/5403 [01:23<00:18, 72.75it/s]2025-09-15 19:26:58,926 - INFO - GDAL signalled an error: err_no=1, msg='N49E110_aspect.tif: TIFFFetchDirectory:dem_tiles\\\\aspect\\\\N49E110_aspect.tif: Can not read TIFF directory count'\n",
      "2025-09-15 19:26:58,926 - INFO - GDAL signalled an error: err_no=1, msg='N49E110_aspect.tif: TIFFReadDirectory:Failed to read directory at offset 51840008'\n",
      "Extracting values: 100%|██████████| 5403/5403 [01:51<00:00, 48.48it/s]\n",
      "2025-09-15 19:27:27,108 - INFO - Step 5: Decoding geomorphon classes...\n",
      "2025-09-15 19:27:27,146 - INFO - ==================================================\n",
      "2025-09-15 19:27:27,148 - INFO - PROCESSING SUMMARY\n",
      "2025-09-15 19:27:27,148 - INFO - ==================================================\n",
      "2025-09-15 19:27:27,149 - INFO - Total rows processed: 5403\n",
      "2025-09-15 19:27:27,149 - INFO - Invalid coordinates: 0\n",
      "2025-09-15 19:27:27,150 - INFO - Missing tiles: 0\n",
      "2025-09-15 19:27:27,151 - INFO - Extraction failures: 0\n",
      "2025-09-15 19:27:27,153 - INFO - Successful DEM extractions: 5126\n",
      "2025-09-15 19:27:27,153 - INFO - Successful slope extractions: 3326\n",
      "2025-09-15 19:27:27,153 - INFO - Successful aspect extractions: 3323\n",
      "2025-09-15 19:27:27,155 - INFO - Successful geomorphon extractions: 0\n",
      "2025-09-15 19:27:27,155 - INFO - ✅ Done! Saved data/negative_samples_within_land_10k_with_coords_topography.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from whitebox.whitebox_tools import WhiteboxTools\n",
    "import earthaccess\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# ---------------------------\n",
    "# Tile ID utilities\n",
    "# ---------------------------\n",
    "def tile_id_from_coords(lat, lon):\n",
    "    \"\"\"Convert coords to tile ID (e.g. N40W106).\"\"\"\n",
    "    import math\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return None\n",
    "    ns = \"N\" if lat >= 0 else \"S\"\n",
    "    ew = \"E\" if lon >= 0 else \"W\"\n",
    "    # Use floor for both positive and negative coordinates to handle edge cases properly\n",
    "    lat_tile = math.floor(lat)\n",
    "    lon_tile = math.floor(lon)\n",
    "    return f\"{ns}{abs(lat_tile):02d}{ew}{abs(lon_tile):03d}\"\n",
    "\n",
    "# ---------------------------\n",
    "# DEM Download\n",
    "# ---------------------------\n",
    "def download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=\"dem_tiles\", prefer=\"SRTMGL1\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    earthaccess.login(strategy=\"environment\", persist=True)\n",
    "    dataset = (\"SRTMGL1\", \"003\") if prefer == \"SRTMGL1\" else (\"COPDEM_GLO_30\", \"001\")\n",
    "    try:\n",
    "        results = earthaccess.search_data(\n",
    "            short_name=dataset[0],\n",
    "            version=dataset[1],\n",
    "            bounding_box=(min_lon, min_lat, max_lon, max_lat),\n",
    "            count=10\n",
    "        )\n",
    "    except IndexError:\n",
    "        return []\n",
    "    if not results or len(results) == 0:\n",
    "        return []\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
    "        paths = earthaccess.download(results, out_dir)\n",
    "    return paths\n",
    "\n",
    "def download_dem_point(lat, lon, out_dir=\"dem_tiles\", buffer=0.1):\n",
    "    min_lon = max(-180.0, lon - buffer)\n",
    "    max_lon = min(180.0, lon + buffer)\n",
    "    min_lat = max(-90.0, lat - buffer)\n",
    "    max_lat = min(90.0, lat + buffer)\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"SRTMGL1\")\n",
    "    if paths:\n",
    "        return paths, \"SRTM\"\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"COPDEM\")\n",
    "    if paths:\n",
    "        return paths, \"Copernicus\"\n",
    "    return [], \"None\"\n",
    "\n",
    "# ---------------------------\n",
    "# HGT → GeoTIFF\n",
    "# ---------------------------\n",
    "def parse_hgt_bounds(hgt_path):\n",
    "    name = os.path.splitext(os.path.basename(hgt_path))[0]\n",
    "    m = re.match(r'([NS])(\\d{1,2})([EW])(\\d{1,3})', name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse HGT name: {hgt_path}\")\n",
    "    lat_sign = 1 if m.group(1).upper() == 'N' else -1\n",
    "    lon_sign = 1 if m.group(3).upper() == 'E' else -1\n",
    "    import math\n",
    "    lat0 = lat_sign * math.floor(int(m.group(2)))\n",
    "    lon0 = lon_sign * math.floor(int(m.group(4)))\n",
    "    west, south = float(lon0), float(lat0)\n",
    "    east, north = west + 1.0, south + 1.0\n",
    "    return west, south, east, north\n",
    "\n",
    "def hgt_to_gtiff(hgt_path, tif_path):\n",
    "    west, south, east, north = parse_hgt_bounds(hgt_path)\n",
    "    nbytes = os.path.getsize(hgt_path)\n",
    "    side = int(np.sqrt(nbytes // 2))\n",
    "    if side not in (3601, 1201):\n",
    "        raise ValueError(f\"Unexpected HGT side length: {side}\")\n",
    "    data = np.fromfile(hgt_path, dtype=\">i2\").reshape((side, side))\n",
    "    data = data[:-1, :-1]\n",
    "    res = 1.0 / (side - 1)\n",
    "    transform = from_origin(west, north, res, res)\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": data.shape[0],\n",
    "        \"width\": data.shape[1],\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"int16\",\n",
    "        \"crs\": \"EPSG:4326\",\n",
    "        \"transform\": transform,\n",
    "        \"nodata\": -32768,\n",
    "        \"tiled\": True,\n",
    "        \"compress\": \"LZW\"\n",
    "    }\n",
    "    with rasterio.open(tif_path, \"w\", **profile) as dst:\n",
    "        dst.write(data, 1)\n",
    "\n",
    "def prepare_tif(path):\n",
    "    \"\"\"Unpack zip/HGT and convert to GeoTIFF. Remove raw files after processing.\"\"\"\n",
    "    if path.lower().endswith(\".tif\"):\n",
    "        return os.path.abspath(path)\n",
    "    if path.lower().endswith(\".zip\"):\n",
    "        tif_out, hgt_out = None, None\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            tifs = [m for m in z.namelist() if m.lower().endswith(\".tif\")]\n",
    "            if tifs:\n",
    "                tif_out = os.path.join(os.path.dirname(path), os.path.basename(tifs[0]))\n",
    "                if not os.path.exists(tif_out):\n",
    "                    z.extract(tifs[0], os.path.dirname(path))\n",
    "                tif_out = os.path.abspath(tif_out)\n",
    "            else:\n",
    "                hgts = [m for m in z.namelist() if m.lower().endswith(\".hgt\")]\n",
    "                if hgts:\n",
    "                    hgt_out = os.path.join(os.path.dirname(path), os.path.basename(hgts[0]))\n",
    "                    if not os.path.exists(hgt_out):\n",
    "                        z.extract(hgts[0], os.path.dirname(path))\n",
    "                    tif_out = hgt_out.replace(\".hgt\", \".tif\")\n",
    "                    if not os.path.exists(tif_out):\n",
    "                        hgt_to_gtiff(hgt_out, tif_out)\n",
    "                    try:\n",
    "                        os.remove(hgt_out)\n",
    "                    except PermissionError:\n",
    "                        pass\n",
    "                    tif_out = os.path.abspath(tif_out)\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        if tif_out:\n",
    "            return tif_out\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .tif or .hgt in {path}\")\n",
    "    raise FileNotFoundError(f\"Unsupported DEM format: {path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Whitebox + helpers\n",
    "# ---------------------------\n",
    "wbt = WhiteboxTools()\n",
    "wbt.verbose = False\n",
    "\n",
    "def valid_raster(path):\n",
    "    \"\"\"Check if a raster exists, non-empty, and can be opened by rasterio.\"\"\"\n",
    "    if not path or not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        return False\n",
    "    try:\n",
    "        with rasterio.open(path) as src:\n",
    "            _ = src.count\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def run_whitebox(tif_file, need_slope=False, need_aspect=False, need_geomorph=False, slope_dir=None, aspect_dir=None, geomorph_dir=None):\n",
    "    tif_file = os.path.abspath(tif_file).replace(\"\\\\\", \"/\")\n",
    "    base_name = os.path.splitext(os.path.basename(tif_file))[0]\n",
    "    \n",
    "    # Create output paths in subfolders\n",
    "    slope_tif = os.path.join(slope_dir, f\"{base_name}_slope.tif\") if slope_dir else f\"{os.path.splitext(tif_file)[0]}_slope.tif\"\n",
    "    aspect_tif = os.path.join(aspect_dir, f\"{base_name}_aspect.tif\") if aspect_dir else f\"{os.path.splitext(tif_file)[0]}_aspect.tif\"\n",
    "    geomorph_tif = os.path.join(geomorph_dir, f\"{base_name}_geomorph.tif\") if geomorph_dir else f\"{os.path.splitext(tif_file)[0]}_geomorph.tif\"\n",
    "    \n",
    "    if need_slope and not valid_raster(slope_tif):\n",
    "        wbt.slope(dem=tif_file, output=slope_tif, zfactor=1.0, units=\"degrees\")\n",
    "    if need_aspect and not valid_raster(aspect_tif):\n",
    "        wbt.aspect(dem=tif_file, output=aspect_tif)\n",
    "    if need_geomorph and not valid_raster(geomorph_tif):\n",
    "        wbt.geomorphons(dem=tif_file, output=geomorph_tif, search=50, threshold=0.0, forms=True)\n",
    "    return (\n",
    "        tif_file,\n",
    "        slope_tif if valid_raster(slope_tif) else None,\n",
    "        aspect_tif if valid_raster(aspect_tif) else None,\n",
    "        geomorph_tif if valid_raster(geomorph_tif) else None\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# Extract raster value\n",
    "# ---------------------------\n",
    "def extract_value(raster, lat, lon):\n",
    "    if not valid_raster(raster):\n",
    "        return None\n",
    "    try:\n",
    "        with rasterio.open(raster) as src:\n",
    "            nd = src.nodata\n",
    "            for val in src.sample([(lon, lat)]):\n",
    "                v = float(val[0])\n",
    "                if np.isnan(v) or (nd is not None and v == nd):\n",
    "                    return None\n",
    "                return v\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------\n",
    "def enrich_csv(input_csv, output_csv, out_dir=\"dem_tiles\", download_tiles=True, variables=[\"dem\", \"slope\", \"aspect\", \"geomorphons\"], verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced version with detailed error logging and failure tracking.\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Input CSV file path\n",
    "        output_csv: Output CSV file path\n",
    "        out_dir: Directory for DEM tiles\n",
    "        download_tiles: Whether to download DEM tiles (if False, only use existing files)\n",
    "        variables: List of variables to append. Options: [\"dem\", \"slope\", \"aspect\", \"geomorphons\"]\n",
    "        verbose: Print detailed progress and error information\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Setup logging\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        logger = logging.getLogger(__name__)\n",
    "    else:\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.setLevel(logging.WARNING)\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subfolders for different file types\n",
    "    dem_dir = os.path.join(out_dir, \"dem\")\n",
    "    slope_dir = os.path.join(out_dir, \"slope\")\n",
    "    aspect_dir = os.path.join(out_dir, \"aspect\")\n",
    "    geomorph_dir = os.path.join(out_dir, \"geomorphons\")\n",
    "    \n",
    "    os.makedirs(dem_dir, exist_ok=True)\n",
    "    os.makedirs(slope_dir, exist_ok=True)\n",
    "    os.makedirs(aspect_dir, exist_ok=True)\n",
    "    os.makedirs(geomorph_dir, exist_ok=True)\n",
    "    \n",
    "    # Derive individual boolean flags from variables list\n",
    "    generate_dem = \"dem\" in variables\n",
    "    generate_slope = \"slope\" in variables\n",
    "    generate_aspect = \"aspect\" in variables\n",
    "    generate_geomorphons = \"geomorphons\" in variables\n",
    "    \n",
    "    # Validate variables list\n",
    "    valid_variables = [\"dem\", \"slope\", \"aspect\", \"geomorphons\"]\n",
    "    invalid_vars = [var for var in variables if var not in valid_variables]\n",
    "    if invalid_vars:\n",
    "        raise ValueError(f\"Invalid variables: {invalid_vars}. Valid options are: {valid_variables}\")\n",
    "    \n",
    "    # Validate input file\n",
    "    if not os.path.exists(input_csv):\n",
    "        raise FileNotFoundError(f\"Input CSV file not found: {input_csv}\")\n",
    "    \n",
    "    logger.info(f\"Loading CSV: {input_csv}\")\n",
    "    df = pd.read_csv(input_csv)\n",
    "    df = df[df[\"y\"].between(-56, 60)]\n",
    "\n",
    "    logger.info(f\"Loaded {len(df)} rows\")\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = [\"x\", \"y\"]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Initialize columns based on requested variables\n",
    "    base_cols = []\n",
    "    if generate_dem:\n",
    "        base_cols.extend([\"dem\", \"dem_source\"])\n",
    "    if generate_slope:\n",
    "        base_cols.append(\"slope\")\n",
    "    if generate_aspect:\n",
    "        base_cols.append(\"aspect\")\n",
    "    if generate_geomorphons:\n",
    "        base_cols.extend([\"geomorphon\", \"geomorphon_class\"])\n",
    "    \n",
    "    for col in base_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Add failure tracking columns for requested variables\n",
    "    failure_cols = []\n",
    "    if generate_dem:\n",
    "        failure_cols.append(\"dem_failure\")\n",
    "    if generate_slope:\n",
    "        failure_cols.append(\"slope_failure\")\n",
    "    if generate_aspect:\n",
    "        failure_cols.append(\"aspect_failure\")\n",
    "    if generate_geomorphons:\n",
    "        failure_cols.append(\"geomorphon_failure\")\n",
    "    \n",
    "    for col in failure_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"invalid_coords\": 0,\n",
    "        \"missing_tiles\": 0,\n",
    "        \"extraction_failures\": 0\n",
    "    }\n",
    "    \n",
    "    # Add variable-specific stats based on requested variables\n",
    "    if generate_dem:\n",
    "        stats[\"successful_dem\"] = 0\n",
    "    if generate_slope:\n",
    "        stats[\"successful_slope\"] = 0\n",
    "    if generate_aspect:\n",
    "        stats[\"successful_aspect\"] = 0\n",
    "    if generate_geomorphons:\n",
    "        stats[\"successful_geomorphon\"] = 0\n",
    "\n",
    "    # Step 1: Collect per-tile needs\n",
    "    logger.info(\"Step 1: Scanning CSV for tile requirements...\")\n",
    "    tile_needs = {}\n",
    "    invalid_coord_rows = []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Scanning CSV\"):\n",
    "        lat, lon = row[\"y\"], row[\"x\"]\n",
    "        \n",
    "        # Check for invalid coordinates\n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            invalid_coord_rows.append(i)\n",
    "            df.at[i, \"dem_failure\"] = \"Invalid coordinates: NaN values\"\n",
    "            stats[\"invalid_coords\"] += 1\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            tid = tile_id_from_coords(lat, lon)\n",
    "            if tid is None:\n",
    "                invalid_coord_rows.append(i)\n",
    "                df.at[i, \"dem_failure\"] = f\"Invalid coordinates: lat={lat}, lon={lon}\"\n",
    "                stats[\"invalid_coords\"] += 1\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            invalid_coord_rows.append(i)\n",
    "            df.at[i, \"dem_failure\"] = f\"Coordinate conversion error: {str(e)}\"\n",
    "            stats[\"invalid_coords\"] += 1\n",
    "            continue\n",
    "            \n",
    "        base = os.path.join(dem_dir, tid)\n",
    "        slope_file = os.path.join(slope_dir, f\"{tid}_slope.tif\")\n",
    "        aspect_file = os.path.join(aspect_dir, f\"{tid}_aspect.tif\")\n",
    "        geomorph_file = os.path.join(geomorph_dir, f\"{tid}_geomorph.tif\")\n",
    "        \n",
    "        if tid not in tile_needs:\n",
    "            tile_needs[tid] = {\"dem\": False, \"slope\": False, \"aspect\": False, \"geomorphon\": False}\n",
    "            \n",
    "        if generate_dem and pd.isna(row.get(\"dem\")) and not valid_raster(f\"{base}.tif\"):\n",
    "            tile_needs[tid][\"dem\"] = True\n",
    "        if generate_slope and pd.isna(row.get(\"slope\")) and not valid_raster(slope_file):\n",
    "            tile_needs[tid][\"slope\"] = True\n",
    "        if generate_aspect and pd.isna(row.get(\"aspect\")) and not valid_raster(aspect_file):\n",
    "            tile_needs[tid][\"aspect\"] = True\n",
    "        if generate_geomorphons and pd.isna(row.get(\"geomorphon\")) and not valid_raster(geomorph_file):\n",
    "            tile_needs[tid][\"geomorphon\"] = True\n",
    "            \n",
    "    tile_needs = {tid: needs for tid, needs in tile_needs.items() if any(needs.values())}\n",
    "    logger.info(f\"Found {len(tile_needs)} tiles that need processing\")\n",
    "\n",
    "    # Step 2 & 3: Only run if download_tiles is True\n",
    "    downloaded = {}\n",
    "    tile_results = {}\n",
    "    if download_tiles:\n",
    "        logger.info(\"Step 2: Downloading and preparing tiles...\")\n",
    "        for tid, needs in tqdm(tile_needs.items(), desc=\"Preparing tiles\"):\n",
    "            local_tif = os.path.join(dem_dir, f\"{tid}.tif\")\n",
    "            if valid_raster(local_tif):\n",
    "                downloaded[tid] = ([local_tif], \"Local\")\n",
    "                continue\n",
    "                \n",
    "            m = re.match(r'([NS])(\\d{2})([EW])(\\d{3})', tid)\n",
    "            if not m:\n",
    "                logger.warning(f\"Could not parse tile ID: {tid}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                lat0 = int(m.group(2)) * (1 if m.group(1) == \"N\" else -1)\n",
    "                lon0 = int(m.group(4)) * (1 if m.group(3) == \"E\" else -1)\n",
    "                zip_paths, source = download_dem_point(lat0 + 0.5, lon0 + 0.5, out_dir=out_dir)\n",
    "                if zip_paths:\n",
    "                    tifs = [prepare_tif(zp) for zp in zip_paths]\n",
    "                    # Move processed files to dem subfolder\n",
    "                    moved_tifs = []\n",
    "                    for tif in tifs:\n",
    "                        target_path = os.path.join(dem_dir, f\"{tid}.tif\")\n",
    "                        if tif != target_path:\n",
    "                            import shutil\n",
    "                            shutil.move(tif, target_path)\n",
    "                        moved_tifs.append(target_path)\n",
    "                    downloaded[tid] = (moved_tifs, source)\n",
    "                else:\n",
    "                    logger.warning(f\"No DEM data available for tile {tid}\")\n",
    "                    stats[\"missing_tiles\"] += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error downloading tile {tid}: {str(e)}\")\n",
    "                stats[\"missing_tiles\"] += 1\n",
    "\n",
    "        logger.info(\"Step 3: Running Whitebox processing...\")\n",
    "        for tid, (tifs, source) in tqdm(downloaded.items(), desc=\"Running Whitebox\"):\n",
    "            needs = tile_needs.get(tid, {})\n",
    "            for tif in tifs:\n",
    "                try:\n",
    "                    tif_path, slope_path, aspect_path, geomorph_path = run_whitebox(\n",
    "                        tif,\n",
    "                        need_slope=generate_slope and needs.get(\"slope\", False),\n",
    "                        need_aspect=generate_aspect and needs.get(\"aspect\", False),\n",
    "                        need_geomorph=generate_geomorphons and needs.get(\"geomorphon\", False),\n",
    "                        slope_dir=slope_dir,\n",
    "                        aspect_dir=aspect_dir,\n",
    "                        geomorph_dir=geomorph_dir\n",
    "                    )\n",
    "                    tile_results[tid] = {\n",
    "                        \"tif\": tif_path,\n",
    "                        \"slope\": slope_path,\n",
    "                        \"aspect\": aspect_path,\n",
    "                        \"geomorphon\": geomorph_path,\n",
    "                        \"source\": source\n",
    "                    }\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Whitebox processing failed for tile {tid}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    # Step 4: Extract values from whatever exists\n",
    "    logger.info(\"Step 4: Extracting values from rasters...\")\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting values\"):\n",
    "        # Skip rows with invalid coordinates\n",
    "        if i in invalid_coord_rows:\n",
    "            continue\n",
    "            \n",
    "        lat, lon = row[\"y\"], row[\"x\"]\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid is None:\n",
    "            continue\n",
    "            \n",
    "        dem_base = os.path.join(dem_dir, tid)\n",
    "        slope_base = os.path.join(slope_dir, f\"{tid}_slope.tif\")\n",
    "        aspect_base = os.path.join(aspect_dir, f\"{tid}_aspect.tif\")\n",
    "        geomorph_base = os.path.join(geomorph_dir, f\"{tid}_geomorph.tif\")\n",
    "        \n",
    "        tr = tile_results.get(tid, {\n",
    "            \"tif\": f\"{dem_base}.tif\" if valid_raster(f\"{dem_base}.tif\") else None,\n",
    "            \"slope\": slope_base if valid_raster(slope_base) else None,\n",
    "            \"aspect\": aspect_base if valid_raster(aspect_base) else None,\n",
    "            \"geomorphon\": geomorph_base if valid_raster(geomorph_base) else None,\n",
    "            \"source\": \"Local\"\n",
    "        })\n",
    "        \n",
    "        # Extract DEM value\n",
    "        if generate_dem and pd.isna(row.get(\"dem\")) and tr[\"tif\"]:\n",
    "            try:\n",
    "                dem_value = extract_value(tr[\"tif\"], lat, lon)\n",
    "                if dem_value is not None:\n",
    "                    df.at[i, \"dem\"] = dem_value\n",
    "                    df.at[i, \"dem_source\"] = tr[\"source\"]\n",
    "                    stats[\"successful_dem\"] += 1\n",
    "                else:\n",
    "                    df.at[i, \"dem_failure\"] = \"No data value or out of bounds\"\n",
    "            except Exception as e:\n",
    "                df.at[i, \"dem_failure\"] = f\"Extraction error: {str(e)}\"\n",
    "                stats[\"extraction_failures\"] += 1\n",
    "        elif generate_dem and pd.isna(row.get(\"dem\")):\n",
    "            df.at[i, \"dem_failure\"] = \"No DEM raster available\"\n",
    "        \n",
    "        # Extract slope value\n",
    "        if generate_slope and pd.isna(row.get(\"slope\")) and tr[\"slope\"]:\n",
    "            try:\n",
    "                slope_value = extract_value(tr[\"slope\"], lat, lon)\n",
    "                if slope_value is not None:\n",
    "                    df.at[i, \"slope\"] = slope_value\n",
    "                    stats[\"successful_slope\"] += 1\n",
    "                else:\n",
    "                    df.at[i, \"slope_failure\"] = \"No data value or out of bounds\"\n",
    "            except Exception as e:\n",
    "                df.at[i, \"slope_failure\"] = f\"Extraction error: {str(e)}\"\n",
    "                stats[\"extraction_failures\"] += 1\n",
    "        elif generate_slope and pd.isna(row.get(\"slope\")):\n",
    "            df.at[i, \"slope_failure\"] = \"No slope raster available\"\n",
    "        \n",
    "        # Extract aspect value\n",
    "        if generate_aspect and pd.isna(row.get(\"aspect\")) and tr[\"aspect\"]:\n",
    "            try:\n",
    "                aspect_value = extract_value(tr[\"aspect\"], lat, lon)\n",
    "                if aspect_value is not None:\n",
    "                    df.at[i, \"aspect\"] = aspect_value\n",
    "                    stats[\"successful_aspect\"] += 1\n",
    "                else:\n",
    "                    df.at[i, \"aspect_failure\"] = \"No data value or out of bounds\"\n",
    "            except Exception as e:\n",
    "                df.at[i, \"aspect_failure\"] = f\"Extraction error: {str(e)}\"\n",
    "                stats[\"extraction_failures\"] += 1\n",
    "        elif generate_aspect and pd.isna(row.get(\"aspect\")):\n",
    "            df.at[i, \"aspect_failure\"] = \"No aspect raster available\"\n",
    "        \n",
    "        # Extract geomorphon value\n",
    "        if generate_geomorphons and pd.isna(row.get(\"geomorphon\")) and tr[\"geomorphon\"]:\n",
    "            try:\n",
    "                geomorph_value = extract_value(tr[\"geomorphon\"], lat, lon)\n",
    "                if geomorph_value is not None:\n",
    "                    df.at[i, \"geomorphon\"] = geomorph_value\n",
    "                    stats[\"successful_geomorphon\"] += 1\n",
    "                else:\n",
    "                    df.at[i, \"geomorphon_failure\"] = \"No data value or out of bounds\"\n",
    "            except Exception as e:\n",
    "                df.at[i, \"geomorphon_failure\"] = f\"Extraction error: {str(e)}\"\n",
    "                stats[\"extraction_failures\"] += 1\n",
    "        elif generate_geomorphons and pd.isna(row.get(\"geomorphon\")):\n",
    "            df.at[i, \"geomorphon_failure\"] = \"No geomorphon raster available\"\n",
    "\n",
    "    # Step 5: Decode geomorphons (only if geomorphons were generated)\n",
    "    if generate_geomorphons:\n",
    "        logger.info(\"Step 5: Decoding geomorphon classes...\")\n",
    "        geomorph_classes = {\n",
    "            1: \"flat\", 2: \"summit\", 3: \"ridge\", 4: \"shoulder\", 5: \"spur\",\n",
    "            6: \"slope\", 7: \"hollow\", 8: \"footslope\", 9: \"valley\", 10: \"pit\"\n",
    "        }\n",
    "        df[\"geomorphon_class\"] = df[\"geomorphon\"].map(geomorph_classes)\n",
    "\n",
    "    # Save results\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(\"PROCESSING SUMMARY\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"Total rows processed: {stats['total_rows']}\")\n",
    "    logger.info(f\"Invalid coordinates: {stats['invalid_coords']}\")\n",
    "    logger.info(f\"Missing tiles: {stats['missing_tiles']}\")\n",
    "    logger.info(f\"Extraction failures: {stats['extraction_failures']}\")\n",
    "    \n",
    "    # Show stats only for requested variables\n",
    "    if generate_dem:\n",
    "        logger.info(f\"Successful DEM extractions: {stats['successful_dem']}\")\n",
    "    if generate_slope:\n",
    "        logger.info(f\"Successful slope extractions: {stats['successful_slope']}\")\n",
    "    if generate_aspect:\n",
    "        logger.info(f\"Successful aspect extractions: {stats['successful_aspect']}\")\n",
    "    if generate_geomorphons:\n",
    "        logger.info(f\"Successful geomorphon extractions: {stats['successful_geomorphon']}\")\n",
    "    \n",
    "    logger.info(f\"✅ Done! Saved {output_csv}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    enrich_csv(\n",
    "        \"data/negative_samples_within_land_10k_with_coords.csv\",\n",
    "        \"data/negative_samples_within_land_10k_with_coords_topography.csv\",\n",
    "        out_dir=\"dem_tiles\",  # Files will be organized in subfolders: dem/, slope/, aspect/, geomorphons/\n",
    "        download_tiles=False,  # 🚨 Set to True if you want to download new tiles\n",
    "        variables=[\"dem\", \"slope\", \"aspect\", \"geomorphons\"]  # Specify which variables to append\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading DEM tiles to Google Cloud Storage...\n",
      "Found 15375 .tif files to upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 1/15375 [00:00<2:44:39,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E009.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 2/15375 [00:01<2:23:46,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E010.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 3/15375 [00:01<2:35:37,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E011.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 4/15375 [00:03<4:12:19,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E011_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 5/15375 [00:04<4:51:33,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E011_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 6/15375 [00:05<4:10:28,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E012.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 7/15375 [00:05<3:33:35,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E013.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 8/15375 [00:07<4:54:20,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E013_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 9/15375 [00:09<6:05:45,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E013_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 10/15375 [00:10<4:56:59,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E014.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 11/15375 [00:10<4:07:15,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E015.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 12/15375 [00:11<3:20:43,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E016.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 13/15375 [00:13<4:41:31,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E016_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 14/15375 [00:14<5:19:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E016_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 15/15375 [00:15<4:11:11,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E017.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 16/15375 [00:15<3:21:53,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E018.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 17/15375 [00:16<4:12:59,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E018_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 18/15375 [00:18<4:50:52,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E018_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 19/15375 [00:18<3:49:28,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E019.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 20/15375 [00:20<4:27:03,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E019_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 21/15375 [00:21<4:51:52,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E019_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 22/15375 [00:21<3:57:52,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E020.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 23/15375 [00:22<3:20:27,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E021.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 24/15375 [00:23<3:59:58,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E021_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 25/15375 [00:24<4:24:55,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E021_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 26/15375 [00:25<3:37:54,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E023.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 27/15375 [00:26<4:03:55,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E023_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 28/15375 [00:28<4:52:39,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E023_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 29/15375 [00:28<3:58:20,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E024.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 30/15375 [00:29<4:34:43,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E024_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 31/15375 [00:31<4:55:34,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E024_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 32/15375 [00:31<4:01:24,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E025.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 33/15375 [00:33<4:32:05,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E025_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 34/15375 [00:34<4:52:09,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E025_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 35/15375 [00:34<4:02:11,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E026.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 36/15375 [00:35<3:23:41,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E027.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 37/15375 [00:37<4:32:58,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E027_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 38/15375 [00:38<5:06:08,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E027_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 39/15375 [00:39<4:15:28,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E028.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 40/15375 [00:39<3:39:20,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E030.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 41/15375 [00:40<3:11:55,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E031.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 42/15375 [00:40<2:48:52,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E032.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 43/15375 [00:42<3:58:16,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E032_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 44/15375 [00:43<4:53:57,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E032_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 45/15375 [00:44<3:55:07,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E033.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 46/15375 [00:44<3:25:49,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E037.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 47/15375 [00:46<4:20:17,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E037_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 48/15375 [00:47<5:05:44,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E037_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 49/15375 [00:48<4:08:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E038.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 50/15375 [00:48<3:18:26,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E039.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 51/15375 [00:50<4:07:13,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E039_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 52/15375 [00:51<4:38:48,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E039_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 53/15375 [00:51<3:52:08,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E101.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 54/15375 [00:53<4:31:56,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E101_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 55/15375 [00:54<4:47:54,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E101_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 56/15375 [00:55<3:58:59,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E110.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 57/15375 [00:56<4:41:17,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E110_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 58/15375 [00:57<5:07:06,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E110_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 59/15375 [00:58<4:12:52,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E112.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 60/15375 [00:59<4:40:33,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E112_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 61/15375 [01:01<4:55:51,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E112_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 62/15375 [01:01<4:13:38,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E113.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 63/15375 [01:03<4:41:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E113_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 64/15375 [01:04<5:01:47,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E113_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 65/15375 [01:04<4:04:17,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E116.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 66/15375 [01:06<4:30:49,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E116_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 67/15375 [01:07<5:12:05,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E116_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 68/15375 [01:08<4:20:26,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E117.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 69/15375 [01:09<5:06:13,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E117_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 70/15375 [01:11<5:31:29,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E117_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 71/15375 [01:11<4:14:02,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E119.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 72/15375 [01:13<4:52:23,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E119_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 73/15375 [01:14<5:12:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E119_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 74/15375 [01:14<4:00:39,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E127.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 75/15375 [01:16<4:36:22,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E127_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   0%|          | 76/15375 [01:17<5:05:27,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E127_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 77/15375 [01:18<3:58:15,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E128.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 78/15375 [01:19<4:29:30,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E128_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 79/15375 [01:20<4:54:28,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00E128_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 80/15375 [01:21<4:08:22,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W053.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 81/15375 [01:22<4:41:40,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W053_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 82/15375 [01:24<4:59:17,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W053_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 83/15375 [01:24<4:10:08,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W054.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 84/15375 [01:26<4:38:33,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W054_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 85/15375 [01:27<5:24:46,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W054_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 86/15375 [01:28<4:35:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W055.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 87/15375 [01:29<4:07:19,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W056.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 88/15375 [01:30<5:06:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W056_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 89/15375 [01:32<5:42:27,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W056_slope.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 90/15375 [01:33<4:42:03,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W059.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 91/15375 [01:34<5:16:20,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded: dem_tiles/N00W059_aspect.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to GCS:   1%|          | 91/15375 [01:35<4:26:09,  1.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 157\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Upload DEM tiles to GCS\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading DEM tiles to Google Cloud Storage...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m \u001b[43mupload_dem_tiles_to_gcs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdem_tiles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBUCKET_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdem_tiles/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# List files in bucket\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mListing files in bucket...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mupload_dem_tiles_to_gcs\u001b[1;34m(local_dir, bucket_name, gcs_prefix)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Create blob and upload\u001b[39;00m\n\u001b[0;32m     43\u001b[0m blob \u001b[38;5;241m=\u001b[39m bucket\u001b[38;5;241m.\u001b[39mblob(blob_name)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_from_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m uploaded_files\u001b[38;5;241m.\u001b[39mappend(blob_name)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Uploaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:3013\u001b[0m, in \u001b[0;36mBlob.upload_from_filename\u001b[1;34m(self, filename, content_type, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Upload this blob's contents from the content of a named file.\u001b[39;00m\n\u001b[0;32m   2919\u001b[0m \n\u001b[0;32m   2920\u001b[0m \u001b[38;5;124;03mThe content type of the upload will be determined in order\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3010\u001b[0m \u001b[38;5;124;03m    to configure them.\u001b[39;00m\n\u001b[0;32m   3011\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m create_trace_span(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorage.Blob.uploadFromFilename\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 3013\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_filename_and_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:2896\u001b[0m, in \u001b[0;36mBlob._handle_filename_and_upload\u001b[1;34m(self, filename, content_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2894\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[0;32m   2895\u001b[0m     total_bytes \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfstat(file_obj\u001b[38;5;241m.\u001b[39mfileno())\u001b[38;5;241m.\u001b[39mst_size\n\u001b[1;32m-> 2896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_and_do_upload(\n\u001b[0;32m   2897\u001b[0m         file_obj,\n\u001b[0;32m   2898\u001b[0m         content_type\u001b[38;5;241m=\u001b[39mcontent_type,\n\u001b[0;32m   2899\u001b[0m         size\u001b[38;5;241m=\u001b[39mtotal_bytes,\n\u001b[0;32m   2900\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   2901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2902\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:2718\u001b[0m, in \u001b[0;36mBlob._prep_and_do_upload\u001b[1;34m(self, file_obj, rewind, size, content_type, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[0;32m   2715\u001b[0m predefined_acl \u001b[38;5;241m=\u001b[39m ACL\u001b[38;5;241m.\u001b[39mvalidate_predefined(predefined_acl)\n\u001b[0;32m   2717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2718\u001b[0m     created_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2722\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2730\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2732\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(created_json)\n\u001b[0;32m   2734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:2563\u001b[0m, in \u001b[0;36mBlob._do_upload\u001b[1;34m(self, client, stream, content_type, size, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[0;32m   2547\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_multipart_upload(\n\u001b[0;32m   2548\u001b[0m         client,\n\u001b[0;32m   2549\u001b[0m         stream,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2560\u001b[0m         command\u001b[38;5;241m=\u001b[39mcommand,\n\u001b[0;32m   2561\u001b[0m     )\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2563\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_resumable_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2567\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredefined_acl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\blob.py:2413\u001b[0m, in \u001b[0;36mBlob._do_resumable_upload\u001b[1;34m(self, client, stream, content_type, size, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[0;32m   2411\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m upload\u001b[38;5;241m.\u001b[39mfinished:\n\u001b[0;32m   2412\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2413\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mupload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransmit_next_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DataCorruption:\n\u001b[0;32m   2415\u001b[0m         \u001b[38;5;66;03m# Attempt to delete the corrupted object.\u001b[39;00m\n\u001b[0;32m   2416\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete()\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\upload.py:529\u001b[0m, in \u001b[0;36mResumableUpload.transmit_next_chunk\u001b[1;34m(self, transport, timeout)\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_resumable_response(result, \u001b[38;5;28mlen\u001b[39m(payload))\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_strategy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\_request_helpers.py:107\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[1;34m(func, retry_strategy)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retry_strategy:\n\u001b[0;32m    106\u001b[0m     func \u001b[38;5;241m=\u001b[39m retry_strategy(func)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\cloud\\storage\\_media\\requests\\upload.py:521\u001b[0m, in \u001b[0;36mResumableUpload.transmit_next_chunk.<locals>.retriable_request\u001b[1;34m()\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretriable_request\u001b[39m():\n\u001b[1;32m--> 521\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_resumable_response(result, \u001b[38;5;28mlen\u001b[39m(payload))\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\google\\auth\\transport\\requests.py:540\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[1;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[0;32m    539\u001b[0m     _helpers\u001b[38;5;241m.\u001b[39mrequest_log(_LOGGER, method, url, data, headers)\n\u001b[1;32m--> 540\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(AuthorizedSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    541\u001b[0m         method,\n\u001b[0;32m    542\u001b[0m         url,\n\u001b[0;32m    543\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    544\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[0;32m    545\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    547\u001b[0m     )\n\u001b[0;32m    548\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\urllib3\\connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\urllib3\\connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    414\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest_chunked(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 416\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\urllib3\\connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (six\u001b[38;5;241m.\u001b[39mensure_str(k\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m headers):\n\u001b[0;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1283\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1328\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1079\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encode_chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_vsn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m11\u001b[39m:\n\u001b[0;32m   1076\u001b[0m         \u001b[38;5;66;03m# chunked encoding\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124mX\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m chunk \\\n\u001b[0;32m   1078\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1079\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encode_chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_vsn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m11\u001b[39m:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# end chunked transfer\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\http\\client.py:1001\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\ssl.py:1238\u001b[0m, in \u001b[0;36mSSLSocket.sendall\u001b[1;34m(self, data, flags)\u001b[0m\n\u001b[0;32m   1236\u001b[0m         amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(byte_view)\n\u001b[0;32m   1237\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m amount:\n\u001b[1;32m-> 1238\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m             count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\ssl.py:1207\u001b[0m, in \u001b[0;36mSSLSocket.send\u001b[1;34m(self, data, flags)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1205\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to send() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1206\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(data, flags)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Upload DEM tiles to Google Cloud Storage\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "def upload_dem_tiles_to_gcs(local_dir=\"dem_tiles\", bucket_name=\"your-bucket-name\", gcs_prefix=\"dem_tiles/\"):\n",
    "    \"\"\"\n",
    "    Upload all DEM tiles from local directory to Google Cloud Storage bucket.\n",
    "    \n",
    "    Args:\n",
    "        local_dir: Local directory containing DEM tiles\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        gcs_prefix: Prefix for files in the bucket (e.g., \"dem_tiles/\")\n",
    "    \n",
    "    Returns:\n",
    "        List of uploaded file names\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    # Find all .tif files in the local directory\n",
    "    pattern = os.path.join(local_dir, \"**\", \"*.tif\")\n",
    "    local_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    if not local_files:\n",
    "        print(f\"No .tif files found in {local_dir}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(local_files)} .tif files to upload\")\n",
    "    \n",
    "    uploaded_files = []\n",
    "    failed_uploads = []\n",
    "    \n",
    "    for local_file in tqdm(local_files, desc=\"Uploading to GCS\"):\n",
    "        try:\n",
    "            # Create the destination blob name\n",
    "            relative_path = os.path.relpath(local_file, local_dir)\n",
    "            blob_name = gcs_prefix + relative_path.replace(\"\\\\\", \"/\")  # Ensure forward slashes\n",
    "            \n",
    "            # Create blob and upload\n",
    "            blob = bucket.blob(blob_name)\n",
    "            blob.upload_from_filename(local_file)\n",
    "            \n",
    "            uploaded_files.append(blob_name)\n",
    "            print(f\"✅ Uploaded: {blob_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_uploads.append((local_file, str(e)))\n",
    "            print(f\"❌ Failed to upload {local_file}: {str(e)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n📊 Upload Summary:\")\n",
    "    print(f\"✅ Successfully uploaded: {len(uploaded_files)} files\")\n",
    "    print(f\"❌ Failed uploads: {len(failed_uploads)} files\")\n",
    "    \n",
    "    if failed_uploads:\n",
    "        print(\"\\nFailed uploads:\")\n",
    "        for file_path, error in failed_uploads:\n",
    "            print(f\"  - {file_path}: {error}\")\n",
    "    \n",
    "    return uploaded_files\n",
    "\n",
    "def download_dem_tiles_from_gcs(bucket_name=\"your-bucket-name\", gcs_prefix=\"dem_tiles/\", local_dir=\"dem_tiles\"):\n",
    "    \"\"\"\n",
    "    Download DEM tiles from Google Cloud Storage bucket to local directory.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        gcs_prefix: Prefix for files in the bucket (e.g., \"dem_tiles/\")\n",
    "        local_dir: Local directory to download files to\n",
    "    \n",
    "    Returns:\n",
    "        List of downloaded file names\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    # List all blobs with the given prefix\n",
    "    blobs = bucket.list_blobs(prefix=gcs_prefix)\n",
    "    \n",
    "    # Filter for .tif files\n",
    "    tif_blobs = [blob for blob in blobs if blob.name.endswith('.tif')]\n",
    "    \n",
    "    if not tif_blobs:\n",
    "        print(f\"No .tif files found in bucket {bucket_name} with prefix {gcs_prefix}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(tif_blobs)} .tif files to download\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    failed_downloads = []\n",
    "    \n",
    "    for blob in tqdm(tif_blobs, desc=\"Downloading from GCS\"):\n",
    "        try:\n",
    "            # Create local file path\n",
    "            relative_path = blob.name[len(gcs_prefix):]  # Remove prefix\n",
    "            local_file = os.path.join(local_dir, relative_path)\n",
    "            \n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(os.path.dirname(local_file), exist_ok=True)\n",
    "            \n",
    "            # Download the file\n",
    "            blob.download_to_filename(local_file)\n",
    "            \n",
    "            downloaded_files.append(local_file)\n",
    "            print(f\"✅ Downloaded: {blob.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_downloads.append((blob.name, str(e)))\n",
    "            print(f\"❌ Failed to download {blob.name}: {str(e)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n📊 Download Summary:\")\n",
    "    print(f\"✅ Successfully downloaded: {len(downloaded_files)} files\")\n",
    "    print(f\"❌ Failed downloads: {len(failed_downloads)} files\")\n",
    "    \n",
    "    if failed_downloads:\n",
    "        print(\"\\nFailed downloads:\")\n",
    "        for blob_name, error in failed_downloads:\n",
    "            print(f\"  - {blob_name}: {error}\")\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "def list_gcs_dem_tiles(bucket_name=\"your-bucket-name\", gcs_prefix=\"dem_tiles/\"):\n",
    "    \"\"\"\n",
    "    List all DEM tiles in the GCS bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        gcs_prefix: Prefix for files in the bucket (e.g., \"dem_tiles/\")\n",
    "    \n",
    "    Returns:\n",
    "        List of blob names\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    blobs = bucket.list_blobs(prefix=gcs_prefix)\n",
    "    tif_blobs = [blob.name for blob in blobs if blob.name.endswith('.tif')]\n",
    "    \n",
    "    print(f\"Found {len(tif_blobs)} .tif files in bucket {bucket_name}\")\n",
    "    for blob_name in tif_blobs:\n",
    "        print(f\"  - {blob_name}\")\n",
    "    \n",
    "    return tif_blobs\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your bucket name\n",
    "    BUCKET_NAME = \"mushroom-radar\"\n",
    "    \n",
    "    # Upload DEM tiles to GCS\n",
    "    print(\"Uploading DEM tiles to Google Cloud Storage...\")\n",
    "    uploaded = upload_dem_tiles_to_gcs(\n",
    "        local_dir=\"dem_tiles\",\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        gcs_prefix=\"dem_tiles/\"\n",
    "    )\n",
    "    \n",
    "    # List files in bucket\n",
    "    print(\"\\nListing files in bucket...\")\n",
    "    list_gcs_dem_tiles(bucket_name=BUCKET_NAME, gcs_prefix=\"dem_tiles/\")\n",
    "    \n",
    "    # Download DEM tiles from GCS (example)\n",
    "    # print(\"\\nDownloading DEM tiles from Google Cloud Storage...\")\n",
    "    # downloaded = download_dem_tiles_from_gcs(\n",
    "    #     bucket_name=BUCKET_NAME,\n",
    "    #     gcs_prefix=\"dem_tiles/\",\n",
    "    #     local_dir=\"dem_tiles_downloaded\"\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append variables for inference (on vector file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "data/polygons.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataSourceError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 255\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 255\u001b[0m     \u001b[43menrich_geojson\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/polygons.geojson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/polygons_with_topography.geojson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 191\u001b[0m, in \u001b[0;36menrich_geojson\u001b[1;34m(input_geojson, output_geojson, out_dir)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21menrich_geojson\u001b[39m(input_geojson, output_geojson, out_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdem_tiles\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    189\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(out_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 191\u001b[0m     gdf \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_geojson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# Add expected cols\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslope\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maspect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeomorphon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdem_source\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeomorphon_class\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\geopandas\\io\\file.py:294\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m             from_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_file_pyogrio(\n\u001b[0;32m    295\u001b[0m         filename, bbox\u001b[38;5;241m=\u001b[39mbbox, mask\u001b[38;5;241m=\u001b[39mmask, columns\u001b[38;5;241m=\u001b[39mcolumns, rows\u001b[38;5;241m=\u001b[39mrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_file_like(filename):\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\geopandas\\io\\file.py:547\u001b[0m, in \u001b[0;36m_read_file_pyogrio\u001b[1;34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keywords are deprecated, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future release. You can use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    543\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pyogrio\u001b[38;5;241m.\u001b[39mread_dataframe(path_or_bytes, bbox\u001b[38;5;241m=\u001b[39mbbox, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\pyogrio\\geopandas.py:275\u001b[0m, in \u001b[0;36mread_dataframe\u001b[1;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime_as_string\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m result \u001b[38;5;241m=\u001b[39m read_func(\n\u001b[0;32m    276\u001b[0m     path_or_buffer,\n\u001b[0;32m    277\u001b[0m     layer\u001b[38;5;241m=\u001b[39mlayer,\n\u001b[0;32m    278\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    279\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    280\u001b[0m     read_geometry\u001b[38;5;241m=\u001b[39mread_geometry,\n\u001b[0;32m    281\u001b[0m     force_2d\u001b[38;5;241m=\u001b[39mgdal_force_2d,\n\u001b[0;32m    282\u001b[0m     skip_features\u001b[38;5;241m=\u001b[39mskip_features,\n\u001b[0;32m    283\u001b[0m     max_features\u001b[38;5;241m=\u001b[39mmax_features,\n\u001b[0;32m    284\u001b[0m     where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[0;32m    285\u001b[0m     bbox\u001b[38;5;241m=\u001b[39mbbox,\n\u001b[0;32m    286\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    287\u001b[0m     fids\u001b[38;5;241m=\u001b[39mfids,\n\u001b[0;32m    288\u001b[0m     sql\u001b[38;5;241m=\u001b[39msql,\n\u001b[0;32m    289\u001b[0m     sql_dialect\u001b[38;5;241m=\u001b[39msql_dialect,\n\u001b[0;32m    290\u001b[0m     return_fids\u001b[38;5;241m=\u001b[39mfid_as_index,\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    292\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferli\\anaconda3\\envs\\gchm\\lib\\site-packages\\pyogrio\\raw.py:198\u001b[0m, in \u001b[0;36mread\u001b[1;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mpyogrio/_io.pyx:1313\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpyogrio/_io.pyx:232\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDataSourceError\u001b[0m: data/polygons.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from whitebox.whitebox_tools import WhiteboxTools\n",
    "import earthaccess\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "import io\n",
    "import geopandas as gpd\n",
    "\n",
    "# ---------------------------\n",
    "# Tile ID utilities\n",
    "# ---------------------------\n",
    "def tile_id_from_coords(lat, lon):\n",
    "    \"\"\"Convert coords to tile ID (e.g. N40W106).\"\"\"\n",
    "    import math\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return None\n",
    "    ns = \"N\" if lat >= 0 else \"S\"\n",
    "    ew = \"E\" if lon >= 0 else \"W\"\n",
    "    # Use floor for both positive and negative coordinates to handle edge cases properly\n",
    "    lat_tile = math.floor(lat)\n",
    "    lon_tile = math.floor(lon)\n",
    "    return f\"{ns}{abs(lat_tile):02d}{ew}{abs(lon_tile):03d}\"\n",
    "\n",
    "# ---------------------------\n",
    "# DEM Download\n",
    "# ---------------------------\n",
    "def download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=\"dem_tiles\", prefer=\"SRTMGL1\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    earthaccess.login(strategy=\"environment\", persist=True)\n",
    "\n",
    "    dataset = (\"SRTMGL1\", \"003\") if prefer == \"SRTMGL1\" else (\"COPDEM_GLO_30\", \"001\")\n",
    "\n",
    "    try:\n",
    "        results = earthaccess.search_data(\n",
    "            short_name=dataset[0],\n",
    "            version=dataset[1],\n",
    "            bounding_box=(min_lon, min_lat, max_lon, max_lat),\n",
    "            count=10\n",
    "        )\n",
    "    except IndexError:\n",
    "        return []\n",
    "\n",
    "    if not results or len(results) == 0:\n",
    "        return []\n",
    "\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
    "        paths = earthaccess.download(results, out_dir)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def download_dem_point(lat, lon, out_dir=\"dem_tiles\", buffer=0.1):\n",
    "    # Clamp to valid ranges\n",
    "    min_lon = max(-180.0, lon - buffer)\n",
    "    max_lon = min(180.0, lon + buffer)\n",
    "    min_lat = max(-90.0, lat - buffer)\n",
    "    max_lat = min(90.0, lat + buffer)\n",
    "\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"SRTMGL1\")\n",
    "    if paths:\n",
    "        return paths, \"SRTM\"\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"COPDEM\")\n",
    "    if paths:\n",
    "        return paths, \"Copernicus\"\n",
    "    return [], \"None\"\n",
    "\n",
    "# ---------------------------\n",
    "# HGT → GeoTIFF\n",
    "# ---------------------------\n",
    "def parse_hgt_bounds(hgt_path):\n",
    "    name = os.path.splitext(os.path.basename(hgt_path))[0]\n",
    "    m = re.match(r'([NS])(\\d{1,2})([EW])(\\d{1,3})', name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse HGT name: {hgt_path}\")\n",
    "    lat_sign = 1 if m.group(1).upper() == 'N' else -1\n",
    "    lon_sign = 1 if m.group(3).upper() == 'E' else -1\n",
    "    lat0 = lat_sign * int(m.group(2))\n",
    "    lon0 = lon_sign * int(m.group(4))\n",
    "    west, south = float(lon0), float(lat0)\n",
    "    east, north = west + 1.0, south + 1.0\n",
    "    return west, south, east, north\n",
    "\n",
    "def hgt_to_gtiff(hgt_path, tif_path):\n",
    "    west, south, east, north = parse_hgt_bounds(hgt_path)\n",
    "    nbytes = os.path.getsize(hgt_path)\n",
    "    side = int(np.sqrt(nbytes // 2))\n",
    "    if side not in (3601, 1201):\n",
    "        raise ValueError(f\"Unexpected HGT side length: {side}\")\n",
    "    data = np.fromfile(hgt_path, dtype=\">i2\").reshape((side, side))\n",
    "    data = data[:-1, :-1]\n",
    "    res = 1.0 / (side - 1)\n",
    "\n",
    "    transform = from_origin(west, north, res, res)\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": data.shape[0],\n",
    "        \"width\": data.shape[1],\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"int16\",\n",
    "        \"crs\": \"EPSG:4326\",\n",
    "        \"transform\": transform,\n",
    "        \"nodata\": -32768,\n",
    "        \"tiled\": True,\n",
    "        \"compress\": \"LZW\"\n",
    "    }\n",
    "\n",
    "    with rasterio.open(tif_path, \"w\", **profile) as dst:\n",
    "        dst.write(data, 1)\n",
    "\n",
    "def prepare_tif(path):\n",
    "    \"\"\"Unpack zip/HGT and convert to GeoTIFF. Remove raw files after processing.\"\"\"\n",
    "    if path.lower().endswith(\".tif\"):\n",
    "        return os.path.abspath(path)\n",
    "\n",
    "    if path.lower().endswith(\".zip\"):\n",
    "        tif_out, hgt_out = None, None\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            tifs = [m for m in z.namelist() if m.lower().endswith(\".tif\")]\n",
    "            if tifs:\n",
    "                tif_out = os.path.join(os.path.dirname(path), os.path.basename(tifs[0]))\n",
    "                if not os.path.exists(tif_out):\n",
    "                    z.extract(tifs[0], os.path.dirname(path))\n",
    "                tif_out = os.path.abspath(tif_out)\n",
    "            else:\n",
    "                hgts = [m for m in z.namelist() if m.lower().endswith(\".hgt\")]\n",
    "                if hgts:\n",
    "                    hgt_out = os.path.join(os.path.dirname(path), os.path.basename(hgts[0]))\n",
    "                    if not os.path.exists(hgt_out):\n",
    "                        z.extract(hgts[0], os.path.dirname(path))\n",
    "                    tif_out = hgt_out.replace(\".hgt\", \".tif\")\n",
    "                    if not os.path.exists(tif_out):\n",
    "                        hgt_to_gtiff(hgt_out, tif_out)\n",
    "                    try:\n",
    "                        os.remove(hgt_out)\n",
    "                    except PermissionError:\n",
    "                        pass\n",
    "                    tif_out = os.path.abspath(tif_out)\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        if tif_out:\n",
    "            return tif_out\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .tif or .hgt in {path}\")\n",
    "    raise FileNotFoundError(f\"Unsupported DEM format: {path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Whitebox\n",
    "# ---------------------------\n",
    "wbt = WhiteboxTools()\n",
    "wbt.verbose = False\n",
    "\n",
    "def run_whitebox(tif_file):\n",
    "    tif_file = os.path.abspath(tif_file).replace(\"\\\\\", \"/\")\n",
    "    slope_tif = tif_file.replace(\".tif\", \"_slope.tif\")\n",
    "    aspect_tif = tif_file.replace(\".tif\", \"_aspect.tif\")\n",
    "    geomorph_tif = tif_file.replace(\".tif\", \"_geomorph.tif\")\n",
    "\n",
    "    if not os.path.exists(slope_tif):\n",
    "        wbt.slope(dem=tif_file, output=slope_tif, zfactor=1.0, units=\"degrees\")\n",
    "    if not os.path.exists(aspect_tif):\n",
    "        wbt.aspect(dem=tif_file, output=aspect_tif)\n",
    "    if not os.path.exists(geomorph_tif):\n",
    "        wbt.geomorphons(dem=tif_file, output=geomorph_tif, search=3, threshold=0.0, forms=True)\n",
    "\n",
    "    return slope_tif, aspect_tif, geomorph_tif\n",
    "\n",
    "# ---------------------------\n",
    "# Extract raster value\n",
    "# ---------------------------\n",
    "def extract_value(raster, lat, lon):\n",
    "    if raster is None or not os.path.exists(raster):\n",
    "        return None\n",
    "    with rasterio.open(raster) as src:\n",
    "        for val in src.sample([(lon, lat)]):\n",
    "            return float(val[0])\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline for GeoJSON\n",
    "# ---------------------------\n",
    "def enrich_geojson(input_geojson, output_geojson, out_dir=\"dem_tiles\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    gdf = gpd.read_file(input_geojson)\n",
    "\n",
    "    # Add expected cols\n",
    "    for col in [\"dem\", \"slope\", \"aspect\", \"geomorphon\", \"dem_source\", \"geomorphon_class\"]:\n",
    "        if col not in gdf.columns:\n",
    "            gdf[col] = None\n",
    "\n",
    "    # Collect centroids\n",
    "    centroids = gdf.geometry.centroid\n",
    "    coords = [(pt.y, pt.x) for pt in centroids]  # lat, lon\n",
    "\n",
    "    # Step 1: collect needed tiles\n",
    "    needed_tiles = {}\n",
    "    for (lat, lon) in tqdm(coords, desc=\"Collecting tiles\"):\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid and tid not in needed_tiles:\n",
    "            needed_tiles[tid] = (lat, lon)\n",
    "\n",
    "    # Step 2: prepare tiles\n",
    "    print(\"Tiles needed: \", len(needed_tiles))\n",
    "    downloaded = {}\n",
    "    for tid, (lat, lon) in tqdm(needed_tiles.items(), desc=\"Preparing tiles\"):\n",
    "        tif_path = os.path.join(out_dir, f\"{tid}.tif\")\n",
    "        if os.path.exists(tif_path):\n",
    "            downloaded[tid] = ([tif_path], \"Local\")\n",
    "        else:\n",
    "            zip_paths, source = download_dem_point(lat, lon, out_dir=out_dir)\n",
    "            if zip_paths:\n",
    "                tifs = [prepare_tif(zp) for zp in zip_paths]\n",
    "                downloaded[tid] = (tifs, source)\n",
    "\n",
    "    # Step 3: run Whitebox\n",
    "    tile_results = {}\n",
    "    for tid, (tifs, source) in tqdm(downloaded.items(), desc=\"Running Whitebox\"):\n",
    "        for tif in tifs:\n",
    "            slope_tif, aspect_tif, geomorph_tif = run_whitebox(tif)\n",
    "            tile_results[tid] = (tif, slope_tif, aspect_tif, geomorph_tif, source)\n",
    "\n",
    "    # Step 4: extract values for each centroid\n",
    "    geomorph_classes = {\n",
    "        1: \"flat\", 2: \"summit\", 3: \"ridge\", 4: \"shoulder\", 5: \"spur\",\n",
    "        6: \"slope\", 7: \"hollow\", 8: \"footslope\", 9: \"valley\", 10: \"pit\"\n",
    "    }\n",
    "\n",
    "    for idx, (lat, lon) in enumerate(tqdm(coords, desc=\"Extracting values\")):\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid is None or tid not in tile_results:\n",
    "            continue\n",
    "        tif, slope_tif, aspect_tif, geomorph_tif, source = tile_results[tid]\n",
    "        gdf.at[idx, \"dem\"] = extract_value(tif, lat, lon)\n",
    "        gdf.at[idx, \"slope\"] = extract_value(slope_tif, lat, lon)\n",
    "        gdf.at[idx, \"aspect\"] = extract_value(aspect_tif, lat, lon)\n",
    "        gdf.at[idx, \"geomorphon\"] = extract_value(geomorph_tif, lat, lon)\n",
    "        gdf.at[idx, \"dem_source\"] = source\n",
    "        gdf.at[idx, \"geomorphon_class\"] = geomorph_classes.get(gdf.at[idx, \"geomorphon\"], None)\n",
    "\n",
    "    # Save enriched GeoJSON\n",
    "    gdf.to_file(output_geojson, driver=\"GeoJSON\")\n",
    "    print(f\"✅ Done! Saved {output_geojson}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    enrich_geojson(\n",
    "        \"data/polygons.geojson\",\n",
    "        \"data/polygons_with_topography.geojson\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elevation\n",
    "import os\n",
    "\n",
    "# Output folder\n",
    "output_dir = \"tuscany_tiles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Tuscany bounding box\n",
    "tuscany_bounds = (9.5, 42.2, 12.5, 44.5)\n",
    "\n",
    "# Tile size in degrees (adjust if needed)\n",
    "tile_size = 0.5  \n",
    "\n",
    "# Function to split bbox into smaller tiles\n",
    "def split_bbox(bounds, step):\n",
    "    min_lon, min_lat, max_lon, max_lat = bounds\n",
    "    tiles = []\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lat = min_lat\n",
    "        while lat < max_lat:\n",
    "            tile = (\n",
    "                lon,\n",
    "                lat,\n",
    "                min(lon + step, max_lon),\n",
    "                min(lat + step, max_lat),\n",
    "            )\n",
    "            tiles.append(tile)\n",
    "            lat += step\n",
    "        lon += step\n",
    "    return tiles\n",
    "\n",
    "# Split Tuscany into smaller tiles\n",
    "tiles = split_bbox(tuscany_bounds, tile_size)\n",
    "\n",
    "print(f\"Downloading {len(tiles)} tiles...\")\n",
    "\n",
    "# Download each tile\n",
    "for i, b in enumerate(tiles, 1):\n",
    "    out_file = os.path.join(output_dir, f\"tile_{i}.tif\")\n",
    "    print(f\"Tile {i}/{len(tiles)} -> {out_file} Bounds: {b}\")\n",
    "    elevation.clip(bounds=b, output=out_file, product=\"SRTM1\")\n",
    "\n",
    "print(\"✅ All tiles downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get elevation for one point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elevation \n",
    "\n",
    "def point_to_bounds(point, buffer_size):\n",
    "    \"\"\"\n",
    "    Convert a point to a bounding box with a specified buffer size.\n",
    "    \n",
    "    Args:\n",
    "    - point (tuple): The point coordinates as (lon, lat).\n",
    "    - buffer_size (float): The buffer size in degrees.\n",
    "    \n",
    "    Returns:\n",
    "    - bounds (tuple): The bounding box coordinates as (min_lon, min_lat, max_lon, max_lat).\n",
    "    \"\"\"\n",
    "    lon, lat = point\n",
    "    min_lon = lon - buffer_size\n",
    "    max_lon = lon + buffer_size\n",
    "    min_lat = lat - buffer_size\n",
    "    max_lat = lat + buffer_size\n",
    "    return (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Input point coordinates and buffer size\n",
    "point = (12.5, 41.9)  # Example point coordinates (lon, lat)\n",
    "buffer_size = 0.0005  # Example buffer size in degrees\n",
    "\n",
    "# Convert point to bounding box\n",
    "bounds = point_to_bounds(point, buffer_size)\n",
    "print(bounds)\n",
    "\n",
    "# Perform clipping with the bounding box\n",
    "elevation.clip(bounds=bounds, output='/home/federico/Documents/fungi/Rome-small.tif') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append elevation and aspect values to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import elevation\n",
    "import rasterio\n",
    "import richdem as rd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#data_path = \"data/spain_positive_ready.csv\"\n",
    "#output_path = \"data/spain_positive_ready_with_el_aspect.csv\"\n",
    "\n",
    "data_path = \"data/negative_samples.csv\"\n",
    "output_path = \"data/negative_samples_el.csv\"\n",
    "\n",
    "\n",
    "def point_to_bounds(point, buffer_size):\n",
    "    \"\"\"\n",
    "    Convert a point to a bounding box with a specified buffer size.\n",
    "    \n",
    "    Args:\n",
    "    - point (tuple): The point coordinates as (lon, lat).\n",
    "    - buffer_size (float): The buffer size in degrees.\n",
    "    \n",
    "    Returns:\n",
    "    - bounds (tuple): The bounding box coordinates as (min_lon, min_lat, max_lon, max_lat).\n",
    "    \"\"\"\n",
    "    lon, lat = point\n",
    "    min_lon = lon - buffer_size\n",
    "    max_lon = lon + buffer_size\n",
    "    min_lat = lat - buffer_size\n",
    "    max_lat = lat + buffer_size\n",
    "    return (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Initialize the 'elevation' and 'aspect' columns with NaN values\n",
    "df['elevation'] = float('nan')\n",
    "df['aspect'] = float('nan')\n",
    "\n",
    "buffer_size = 0.0001\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for idx, row in df.iterrows():\n",
    "    # Extract coordinates\n",
    "    lon, lat = row['x'], row['y']\n",
    "    \n",
    "    # Convert the point to a bounding box with a buffer size\n",
    "    bounds = point_to_bounds((lon, lat), buffer_size)\n",
    "\n",
    "    # Perform clipping with the bounding box and save elevation data to a temporary file\n",
    "    elevation.clip(bounds=bounds, output='/home/federico/Documents/Github/ShroomRadar/temp/elev.tif')\n",
    "    \n",
    "    # Read the clipped elevation data using rasterio\n",
    "    with rasterio.open('/home/federico/Documents/Github/ShroomRadar/temp/elev.tif') as src:\n",
    "        # Read elevation data into an array\n",
    "        clipped_data = src.read(1)\n",
    "        \n",
    "        # Calculate the average elevation\n",
    "        average_elevation = np.nanmean(clipped_data)\n",
    "        \n",
    "        # Convert the elevation array to a richdem Digital Elevation Model (DEM)\n",
    "        dem = rd.rdarray(clipped_data, no_data=np.nan)\n",
    "        dem.projection = src.crs.to_string()\n",
    "        \n",
    "        # Calculate aspect using richdem\n",
    "        aspect_array = rd.TerrainAttribute(dem, attrib='aspect')\n",
    "        \n",
    "        # Calculate the mean aspect for this bounding box\n",
    "        mean_aspect = np.nanmean(aspect_array)\n",
    "    \n",
    "    # Update the DataFrame\n",
    "    df.at[idx, 'elevation'] = average_elevation\n",
    "    df.at[idx, 'aspect'] = mean_aspect\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file after each iteration\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Elevation and aspect calculations completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# windows version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SILENT VERSION: No debug output at all\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import richdem as rd\n",
    "from tqdm import tqdm\n",
    "import srtm\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "# Suppress ALL output including richdem debug prints\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['RICHDEM_QUIET'] = '1'\n",
    "\n",
    "# Context manager to suppress stdout temporarily\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "data_path = \"data/negative_samples.csv\"\n",
    "output_path = \"data/negative_samples_el_aspect.csv\"\n",
    "\n",
    "def get_elevation_and_aspect_silent(elevation_data, lat, lon, buffer_size=0.001):\n",
    "    \"\"\"Completely silent elevation and aspect calculation.\"\"\"\n",
    "    try:\n",
    "        # Get elevation\n",
    "        elevation = elevation_data.get_elevation(lat, lon)\n",
    "        if elevation is None:\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Create small grid for aspect\n",
    "        grid_size = 5\n",
    "        half_buffer = buffer_size / 2\n",
    "        \n",
    "        lats = np.linspace(lat - half_buffer, lat + half_buffer, grid_size)\n",
    "        lons = np.linspace(lon - half_buffer, lon + half_buffer, grid_size)\n",
    "        \n",
    "        elevation_grid = np.full((grid_size, grid_size), np.nan)\n",
    "        \n",
    "        for i, lat_sample in enumerate(lats):\n",
    "            for j, lon_sample in enumerate(lons):\n",
    "                elev = elevation_data.get_elevation(lat_sample, lon_sample)\n",
    "                if elev is not None:\n",
    "                    elevation_grid[i, j] = elev\n",
    "        \n",
    "        # Check if we have enough data for aspect\n",
    "        if np.sum(~np.isnan(elevation_grid)) < 9:\n",
    "            return float(elevation), np.nan\n",
    "        \n",
    "        # Calculate aspect with complete output suppression\n",
    "        with suppress_stdout():\n",
    "            dem = rd.rdarray(elevation_grid, no_data=np.nan)\n",
    "            pixel_size = buffer_size / grid_size\n",
    "            \n",
    "            dem.geotransform = [\n",
    "                lon - half_buffer, pixel_size, 0,\n",
    "                lat + half_buffer, 0, -pixel_size\n",
    "            ]\n",
    "            \n",
    "            aspect_array = rd.TerrainAttribute(dem, attrib='aspect')\n",
    "            aspect = np.nanmean(aspect_array)\n",
    "        \n",
    "        return float(elevation), float(aspect) if not np.isnan(aspect) else np.nan\n",
    "        \n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Initialize columns\n",
    "if 'elevation' not in df.columns:\n",
    "    df['elevation'] = float('nan')\n",
    "if 'aspect' not in df.columns:\n",
    "    df['aspect'] = float('nan')\n",
    "\n",
    "# Find rows to process\n",
    "missing_data = df['elevation'].isna() | df['aspect'].isna()\n",
    "rows_to_process = df[missing_data]\n",
    "\n",
    "print(f\"Processing {len(rows_to_process)} rows...\")\n",
    "\n",
    "# Initialize SRTM once\n",
    "print(\"Initializing SRTM data...\")\n",
    "elevation_data = srtm.get_data()\n",
    "print(\"Starting processing...\")\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 1000\n",
    "total_batches = (len(rows_to_process) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(rows_to_process))\n",
    "    batch_rows = rows_to_process.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Process batch with clean progress bar\n",
    "    progress_bar = tqdm(batch_rows.index, \n",
    "                       desc=f\"Batch {batch_idx + 1}/{total_batches}\", \n",
    "                       leave=True,\n",
    "                       ncols=80)\n",
    "    \n",
    "    for idx in progress_bar:\n",
    "        row = df.loc[idx]\n",
    "        lon, lat = row['x'], row['y']\n",
    "        \n",
    "        elevation, aspect = get_elevation_and_aspect_silent(elevation_data, lat, lon)\n",
    "        \n",
    "        df.at[idx, 'elevation'] = elevation\n",
    "        df.at[idx, 'aspect'] = aspect\n",
    "    \n",
    "    # Save progress\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Simple completion update\n",
    "    completed = min((batch_idx + 1) * batch_size, len(rows_to_process))\n",
    "    print(f\"✓ Completed {completed}/{len(rows_to_process)} rows\")\n",
    "\n",
    "# Final summary\n",
    "elevation_count = len(df[~df['elevation'].isna()])\n",
    "aspect_count = len(df[~df['aspect'].isna()])\n",
    "\n",
    "print(f\"\\n🎉 All done!\")\n",
    "print(f\"📈 Elevation: {elevation_count}/{len(df)} points\")\n",
    "print(f\"🧭 Aspect: {aspect_count}/{len(df)} points\")\n",
    "print(f\"💾 Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geojson input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SILENT VERSION: GeoJSON Polygon Adaptation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import richdem as rd\n",
    "from tqdm import tqdm\n",
    "import srtm\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import contextlib\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Suppress ALL output including richdem debug prints\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['RICHDEM_QUIET'] = '1'\n",
    "\n",
    "# Context manager to suppress stdout temporarily\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# File paths - UPDATE THESE\n",
    "input_geojson_path = \"docker//data//base_maps//basque_country_05.geojson\"\n",
    "output_geojson_path = \"docker//data//base_maps//basque_country_05_with_elevation_aspect.geojson\"\n",
    "\n",
    "def get_elevation_and_aspect_silent(elevation_data, lat, lon, buffer_size=0.001):\n",
    "    \"\"\"Completely silent elevation and aspect calculation.\"\"\"\n",
    "    try:\n",
    "        # Get elevation\n",
    "        elevation = elevation_data.get_elevation(lat, lon)\n",
    "        if elevation is None:\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Create small grid for aspect\n",
    "        grid_size = 5\n",
    "        half_buffer = buffer_size / 2\n",
    "        \n",
    "        lats = np.linspace(lat - half_buffer, lat + half_buffer, grid_size)\n",
    "        lons = np.linspace(lon - half_buffer, lon + half_buffer, grid_size)\n",
    "        \n",
    "        elevation_grid = np.full((grid_size, grid_size), np.nan)\n",
    "        \n",
    "        for i, lat_sample in enumerate(lats):\n",
    "            for j, lon_sample in enumerate(lons):\n",
    "                elev = elevation_data.get_elevation(lat_sample, lon_sample)\n",
    "                if elev is not None:\n",
    "                    elevation_grid[i, j] = elev\n",
    "        \n",
    "        # Check if we have enough data for aspect\n",
    "        if np.sum(~np.isnan(elevation_grid)) < 9:\n",
    "            return float(elevation), np.nan\n",
    "        \n",
    "        # Calculate aspect with complete output suppression\n",
    "        with suppress_stdout():\n",
    "            dem = rd.rdarray(elevation_grid, no_data=np.nan)\n",
    "            pixel_size = buffer_size / grid_size\n",
    "            \n",
    "            dem.geotransform = [\n",
    "                lon - half_buffer, pixel_size, 0,\n",
    "                lat + half_buffer, 0, -pixel_size\n",
    "            ]\n",
    "            \n",
    "            aspect_array = rd.TerrainAttribute(dem, attrib='aspect')\n",
    "            aspect = np.nanmean(aspect_array)\n",
    "        \n",
    "        return float(elevation), float(aspect) if not np.isnan(aspect) else np.nan\n",
    "        \n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# Load GeoJSON data\n",
    "print(\"Loading GeoJSON data...\")\n",
    "gdf = gpd.read_file(input_geojson_path)\n",
    "\n",
    "# Ensure we're working with polygons\n",
    "if not all(gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])):\n",
    "    print(\"Warning: Not all geometries are polygons!\")\n",
    "\n",
    "# Calculate centroids\n",
    "print(\"Calculating polygon centroids...\")\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "\n",
    "# Extract centroid coordinates\n",
    "gdf['centroid_lon'] = gdf['centroid'].x\n",
    "gdf['centroid_lat'] = gdf['centroid'].y\n",
    "\n",
    "# Initialize elevation and aspect columns if they don't exist\n",
    "if 'elevation' not in gdf.columns:\n",
    "    gdf['elevation'] = float('nan')\n",
    "if 'aspect' not in gdf.columns:\n",
    "    gdf['aspect'] = float('nan')\n",
    "\n",
    "# Find rows to process (missing elevation or aspect data)\n",
    "missing_data = gdf['elevation'].isna() | gdf['aspect'].isna()\n",
    "rows_to_process = gdf[missing_data]\n",
    "\n",
    "print(f\"Processing {len(rows_to_process)} polygons...\")\n",
    "\n",
    "# Initialize SRTM once\n",
    "print(\"Initializing SRTM data...\")\n",
    "elevation_data = srtm.get_data()\n",
    "print(\"Starting processing...\")\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 1000\n",
    "total_batches = (len(rows_to_process) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(rows_to_process))\n",
    "    batch_rows = rows_to_process.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Process batch with clean progress bar\n",
    "    progress_bar = tqdm(batch_rows.index, \n",
    "                       desc=f\"Batch {batch_idx + 1}/{total_batches}\", \n",
    "                       leave=True,\n",
    "                       ncols=80)\n",
    "    \n",
    "    for idx in progress_bar:\n",
    "        row = gdf.loc[idx]\n",
    "        lon, lat = row['centroid_lon'], row['centroid_lat']\n",
    "        \n",
    "        elevation, aspect = get_elevation_and_aspect_silent(elevation_data, lat, lon)\n",
    "        \n",
    "        gdf.at[idx, 'elevation'] = elevation\n",
    "        gdf.at[idx, 'aspect'] = aspect\n",
    "    \n",
    "    # Save progress\n",
    "    # Drop the temporary centroid point column before saving\n",
    "    gdf_to_save = gdf.drop(columns=['centroid'])\n",
    "    gdf_to_save.to_file(output_geojson_path, driver='GeoJSON')\n",
    "    \n",
    "    # Simple completion update\n",
    "    completed = min((batch_idx + 1) * batch_size, len(rows_to_process))\n",
    "    print(f\"✓ Completed {completed}/{len(rows_to_process)} polygons\")\n",
    "\n",
    "# Final cleanup and summary\n",
    "gdf_final = gdf.drop(columns=['centroid', 'centroid_lon', 'centroid_lat'])\n",
    "gdf_final.to_file(output_geojson_path, driver='GeoJSON')\n",
    "\n",
    "elevation_count = len(gdf_final[~gdf_final['elevation'].isna()])\n",
    "aspect_count = len(gdf_final[~gdf_final['aspect'].isna()])\n",
    "\n",
    "print(f\"\\n🎉 All done!\")\n",
    "print(f\"📈 Elevation: {elevation_count}/{len(gdf_final)} polygons\")\n",
    "print(f\"🧭 Aspect: {aspect_count}/{len(gdf_final)} polygons\")\n",
    "print(f\"💾 Saved: {output_geojson_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append elevation to geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import elevation \n",
    "\n",
    "def get_mean_elevation(geometry):\n",
    "    # Extract the bounding box coordinates of the polygon\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "\n",
    "    # Clip the elevation data to the extent of the polygon\n",
    "    elevation.clip((minx, miny, maxx, maxy), output='/home/federico/Documents/Github/ShroomRadar/temp/elev.tif')\n",
    "\n",
    "    # Read the clipped elevation data using rasterio\n",
    "    with rasterio.open('/home/federico/Documents/Github/ShroomRadar/temp/elev.tif') as src:\n",
    "        clipped_data = src.read(1)  # Assuming elevation data is stored in the first band\n",
    "\n",
    "    # Calculate the mean elevation\n",
    "    mean_elevation = np.mean(clipped_data)\n",
    "\n",
    "    return mean_elevation\n",
    "\n",
    "\n",
    "# Load the GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('data/siena_with_mode_values.geojson')\n",
    "\n",
    "# Create an empty list to store the mean elevations\n",
    "mean_elevations = []\n",
    "\n",
    "# Iterate over each polygon in the GeoDataFrame\n",
    "for index, row in spain.iterrows():\n",
    "    # Calculate the mean elevation for the current polygon\n",
    "    try:\n",
    "        mean_elevation = get_mean_elevation(row['geometry'])\n",
    "    # Append the mean elevation to the list\n",
    "        mean_elevations.append(mean_elevation)\n",
    "        print(mean_elevations)\n",
    "    except:\n",
    "        mean_elevations.append(np.nan)\n",
    "        print(mean_elevations)\n",
    "\n",
    "# Add the list of mean elevations as a new column in the GeoDataFrame\n",
    "spain['mean_elevation'] = mean_elevations\n",
    "\n",
    "# Save the GeoDataFrame to a new GeoJSON file\n",
    "spain.to_file('siena_ready_05km.geojson', driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import elevation \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_mean_elevation(geometry):\n",
    "    # Extract the bounding box coordinates of the polygon\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "\n",
    "    # Clip the elevation data to the extent of the polygon\n",
    "    elevation.clip((minx, miny, maxx, maxy), output='/home/federico/Documents/Github/ShroomRadar/temp')\n",
    "\n",
    "    # Read the clipped elevation data using rasterio\n",
    "    with rasterio.open('/home/federico/Documents/Github/ShroomRadar/temp') as src:\n",
    "        clipped_data = src.read(1)  # Assuming elevation data is stored in the first band\n",
    "\n",
    "    # Calculate the mean elevation\n",
    "    mean_elevation = np.mean(clipped_data)\n",
    "\n",
    "    return mean_elevation\n",
    "\n",
    "# Load the GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('data/siena_with_mode_values.geojson')\n",
    "\n",
    "# Create an empty list to store the mean elevations\n",
    "mean_elevations = []\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over each polygon in the GeoDataFrame\n",
    "for index, row in tqdm(spain.iterrows(), total=len(spain)):\n",
    "    # Calculate the mean elevation for the current polygon\n",
    "    mean_elevation = get_mean_elevation(row['geometry'])\n",
    "    # Append the mean elevation to the list\n",
    "    mean_elevations.append(mean_elevation)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print total time taken\n",
    "print(\"Total time taken: {:.2f} seconds\".format(end_time - start_time))\n",
    "\n",
    "# Add the list of mean elevations as a new column in the GeoDataFrame\n",
    "spain['mean_elevation'] = mean_elevations\n",
    "\n",
    "# Save the GeoDataFrame to a new GeoJSON file\n",
    "spain.to_file('data/siena_ready_05km.geojson', driver='GeoJSON')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
