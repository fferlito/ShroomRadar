{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import earthaccess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Login using environment variables\n",
    "auth = earthaccess.login(strategy=\"environment\")\n",
    "\n",
    "# Search for Rome area\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"SRTMGL1\",\n",
    "    version=\"003\",\n",
    "    bounding_box=(12.35, 41.8, 12.65, 42.0)  # min lon, min lat, max lon, max lat\n",
    ")\n",
    "\n",
    "# Download tiles\n",
    "paths = earthaccess.download(results, \"./srtm_tiles\")\n",
    "print(paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append variables for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from whitebox.whitebox_tools import WhiteboxTools\n",
    "import earthaccess\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# ---------------------------\n",
    "# Tile ID utilities\n",
    "# ---------------------------\n",
    "def tile_id_from_coords(lat, lon):\n",
    "    \"\"\"Convert coords to tile ID (e.g. N40W106).\"\"\"\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return None\n",
    "    ns = \"N\" if lat >= 0 else \"S\"\n",
    "    ew = \"E\" if lon >= 0 else \"W\"\n",
    "    return f\"{ns}{abs(int(lat)):02d}{ew}{abs(int(lon)):03d}\"\n",
    "\n",
    "# ---------------------------\n",
    "# DEM Download\n",
    "# ---------------------------\n",
    "def download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=\"dem_tiles\", prefer=\"SRTMGL1\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    earthaccess.login(strategy=\"environment\", persist=True)\n",
    "    dataset = (\"SRTMGL1\", \"003\") if prefer == \"SRTMGL1\" else (\"COPDEM_GLO_30\", \"001\")\n",
    "    try:\n",
    "        results = earthaccess.search_data(\n",
    "            short_name=dataset[0],\n",
    "            version=dataset[1],\n",
    "            bounding_box=(min_lon, min_lat, max_lon, max_lat),\n",
    "            count=10\n",
    "        )\n",
    "    except IndexError:\n",
    "        return []\n",
    "    if not results or len(results) == 0:\n",
    "        return []\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
    "        paths = earthaccess.download(results, out_dir)\n",
    "    return paths\n",
    "\n",
    "def download_dem_point(lat, lon, out_dir=\"dem_tiles\", buffer=0.1):\n",
    "    min_lon = max(-180.0, lon - buffer)\n",
    "    max_lon = min(180.0, lon + buffer)\n",
    "    min_lat = max(-90.0, lat - buffer)\n",
    "    max_lat = min(90.0, lat + buffer)\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"SRTMGL1\")\n",
    "    if paths:\n",
    "        return paths, \"SRTM\"\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"COPDEM\")\n",
    "    if paths:\n",
    "        return paths, \"Copernicus\"\n",
    "    return [], \"None\"\n",
    "\n",
    "# ---------------------------\n",
    "# HGT â†’ GeoTIFF\n",
    "# ---------------------------\n",
    "def parse_hgt_bounds(hgt_path):\n",
    "    name = os.path.splitext(os.path.basename(hgt_path))[0]\n",
    "    m = re.match(r'([NS])(\\d{1,2})([EW])(\\d{1,3})', name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse HGT name: {hgt_path}\")\n",
    "    lat_sign = 1 if m.group(1).upper() == 'N' else -1\n",
    "    lon_sign = 1 if m.group(3).upper() == 'E' else -1\n",
    "    lat0 = lat_sign * int(m.group(2))\n",
    "    lon0 = lon_sign * int(m.group(4))\n",
    "    west, south = float(lon0), float(lat0)\n",
    "    east, north = west + 1.0, south + 1.0\n",
    "    return west, south, east, north\n",
    "\n",
    "def hgt_to_gtiff(hgt_path, tif_path):\n",
    "    west, south, east, north = parse_hgt_bounds(hgt_path)\n",
    "    nbytes = os.path.getsize(hgt_path)\n",
    "    side = int(np.sqrt(nbytes // 2))\n",
    "    if side not in (3601, 1201):\n",
    "        raise ValueError(f\"Unexpected HGT side length: {side}\")\n",
    "    data = np.fromfile(hgt_path, dtype=\">i2\").reshape((side, side))\n",
    "    data = data[:-1, :-1]\n",
    "    res = 1.0 / (side - 1)\n",
    "    transform = from_origin(west, north, res, res)\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": data.shape[0],\n",
    "        \"width\": data.shape[1],\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"int16\",\n",
    "        \"crs\": \"EPSG:4326\",\n",
    "        \"transform\": transform,\n",
    "        \"nodata\": -32768,\n",
    "        \"tiled\": True,\n",
    "        \"compress\": \"LZW\"\n",
    "    }\n",
    "    with rasterio.open(tif_path, \"w\", **profile) as dst:\n",
    "        dst.write(data, 1)\n",
    "\n",
    "def prepare_tif(path):\n",
    "    \"\"\"Unpack zip/HGT and convert to GeoTIFF. Remove raw files after processing.\"\"\"\n",
    "    if path.lower().endswith(\".tif\"):\n",
    "        return os.path.abspath(path)\n",
    "    if path.lower().endswith(\".zip\"):\n",
    "        tif_out, hgt_out = None, None\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            tifs = [m for m in z.namelist() if m.lower().endswith(\".tif\")]\n",
    "            if tifs:\n",
    "                tif_out = os.path.join(os.path.dirname(path), os.path.basename(tifs[0]))\n",
    "                if not os.path.exists(tif_out):\n",
    "                    z.extract(tifs[0], os.path.dirname(path))\n",
    "                tif_out = os.path.abspath(tif_out)\n",
    "            else:\n",
    "                hgts = [m for m in z.namelist() if m.lower().endswith(\".hgt\")]\n",
    "                if hgts:\n",
    "                    hgt_out = os.path.join(os.path.dirname(path), os.path.basename(hgts[0]))\n",
    "                    if not os.path.exists(hgt_out):\n",
    "                        z.extract(hgts[0], os.path.dirname(path))\n",
    "                    tif_out = hgt_out.replace(\".hgt\", \".tif\")\n",
    "                    if not os.path.exists(tif_out):\n",
    "                        hgt_to_gtiff(hgt_out, tif_out)\n",
    "                    try:\n",
    "                        os.remove(hgt_out)\n",
    "                    except PermissionError:\n",
    "                        pass\n",
    "                    tif_out = os.path.abspath(tif_out)\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        if tif_out:\n",
    "            return tif_out\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .tif or .hgt in {path}\")\n",
    "    raise FileNotFoundError(f\"Unsupported DEM format: {path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Whitebox + helpers\n",
    "# ---------------------------\n",
    "wbt = WhiteboxTools()\n",
    "wbt.verbose = False\n",
    "\n",
    "def valid_raster(path):\n",
    "    \"\"\"Check if a raster exists, non-empty, and can be opened by rasterio.\"\"\"\n",
    "    if not path or not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        return False\n",
    "    try:\n",
    "        with rasterio.open(path) as src:\n",
    "            _ = src.count\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def run_whitebox(tif_file, need_slope=False, need_aspect=False, need_geomorph=False):\n",
    "    tif_file = os.path.abspath(tif_file).replace(\"\\\\\", \"/\")\n",
    "    base, _ = os.path.splitext(tif_file)\n",
    "    slope_tif = f\"{base}_slope.tif\"\n",
    "    aspect_tif = f\"{base}_aspect.tif\"\n",
    "    geomorph_tif = f\"{base}_geomorph.tif\"\n",
    "    if need_slope and not valid_raster(slope_tif):\n",
    "        wbt.slope(dem=tif_file, output=slope_tif, zfactor=1.0, units=\"degrees\")\n",
    "    if need_aspect and not valid_raster(aspect_tif):\n",
    "        wbt.aspect(dem=tif_file, output=aspect_tif)\n",
    "    if need_geomorph and not valid_raster(geomorph_tif):\n",
    "        wbt.geomorphons(dem=tif_file, output=geomorph_tif, search=3, threshold=0.0, forms=True)\n",
    "    return (\n",
    "        tif_file,\n",
    "        slope_tif if valid_raster(slope_tif) else None,\n",
    "        aspect_tif if valid_raster(aspect_tif) else None,\n",
    "        geomorph_tif if valid_raster(geomorph_tif) else None\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# Extract raster value\n",
    "# ---------------------------\n",
    "def extract_value(raster, lat, lon):\n",
    "    if not valid_raster(raster):\n",
    "        return None\n",
    "    try:\n",
    "        with rasterio.open(raster) as src:\n",
    "            nd = src.nodata\n",
    "            for val in src.sample([(lon, lat)]):\n",
    "                v = float(val[0])\n",
    "                if np.isnan(v) or (nd is not None and v == nd):\n",
    "                    return None\n",
    "                return v\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline\n",
    "# ---------------------------\n",
    "def enrich_csv(input_csv, output_csv, out_dir=\"dem_tiles\", skip_processing=False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    for col in [\"dem\", \"slope\", \"aspect\", \"geomorphon\", \"dem_source\", \"geomorphon_class\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Step 1: Collect per-tile needs\n",
    "    tile_needs = {}\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Scanning CSV\"):\n",
    "        lat, lon = row[\"y\"], row[\"x\"]\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        base = os.path.join(out_dir, tid)\n",
    "        slope_file, aspect_file, geomorph_file = f\"{base}_slope.tif\", f\"{base}_aspect.tif\", f\"{base}_geomorph.tif\"\n",
    "        if tid not in tile_needs:\n",
    "            tile_needs[tid] = {\"dem\": False, \"slope\": False, \"aspect\": False, \"geomorphon\": False}\n",
    "        if pd.isna(row.get(\"dem\")) and not valid_raster(f\"{base}.tif\"):\n",
    "            tile_needs[tid][\"dem\"] = True\n",
    "        if pd.isna(row.get(\"slope\")) and not valid_raster(slope_file):\n",
    "            tile_needs[tid][\"slope\"] = True\n",
    "        if pd.isna(row.get(\"aspect\")) and not valid_raster(aspect_file):\n",
    "            tile_needs[tid][\"aspect\"] = True\n",
    "        if pd.isna(row.get(\"geomorphon\")) and not valid_raster(geomorph_file):\n",
    "            tile_needs[tid][\"geomorphon\"] = True\n",
    "    tile_needs = {tid: needs for tid, needs in tile_needs.items() if any(needs.values())}\n",
    "\n",
    "    # Step 2 & 3: Only run if skip_processing is False\n",
    "    downloaded = {}\n",
    "    tile_results = {}\n",
    "    if not skip_processing:\n",
    "        for tid, needs in tqdm(tile_needs.items(), desc=\"Preparing tiles\"):\n",
    "            local_tif = os.path.join(out_dir, f\"{tid}.tif\")\n",
    "            if valid_raster(local_tif):\n",
    "                downloaded[tid] = ([local_tif], \"Local\")\n",
    "                continue\n",
    "            m = re.match(r'([NS])(\\d{2})([EW])(\\d{3})', tid)\n",
    "            if not m:\n",
    "                continue\n",
    "            lat0 = int(m.group(2)) * (1 if m.group(1) == \"N\" else -1)\n",
    "            lon0 = int(m.group(4)) * (1 if m.group(3) == \"E\" else -1)\n",
    "            zip_paths, source = download_dem_point(lat0 + 0.5, lon0 + 0.5, out_dir=out_dir)\n",
    "            if zip_paths:\n",
    "                tifs = [prepare_tif(zp) for zp in zip_paths]\n",
    "                downloaded[tid] = (tifs, source)\n",
    "\n",
    "        for tid, (tifs, source) in tqdm(downloaded.items(), desc=\"Running Whitebox\"):\n",
    "            needs = tile_needs.get(tid, {})\n",
    "            for tif in tifs:\n",
    "                tif_path, slope_path, aspect_path, geomorph_path = run_whitebox(\n",
    "                    tif,\n",
    "                    need_slope=needs.get(\"slope\", False),\n",
    "                    need_aspect=needs.get(\"aspect\", False),\n",
    "                    need_geomorph=needs.get(\"geomorphon\", False)\n",
    "                )\n",
    "                tile_results[tid] = {\n",
    "                    \"tif\": tif_path,\n",
    "                    \"slope\": slope_path,\n",
    "                    \"aspect\": aspect_path,\n",
    "                    \"geomorphon\": geomorph_path,\n",
    "                    \"source\": source\n",
    "                }\n",
    "                break\n",
    "\n",
    "    # Step 4: Extract values from whatever exists\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting values\"):\n",
    "        lat, lon = row[\"y\"], row[\"x\"]\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        base = os.path.join(out_dir, tid)\n",
    "        tr = tile_results.get(tid, {\n",
    "            \"tif\": f\"{base}.tif\" if valid_raster(f\"{base}.tif\") else None,\n",
    "            \"slope\": f\"{base}_slope.tif\" if valid_raster(f\"{base}_slope.tif\") else None,\n",
    "            \"aspect\": f\"{base}_aspect.tif\" if valid_raster(f\"{base}_aspect.tif\") else None,\n",
    "            \"geomorphon\": f\"{base}_geomorph.tif\" if valid_raster(f\"{base}_geomorph.tif\") else None,\n",
    "            \"source\": \"Local\"\n",
    "        })\n",
    "        try:\n",
    "            if pd.isna(row.get(\"dem\")) and tr[\"tif\"]:\n",
    "                df.at[i, \"dem\"] = extract_value(tr[\"tif\"], lat, lon)\n",
    "                df.at[i, \"dem_source\"] = tr[\"source\"]\n",
    "            if pd.isna(row.get(\"slope\")) and tr[\"slope\"]:\n",
    "                df.at[i, \"slope\"] = extract_value(tr[\"slope\"], lat, lon)\n",
    "            if pd.isna(row.get(\"aspect\")) and tr[\"aspect\"]:\n",
    "                df.at[i, \"aspect\"] = extract_value(tr[\"aspect\"], lat, lon)\n",
    "            if pd.isna(row.get(\"geomorphon\")) and tr[\"geomorphon\"]:\n",
    "                df.at[i, \"geomorphon\"] = extract_value(tr[\"geomorphon\"], lat, lon)\n",
    "        except Exception:\n",
    "            # Skip row if any error during reading\n",
    "            continue\n",
    "\n",
    "    # Step 5: Decode geomorphons\n",
    "    geomorph_classes = {\n",
    "        1: \"flat\", 2: \"summit\", 3: \"ridge\", 4: \"shoulder\", 5: \"spur\",\n",
    "        6: \"slope\", 7: \"hollow\", 8: \"footslope\", 9: \"valley\", 10: \"pit\"\n",
    "    }\n",
    "    df[\"geomorphon_class\"] = df[\"geomorphon\"].map(geomorph_classes)\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"âœ… Done! Saved {output_csv}\")\n",
    "\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    enrich_csv(\n",
    "        \"data/negative_samples_within_land_10k_with_coords.csv\",\n",
    "        \"data/negative_samples_within_land_10k_with_coords_topography.csv\",\n",
    "        skip_processing=True  # ðŸš¨ Set to False if you want downloads/processing\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append variables for inference (on vector file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from whitebox.whitebox_tools import WhiteboxTools\n",
    "import earthaccess\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "import io\n",
    "import geopandas as gpd\n",
    "\n",
    "# ---------------------------\n",
    "# Tile ID utilities\n",
    "# ---------------------------\n",
    "def tile_id_from_coords(lat, lon):\n",
    "    \"\"\"Convert coords to tile ID (e.g. N40W106).\"\"\"\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return None\n",
    "    ns = \"N\" if lat >= 0 else \"S\"\n",
    "    ew = \"E\" if lon >= 0 else \"W\"\n",
    "    return f\"{ns}{abs(int(lat)):02d}{ew}{abs(int(lon)):03d}\"\n",
    "\n",
    "# ---------------------------\n",
    "# DEM Download\n",
    "# ---------------------------\n",
    "def download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=\"dem_tiles\", prefer=\"SRTMGL1\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    earthaccess.login(strategy=\"environment\", persist=True)\n",
    "\n",
    "    dataset = (\"SRTMGL1\", \"003\") if prefer == \"SRTMGL1\" else (\"COPDEM_GLO_30\", \"001\")\n",
    "\n",
    "    try:\n",
    "        results = earthaccess.search_data(\n",
    "            short_name=dataset[0],\n",
    "            version=dataset[1],\n",
    "            bounding_box=(min_lon, min_lat, max_lon, max_lat),\n",
    "            count=10\n",
    "        )\n",
    "    except IndexError:\n",
    "        return []\n",
    "\n",
    "    if not results or len(results) == 0:\n",
    "        return []\n",
    "\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
    "        paths = earthaccess.download(results, out_dir)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def download_dem_point(lat, lon, out_dir=\"dem_tiles\", buffer=0.1):\n",
    "    # Clamp to valid ranges\n",
    "    min_lon = max(-180.0, lon - buffer)\n",
    "    max_lon = min(180.0, lon + buffer)\n",
    "    min_lat = max(-90.0, lat - buffer)\n",
    "    max_lat = min(90.0, lat + buffer)\n",
    "\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"SRTMGL1\")\n",
    "    if paths:\n",
    "        return paths, \"SRTM\"\n",
    "    paths = download_dem_bbox(min_lon, min_lat, max_lon, max_lat, out_dir=out_dir, prefer=\"COPDEM\")\n",
    "    if paths:\n",
    "        return paths, \"Copernicus\"\n",
    "    return [], \"None\"\n",
    "\n",
    "# ---------------------------\n",
    "# HGT â†’ GeoTIFF\n",
    "# ---------------------------\n",
    "def parse_hgt_bounds(hgt_path):\n",
    "    name = os.path.splitext(os.path.basename(hgt_path))[0]\n",
    "    m = re.match(r'([NS])(\\d{1,2})([EW])(\\d{1,3})', name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse HGT name: {hgt_path}\")\n",
    "    lat_sign = 1 if m.group(1).upper() == 'N' else -1\n",
    "    lon_sign = 1 if m.group(3).upper() == 'E' else -1\n",
    "    lat0 = lat_sign * int(m.group(2))\n",
    "    lon0 = lon_sign * int(m.group(4))\n",
    "    west, south = float(lon0), float(lat0)\n",
    "    east, north = west + 1.0, south + 1.0\n",
    "    return west, south, east, north\n",
    "\n",
    "def hgt_to_gtiff(hgt_path, tif_path):\n",
    "    west, south, east, north = parse_hgt_bounds(hgt_path)\n",
    "    nbytes = os.path.getsize(hgt_path)\n",
    "    side = int(np.sqrt(nbytes // 2))\n",
    "    if side not in (3601, 1201):\n",
    "        raise ValueError(f\"Unexpected HGT side length: {side}\")\n",
    "    data = np.fromfile(hgt_path, dtype=\">i2\").reshape((side, side))\n",
    "    data = data[:-1, :-1]\n",
    "    res = 1.0 / (side - 1)\n",
    "\n",
    "    transform = from_origin(west, north, res, res)\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": data.shape[0],\n",
    "        \"width\": data.shape[1],\n",
    "        \"count\": 1,\n",
    "        \"dtype\": \"int16\",\n",
    "        \"crs\": \"EPSG:4326\",\n",
    "        \"transform\": transform,\n",
    "        \"nodata\": -32768,\n",
    "        \"tiled\": True,\n",
    "        \"compress\": \"LZW\"\n",
    "    }\n",
    "\n",
    "    with rasterio.open(tif_path, \"w\", **profile) as dst:\n",
    "        dst.write(data, 1)\n",
    "\n",
    "def prepare_tif(path):\n",
    "    \"\"\"Unpack zip/HGT and convert to GeoTIFF. Remove raw files after processing.\"\"\"\n",
    "    if path.lower().endswith(\".tif\"):\n",
    "        return os.path.abspath(path)\n",
    "\n",
    "    if path.lower().endswith(\".zip\"):\n",
    "        tif_out, hgt_out = None, None\n",
    "        with zipfile.ZipFile(path, \"r\") as z:\n",
    "            tifs = [m for m in z.namelist() if m.lower().endswith(\".tif\")]\n",
    "            if tifs:\n",
    "                tif_out = os.path.join(os.path.dirname(path), os.path.basename(tifs[0]))\n",
    "                if not os.path.exists(tif_out):\n",
    "                    z.extract(tifs[0], os.path.dirname(path))\n",
    "                tif_out = os.path.abspath(tif_out)\n",
    "            else:\n",
    "                hgts = [m for m in z.namelist() if m.lower().endswith(\".hgt\")]\n",
    "                if hgts:\n",
    "                    hgt_out = os.path.join(os.path.dirname(path), os.path.basename(hgts[0]))\n",
    "                    if not os.path.exists(hgt_out):\n",
    "                        z.extract(hgts[0], os.path.dirname(path))\n",
    "                    tif_out = hgt_out.replace(\".hgt\", \".tif\")\n",
    "                    if not os.path.exists(tif_out):\n",
    "                        hgt_to_gtiff(hgt_out, tif_out)\n",
    "                    try:\n",
    "                        os.remove(hgt_out)\n",
    "                    except PermissionError:\n",
    "                        pass\n",
    "                    tif_out = os.path.abspath(tif_out)\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except PermissionError:\n",
    "            pass\n",
    "        if tif_out:\n",
    "            return tif_out\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .tif or .hgt in {path}\")\n",
    "    raise FileNotFoundError(f\"Unsupported DEM format: {path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Whitebox\n",
    "# ---------------------------\n",
    "wbt = WhiteboxTools()\n",
    "wbt.verbose = False\n",
    "\n",
    "def run_whitebox(tif_file):\n",
    "    tif_file = os.path.abspath(tif_file).replace(\"\\\\\", \"/\")\n",
    "    slope_tif = tif_file.replace(\".tif\", \"_slope.tif\")\n",
    "    aspect_tif = tif_file.replace(\".tif\", \"_aspect.tif\")\n",
    "    geomorph_tif = tif_file.replace(\".tif\", \"_geomorph.tif\")\n",
    "\n",
    "    if not os.path.exists(slope_tif):\n",
    "        wbt.slope(dem=tif_file, output=slope_tif, zfactor=1.0, units=\"degrees\")\n",
    "    if not os.path.exists(aspect_tif):\n",
    "        wbt.aspect(dem=tif_file, output=aspect_tif)\n",
    "    if not os.path.exists(geomorph_tif):\n",
    "        wbt.geomorphons(dem=tif_file, output=geomorph_tif, search=3, threshold=0.0, forms=True)\n",
    "\n",
    "    return slope_tif, aspect_tif, geomorph_tif\n",
    "\n",
    "# ---------------------------\n",
    "# Extract raster value\n",
    "# ---------------------------\n",
    "def extract_value(raster, lat, lon):\n",
    "    if raster is None or not os.path.exists(raster):\n",
    "        return None\n",
    "    with rasterio.open(raster) as src:\n",
    "        for val in src.sample([(lon, lat)]):\n",
    "            return float(val[0])\n",
    "\n",
    "# ---------------------------\n",
    "# Main pipeline for GeoJSON\n",
    "# ---------------------------\n",
    "def enrich_geojson(input_geojson, output_geojson, out_dir=\"dem_tiles\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    gdf = gpd.read_file(input_geojson)\n",
    "\n",
    "    # Add expected cols\n",
    "    for col in [\"dem\", \"slope\", \"aspect\", \"geomorphon\", \"dem_source\", \"geomorphon_class\"]:\n",
    "        if col not in gdf.columns:\n",
    "            gdf[col] = None\n",
    "\n",
    "    # Collect centroids\n",
    "    centroids = gdf.geometry.centroid\n",
    "    coords = [(pt.y, pt.x) for pt in centroids]  # lat, lon\n",
    "\n",
    "    # Step 1: collect needed tiles\n",
    "    needed_tiles = {}\n",
    "    for (lat, lon) in tqdm(coords, desc=\"Collecting tiles\"):\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid and tid not in needed_tiles:\n",
    "            needed_tiles[tid] = (lat, lon)\n",
    "\n",
    "    # Step 2: prepare tiles\n",
    "    print(\"Tiles needed: \", len(needed_tiles))\n",
    "    downloaded = {}\n",
    "    for tid, (lat, lon) in tqdm(needed_tiles.items(), desc=\"Preparing tiles\"):\n",
    "        tif_path = os.path.join(out_dir, f\"{tid}.tif\")\n",
    "        if os.path.exists(tif_path):\n",
    "            downloaded[tid] = ([tif_path], \"Local\")\n",
    "        else:\n",
    "            zip_paths, source = download_dem_point(lat, lon, out_dir=out_dir)\n",
    "            if zip_paths:\n",
    "                tifs = [prepare_tif(zp) for zp in zip_paths]\n",
    "                downloaded[tid] = (tifs, source)\n",
    "\n",
    "    # Step 3: run Whitebox\n",
    "    tile_results = {}\n",
    "    for tid, (tifs, source) in tqdm(downloaded.items(), desc=\"Running Whitebox\"):\n",
    "        for tif in tifs:\n",
    "            slope_tif, aspect_tif, geomorph_tif = run_whitebox(tif)\n",
    "            tile_results[tid] = (tif, slope_tif, aspect_tif, geomorph_tif, source)\n",
    "\n",
    "    # Step 4: extract values for each centroid\n",
    "    geomorph_classes = {\n",
    "        1: \"flat\", 2: \"summit\", 3: \"ridge\", 4: \"shoulder\", 5: \"spur\",\n",
    "        6: \"slope\", 7: \"hollow\", 8: \"footslope\", 9: \"valley\", 10: \"pit\"\n",
    "    }\n",
    "\n",
    "    for idx, (lat, lon) in enumerate(tqdm(coords, desc=\"Extracting values\")):\n",
    "        tid = tile_id_from_coords(lat, lon)\n",
    "        if tid is None or tid not in tile_results:\n",
    "            continue\n",
    "        tif, slope_tif, aspect_tif, geomorph_tif, source = tile_results[tid]\n",
    "        gdf.at[idx, \"dem\"] = extract_value(tif, lat, lon)\n",
    "        gdf.at[idx, \"slope\"] = extract_value(slope_tif, lat, lon)\n",
    "        gdf.at[idx, \"aspect\"] = extract_value(aspect_tif, lat, lon)\n",
    "        gdf.at[idx, \"geomorphon\"] = extract_value(geomorph_tif, lat, lon)\n",
    "        gdf.at[idx, \"dem_source\"] = source\n",
    "        gdf.at[idx, \"geomorphon_class\"] = geomorph_classes.get(gdf.at[idx, \"geomorphon\"], None)\n",
    "\n",
    "    # Save enriched GeoJSON\n",
    "    gdf.to_file(output_geojson, driver=\"GeoJSON\")\n",
    "    print(f\"âœ… Done! Saved {output_geojson}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    enrich_geojson(\n",
    "        \"data/polygons.geojson\",\n",
    "        \"data/polygons_with_topography.geojson\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elevation\n",
    "import os\n",
    "\n",
    "# Output folder\n",
    "output_dir = \"tuscany_tiles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Tuscany bounding box\n",
    "tuscany_bounds = (9.5, 42.2, 12.5, 44.5)\n",
    "\n",
    "# Tile size in degrees (adjust if needed)\n",
    "tile_size = 0.5  \n",
    "\n",
    "# Function to split bbox into smaller tiles\n",
    "def split_bbox(bounds, step):\n",
    "    min_lon, min_lat, max_lon, max_lat = bounds\n",
    "    tiles = []\n",
    "    lon = min_lon\n",
    "    while lon < max_lon:\n",
    "        lat = min_lat\n",
    "        while lat < max_lat:\n",
    "            tile = (\n",
    "                lon,\n",
    "                lat,\n",
    "                min(lon + step, max_lon),\n",
    "                min(lat + step, max_lat),\n",
    "            )\n",
    "            tiles.append(tile)\n",
    "            lat += step\n",
    "        lon += step\n",
    "    return tiles\n",
    "\n",
    "# Split Tuscany into smaller tiles\n",
    "tiles = split_bbox(tuscany_bounds, tile_size)\n",
    "\n",
    "print(f\"Downloading {len(tiles)} tiles...\")\n",
    "\n",
    "# Download each tile\n",
    "for i, b in enumerate(tiles, 1):\n",
    "    out_file = os.path.join(output_dir, f\"tile_{i}.tif\")\n",
    "    print(f\"Tile {i}/{len(tiles)} -> {out_file} Bounds: {b}\")\n",
    "    elevation.clip(bounds=b, output=out_file, product=\"SRTM1\")\n",
    "\n",
    "print(\"âœ… All tiles downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get elevation for one point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elevation \n",
    "\n",
    "def point_to_bounds(point, buffer_size):\n",
    "    \"\"\"\n",
    "    Convert a point to a bounding box with a specified buffer size.\n",
    "    \n",
    "    Args:\n",
    "    - point (tuple): The point coordinates as (lon, lat).\n",
    "    - buffer_size (float): The buffer size in degrees.\n",
    "    \n",
    "    Returns:\n",
    "    - bounds (tuple): The bounding box coordinates as (min_lon, min_lat, max_lon, max_lat).\n",
    "    \"\"\"\n",
    "    lon, lat = point\n",
    "    min_lon = lon - buffer_size\n",
    "    max_lon = lon + buffer_size\n",
    "    min_lat = lat - buffer_size\n",
    "    max_lat = lat + buffer_size\n",
    "    return (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Input point coordinates and buffer size\n",
    "point = (12.5, 41.9)  # Example point coordinates (lon, lat)\n",
    "buffer_size = 0.0005  # Example buffer size in degrees\n",
    "\n",
    "# Convert point to bounding box\n",
    "bounds = point_to_bounds(point, buffer_size)\n",
    "print(bounds)\n",
    "\n",
    "# Perform clipping with the bounding box\n",
    "elevation.clip(bounds=bounds, output='/home/federico/Documents/fungi/Rome-small.tif') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append elevation and aspect values to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import elevation\n",
    "import rasterio\n",
    "import richdem as rd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#data_path = \"data/spain_positive_ready.csv\"\n",
    "#output_path = \"data/spain_positive_ready_with_el_aspect.csv\"\n",
    "\n",
    "data_path = \"data/negative_samples.csv\"\n",
    "output_path = \"data/negative_samples_el.csv\"\n",
    "\n",
    "\n",
    "def point_to_bounds(point, buffer_size):\n",
    "    \"\"\"\n",
    "    Convert a point to a bounding box with a specified buffer size.\n",
    "    \n",
    "    Args:\n",
    "    - point (tuple): The point coordinates as (lon, lat).\n",
    "    - buffer_size (float): The buffer size in degrees.\n",
    "    \n",
    "    Returns:\n",
    "    - bounds (tuple): The bounding box coordinates as (min_lon, min_lat, max_lon, max_lat).\n",
    "    \"\"\"\n",
    "    lon, lat = point\n",
    "    min_lon = lon - buffer_size\n",
    "    max_lon = lon + buffer_size\n",
    "    min_lat = lat - buffer_size\n",
    "    max_lat = lat + buffer_size\n",
    "    return (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Initialize the 'elevation' and 'aspect' columns with NaN values\n",
    "df['elevation'] = float('nan')\n",
    "df['aspect'] = float('nan')\n",
    "\n",
    "buffer_size = 0.0001\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for idx, row in df.iterrows():\n",
    "    # Extract coordinates\n",
    "    lon, lat = row['x'], row['y']\n",
    "    \n",
    "    # Convert the point to a bounding box with a buffer size\n",
    "    bounds = point_to_bounds((lon, lat), buffer_size)\n",
    "\n",
    "    # Perform clipping with the bounding box and save elevation data to a temporary file\n",
    "    elevation.clip(bounds=bounds, output='/home/federico/Documents/Github/ShroomRadar/temp/elev.tif')\n",
    "    \n",
    "    # Read the clipped elevation data using rasterio\n",
    "    with rasterio.open('/home/federico/Documents/Github/ShroomRadar/temp/elev.tif') as src:\n",
    "        # Read elevation data into an array\n",
    "        clipped_data = src.read(1)\n",
    "        \n",
    "        # Calculate the average elevation\n",
    "        average_elevation = np.nanmean(clipped_data)\n",
    "        \n",
    "        # Convert the elevation array to a richdem Digital Elevation Model (DEM)\n",
    "        dem = rd.rdarray(clipped_data, no_data=np.nan)\n",
    "        dem.projection = src.crs.to_string()\n",
    "        \n",
    "        # Calculate aspect using richdem\n",
    "        aspect_array = rd.TerrainAttribute(dem, attrib='aspect')\n",
    "        \n",
    "        # Calculate the mean aspect for this bounding box\n",
    "        mean_aspect = np.nanmean(aspect_array)\n",
    "    \n",
    "    # Update the DataFrame\n",
    "    df.at[idx, 'elevation'] = average_elevation\n",
    "    df.at[idx, 'aspect'] = mean_aspect\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file after each iteration\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Elevation and aspect calculations completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# windows version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SILENT VERSION: No debug output at all\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import richdem as rd\n",
    "from tqdm import tqdm\n",
    "import srtm\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "# Suppress ALL output including richdem debug prints\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['RICHDEM_QUIET'] = '1'\n",
    "\n",
    "# Context manager to suppress stdout temporarily\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "data_path = \"data/negative_samples.csv\"\n",
    "output_path = \"data/negative_samples_el_aspect.csv\"\n",
    "\n",
    "def get_elevation_and_aspect_silent(elevation_data, lat, lon, buffer_size=0.001):\n",
    "    \"\"\"Completely silent elevation and aspect calculation.\"\"\"\n",
    "    try:\n",
    "        # Get elevation\n",
    "        elevation = elevation_data.get_elevation(lat, lon)\n",
    "        if elevation is None:\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Create small grid for aspect\n",
    "        grid_size = 5\n",
    "        half_buffer = buffer_size / 2\n",
    "        \n",
    "        lats = np.linspace(lat - half_buffer, lat + half_buffer, grid_size)\n",
    "        lons = np.linspace(lon - half_buffer, lon + half_buffer, grid_size)\n",
    "        \n",
    "        elevation_grid = np.full((grid_size, grid_size), np.nan)\n",
    "        \n",
    "        for i, lat_sample in enumerate(lats):\n",
    "            for j, lon_sample in enumerate(lons):\n",
    "                elev = elevation_data.get_elevation(lat_sample, lon_sample)\n",
    "                if elev is not None:\n",
    "                    elevation_grid[i, j] = elev\n",
    "        \n",
    "        # Check if we have enough data for aspect\n",
    "        if np.sum(~np.isnan(elevation_grid)) < 9:\n",
    "            return float(elevation), np.nan\n",
    "        \n",
    "        # Calculate aspect with complete output suppression\n",
    "        with suppress_stdout():\n",
    "            dem = rd.rdarray(elevation_grid, no_data=np.nan)\n",
    "            pixel_size = buffer_size / grid_size\n",
    "            \n",
    "            dem.geotransform = [\n",
    "                lon - half_buffer, pixel_size, 0,\n",
    "                lat + half_buffer, 0, -pixel_size\n",
    "            ]\n",
    "            \n",
    "            aspect_array = rd.TerrainAttribute(dem, attrib='aspect')\n",
    "            aspect = np.nanmean(aspect_array)\n",
    "        \n",
    "        return float(elevation), float(aspect) if not np.isnan(aspect) else np.nan\n",
    "        \n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Initialize columns\n",
    "if 'elevation' not in df.columns:\n",
    "    df['elevation'] = float('nan')\n",
    "if 'aspect' not in df.columns:\n",
    "    df['aspect'] = float('nan')\n",
    "\n",
    "# Find rows to process\n",
    "missing_data = df['elevation'].isna() | df['aspect'].isna()\n",
    "rows_to_process = df[missing_data]\n",
    "\n",
    "print(f\"Processing {len(rows_to_process)} rows...\")\n",
    "\n",
    "# Initialize SRTM once\n",
    "print(\"Initializing SRTM data...\")\n",
    "elevation_data = srtm.get_data()\n",
    "print(\"Starting processing...\")\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 1000\n",
    "total_batches = (len(rows_to_process) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(rows_to_process))\n",
    "    batch_rows = rows_to_process.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Process batch with clean progress bar\n",
    "    progress_bar = tqdm(batch_rows.index, \n",
    "                       desc=f\"Batch {batch_idx + 1}/{total_batches}\", \n",
    "                       leave=True,\n",
    "                       ncols=80)\n",
    "    \n",
    "    for idx in progress_bar:\n",
    "        row = df.loc[idx]\n",
    "        lon, lat = row['x'], row['y']\n",
    "        \n",
    "        elevation, aspect = get_elevation_and_aspect_silent(elevation_data, lat, lon)\n",
    "        \n",
    "        df.at[idx, 'elevation'] = elevation\n",
    "        df.at[idx, 'aspect'] = aspect\n",
    "    \n",
    "    # Save progress\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Simple completion update\n",
    "    completed = min((batch_idx + 1) * batch_size, len(rows_to_process))\n",
    "    print(f\"âœ“ Completed {completed}/{len(rows_to_process)} rows\")\n",
    "\n",
    "# Final summary\n",
    "elevation_count = len(df[~df['elevation'].isna()])\n",
    "aspect_count = len(df[~df['aspect'].isna()])\n",
    "\n",
    "print(f\"\\nðŸŽ‰ All done!\")\n",
    "print(f\"ðŸ“ˆ Elevation: {elevation_count}/{len(df)} points\")\n",
    "print(f\"ðŸ§­ Aspect: {aspect_count}/{len(df)} points\")\n",
    "print(f\"ðŸ’¾ Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geojson input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SILENT VERSION: GeoJSON Polygon Adaptation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import richdem as rd\n",
    "from tqdm import tqdm\n",
    "import srtm\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import contextlib\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Suppress ALL output including richdem debug prints\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['RICHDEM_QUIET'] = '1'\n",
    "\n",
    "# Context manager to suppress stdout temporarily\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# File paths - UPDATE THESE\n",
    "input_geojson_path = \"docker//data//base_maps//basque_country_05.geojson\"\n",
    "output_geojson_path = \"docker//data//base_maps//basque_country_05_with_elevation_aspect.geojson\"\n",
    "\n",
    "def get_elevation_and_aspect_silent(elevation_data, lat, lon, buffer_size=0.001):\n",
    "    \"\"\"Completely silent elevation and aspect calculation.\"\"\"\n",
    "    try:\n",
    "        # Get elevation\n",
    "        elevation = elevation_data.get_elevation(lat, lon)\n",
    "        if elevation is None:\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        # Create small grid for aspect\n",
    "        grid_size = 5\n",
    "        half_buffer = buffer_size / 2\n",
    "        \n",
    "        lats = np.linspace(lat - half_buffer, lat + half_buffer, grid_size)\n",
    "        lons = np.linspace(lon - half_buffer, lon + half_buffer, grid_size)\n",
    "        \n",
    "        elevation_grid = np.full((grid_size, grid_size), np.nan)\n",
    "        \n",
    "        for i, lat_sample in enumerate(lats):\n",
    "            for j, lon_sample in enumerate(lons):\n",
    "                elev = elevation_data.get_elevation(lat_sample, lon_sample)\n",
    "                if elev is not None:\n",
    "                    elevation_grid[i, j] = elev\n",
    "        \n",
    "        # Check if we have enough data for aspect\n",
    "        if np.sum(~np.isnan(elevation_grid)) < 9:\n",
    "            return float(elevation), np.nan\n",
    "        \n",
    "        # Calculate aspect with complete output suppression\n",
    "        with suppress_stdout():\n",
    "            dem = rd.rdarray(elevation_grid, no_data=np.nan)\n",
    "            pixel_size = buffer_size / grid_size\n",
    "            \n",
    "            dem.geotransform = [\n",
    "                lon - half_buffer, pixel_size, 0,\n",
    "                lat + half_buffer, 0, -pixel_size\n",
    "            ]\n",
    "            \n",
    "            aspect_array = rd.TerrainAttribute(dem, attrib='aspect')\n",
    "            aspect = np.nanmean(aspect_array)\n",
    "        \n",
    "        return float(elevation), float(aspect) if not np.isnan(aspect) else np.nan\n",
    "        \n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "# Load GeoJSON data\n",
    "print(\"Loading GeoJSON data...\")\n",
    "gdf = gpd.read_file(input_geojson_path)\n",
    "\n",
    "# Ensure we're working with polygons\n",
    "if not all(gdf.geometry.geom_type.isin(['Polygon', 'MultiPolygon'])):\n",
    "    print(\"Warning: Not all geometries are polygons!\")\n",
    "\n",
    "# Calculate centroids\n",
    "print(\"Calculating polygon centroids...\")\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "\n",
    "# Extract centroid coordinates\n",
    "gdf['centroid_lon'] = gdf['centroid'].x\n",
    "gdf['centroid_lat'] = gdf['centroid'].y\n",
    "\n",
    "# Initialize elevation and aspect columns if they don't exist\n",
    "if 'elevation' not in gdf.columns:\n",
    "    gdf['elevation'] = float('nan')\n",
    "if 'aspect' not in gdf.columns:\n",
    "    gdf['aspect'] = float('nan')\n",
    "\n",
    "# Find rows to process (missing elevation or aspect data)\n",
    "missing_data = gdf['elevation'].isna() | gdf['aspect'].isna()\n",
    "rows_to_process = gdf[missing_data]\n",
    "\n",
    "print(f\"Processing {len(rows_to_process)} polygons...\")\n",
    "\n",
    "# Initialize SRTM once\n",
    "print(\"Initializing SRTM data...\")\n",
    "elevation_data = srtm.get_data()\n",
    "print(\"Starting processing...\")\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 1000\n",
    "total_batches = (len(rows_to_process) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(total_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, len(rows_to_process))\n",
    "    batch_rows = rows_to_process.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Process batch with clean progress bar\n",
    "    progress_bar = tqdm(batch_rows.index, \n",
    "                       desc=f\"Batch {batch_idx + 1}/{total_batches}\", \n",
    "                       leave=True,\n",
    "                       ncols=80)\n",
    "    \n",
    "    for idx in progress_bar:\n",
    "        row = gdf.loc[idx]\n",
    "        lon, lat = row['centroid_lon'], row['centroid_lat']\n",
    "        \n",
    "        elevation, aspect = get_elevation_and_aspect_silent(elevation_data, lat, lon)\n",
    "        \n",
    "        gdf.at[idx, 'elevation'] = elevation\n",
    "        gdf.at[idx, 'aspect'] = aspect\n",
    "    \n",
    "    # Save progress\n",
    "    # Drop the temporary centroid point column before saving\n",
    "    gdf_to_save = gdf.drop(columns=['centroid'])\n",
    "    gdf_to_save.to_file(output_geojson_path, driver='GeoJSON')\n",
    "    \n",
    "    # Simple completion update\n",
    "    completed = min((batch_idx + 1) * batch_size, len(rows_to_process))\n",
    "    print(f\"âœ“ Completed {completed}/{len(rows_to_process)} polygons\")\n",
    "\n",
    "# Final cleanup and summary\n",
    "gdf_final = gdf.drop(columns=['centroid', 'centroid_lon', 'centroid_lat'])\n",
    "gdf_final.to_file(output_geojson_path, driver='GeoJSON')\n",
    "\n",
    "elevation_count = len(gdf_final[~gdf_final['elevation'].isna()])\n",
    "aspect_count = len(gdf_final[~gdf_final['aspect'].isna()])\n",
    "\n",
    "print(f\"\\nðŸŽ‰ All done!\")\n",
    "print(f\"ðŸ“ˆ Elevation: {elevation_count}/{len(gdf_final)} polygons\")\n",
    "print(f\"ðŸ§­ Aspect: {aspect_count}/{len(gdf_final)} polygons\")\n",
    "print(f\"ðŸ’¾ Saved: {output_geojson_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append elevation to geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import elevation \n",
    "\n",
    "def get_mean_elevation(geometry):\n",
    "    # Extract the bounding box coordinates of the polygon\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "\n",
    "    # Clip the elevation data to the extent of the polygon\n",
    "    elevation.clip((minx, miny, maxx, maxy), output='/home/federico/Documents/Github/ShroomRadar/temp/elev.tif')\n",
    "\n",
    "    # Read the clipped elevation data using rasterio\n",
    "    with rasterio.open('/home/federico/Documents/Github/ShroomRadar/temp/elev.tif') as src:\n",
    "        clipped_data = src.read(1)  # Assuming elevation data is stored in the first band\n",
    "\n",
    "    # Calculate the mean elevation\n",
    "    mean_elevation = np.mean(clipped_data)\n",
    "\n",
    "    return mean_elevation\n",
    "\n",
    "\n",
    "# Load the GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('data/siena_with_mode_values.geojson')\n",
    "\n",
    "# Create an empty list to store the mean elevations\n",
    "mean_elevations = []\n",
    "\n",
    "# Iterate over each polygon in the GeoDataFrame\n",
    "for index, row in spain.iterrows():\n",
    "    # Calculate the mean elevation for the current polygon\n",
    "    try:\n",
    "        mean_elevation = get_mean_elevation(row['geometry'])\n",
    "    # Append the mean elevation to the list\n",
    "        mean_elevations.append(mean_elevation)\n",
    "        print(mean_elevations)\n",
    "    except:\n",
    "        mean_elevations.append(np.nan)\n",
    "        print(mean_elevations)\n",
    "\n",
    "# Add the list of mean elevations as a new column in the GeoDataFrame\n",
    "spain['mean_elevation'] = mean_elevations\n",
    "\n",
    "# Save the GeoDataFrame to a new GeoJSON file\n",
    "spain.to_file('siena_ready_05km.geojson', driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import elevation \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_mean_elevation(geometry):\n",
    "    # Extract the bounding box coordinates of the polygon\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "\n",
    "    # Clip the elevation data to the extent of the polygon\n",
    "    elevation.clip((minx, miny, maxx, maxy), output='/home/federico/Documents/Github/ShroomRadar/temp')\n",
    "\n",
    "    # Read the clipped elevation data using rasterio\n",
    "    with rasterio.open('/home/federico/Documents/Github/ShroomRadar/temp') as src:\n",
    "        clipped_data = src.read(1)  # Assuming elevation data is stored in the first band\n",
    "\n",
    "    # Calculate the mean elevation\n",
    "    mean_elevation = np.mean(clipped_data)\n",
    "\n",
    "    return mean_elevation\n",
    "\n",
    "# Load the GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('data/siena_with_mode_values.geojson')\n",
    "\n",
    "# Create an empty list to store the mean elevations\n",
    "mean_elevations = []\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over each polygon in the GeoDataFrame\n",
    "for index, row in tqdm(spain.iterrows(), total=len(spain)):\n",
    "    # Calculate the mean elevation for the current polygon\n",
    "    mean_elevation = get_mean_elevation(row['geometry'])\n",
    "    # Append the mean elevation to the list\n",
    "    mean_elevations.append(mean_elevation)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print total time taken\n",
    "print(\"Total time taken: {:.2f} seconds\".format(end_time - start_time))\n",
    "\n",
    "# Add the list of mean elevations as a new column in the GeoDataFrame\n",
    "spain['mean_elevation'] = mean_elevations\n",
    "\n",
    "# Save the GeoDataFrame to a new GeoJSON file\n",
    "spain.to_file('data/siena_ready_05km.geojson', driver='GeoJSON')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
