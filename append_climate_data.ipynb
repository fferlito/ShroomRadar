{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm   # ✅ added\n",
    "\n",
    "\n",
    "# --- Helpers ---\n",
    "def extract_coordinates(coord_str):\n",
    "    if not coord_str or coord_str == '()':\n",
    "        return None\n",
    "    return tuple(map(float, coord_str.strip(\"()\").split(', ')))\n",
    "\n",
    "\n",
    "def parse_datetime_with_timezone(datetime_str):\n",
    "    \"\"\"Try to parse different date formats; return None if invalid.\"\"\"\n",
    "    if not datetime_str:\n",
    "        return None\n",
    "    datetime_str = datetime_str.split(\" \")[0]\n",
    "    for fmt in (\"%Y-%m-%d\", \"%m/%d/%Y\", \"%d/%m/%Y\"):\n",
    "        try:\n",
    "            return datetime.strptime(datetime_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    print(f\"⚠️ Skipping unparsable date: {datetime_str}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def sample_nc_point(file_path, lon, lat):\n",
    "    \"\"\"Read one NetCDF file and return interpolated value at (lon, lat).\"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        with xr.open_dataset(file_path, engine=\"netcdf4\", decode_cf=True) as ds:\n",
    "            var_candidates = [v for v in ds.data_vars\n",
    "                              if v.lower() not in (\"lat\", \"latitude\", \"lon\", \"longitude\", \"time\")]\n",
    "            if not var_candidates:\n",
    "                return np.nan\n",
    "            vname = var_candidates[0]\n",
    "            da = ds[vname]\n",
    "\n",
    "            # Rename dims if needed\n",
    "            rename_map = {}\n",
    "            if \"latitude\" in da.dims: rename_map[\"latitude\"] = \"lat\"\n",
    "            if \"longitude\" in da.dims: rename_map[\"longitude\"] = \"lon\"\n",
    "            if rename_map:\n",
    "                da = da.rename(rename_map)\n",
    "\n",
    "            if \"time\" in da.dims:\n",
    "                da = da.isel(time=0)\n",
    "\n",
    "            if not ((\"lat\" in da.dims) and (\"lon\" in da.dims)):\n",
    "                return np.nan\n",
    "\n",
    "            # Bilinear interpolation, fallback to nearest\n",
    "            val = da.interp(lat=lat, lon=lon, method=\"linear\").values.item()\n",
    "            if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "                val = da.interp(lat=lat, lon=lon, method=\"nearest\").values.item()\n",
    "\n",
    "            return float(val) if val is not None else np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {file_path}: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_environmental_data(coord, date, base_dir, variable, num_days=14):\n",
    "    \"\"\"Get 14-day timeseries for a variable at coord, with NRT→Past fallback and temporal interpolation.\"\"\"\n",
    "    values = []\n",
    "    for i in range(num_days):\n",
    "        current_date = date - timedelta(days=i)\n",
    "        file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "\n",
    "        nrt_file = os.path.join(base_dir, \"NRT\", variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "        past_file = os.path.join(base_dir, \"Past\", variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "\n",
    "        if os.path.isfile(nrt_file):\n",
    "            data_file = nrt_file\n",
    "        elif os.path.isfile(past_file):\n",
    "            data_file = past_file\n",
    "        else:\n",
    "            values.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        val = sample_nc_point(data_file, coord[0], coord[1])\n",
    "        values.append(val)\n",
    "\n",
    "    # interpolate missing values in time\n",
    "    s = pd.Series(values[::-1])  # oldest→newest\n",
    "    s = s.interpolate(limit_direction=\"both\")\n",
    "    return s[::-1].tolist()  # back to newest→oldest\n",
    "\n",
    "\n",
    "# --- Main pipeline ---\n",
    "\n",
    "# Read the CSV\n",
    "input_csv = \"data/negative_samples_within_land_10k_with_coords.csv\"\n",
    "data = []\n",
    "with open(input_csv, \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if 'location' in row and row['location']:\n",
    "            data.append(row)\n",
    "\n",
    "print(f\"Loaded {len(data)} rows from {input_csv}\")\n",
    "\n",
    "variables = ['P', 'Pres', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "climate_base = \"climate_data\"  # root folder containing NRT/ and Past/\n",
    "\n",
    "for idx, row in enumerate(tqdm(data, desc=\"Processing observations\")):\n",
    "    coords = extract_coordinates(row['location'])\n",
    "    if coords is None:\n",
    "        continue\n",
    "\n",
    "    obs_date = parse_datetime_with_timezone(row['observed_on'])\n",
    "    if obs_date is None:\n",
    "        continue  \n",
    "\n",
    "    for variable in variables:\n",
    "        env_data = get_environmental_data(coords, obs_date, climate_base, variable)\n",
    "        for i, val in enumerate(env_data, start=1):\n",
    "            row[f\"{variable}_{i}\"] = val\n",
    "\n",
    "    if idx == 0:\n",
    "        print(f\"--- Preview row {idx} ---\")\n",
    "        for variable in variables:\n",
    "            print(variable, [row[f\"{variable}_{i}\"] for i in range(1, 6)], \"...\")\n",
    "\n",
    "\n",
    "# Save enriched CSV\n",
    "output_csv = \"data/negative_samples_within_land_10k_with_coords_climate.csv\"\n",
    "fieldnames = list(data[0].keys())\n",
    "with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"✅ Data has been updated and saved to\", output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.dropna(subset=[\"P_1\"]) # enforce required fields present\n",
    "\n",
    "df.to_csv(\"data/inaturalist_boletus_edulis_with_el_aspect_corine_climate_nonan.csv\", index=False)\n",
    "print(\"✅ Saved cleaned CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import rasterio\n",
    "from rasterio import mask\n",
    "\n",
    "# Function to calculate the average value of environmental data within a polygon area\n",
    "def get_average_environmental_data(polygon, date, data_dir, variable):\n",
    "    # Define the number of days to consider for the average (including today)\n",
    "    num_days = 5\n",
    "\n",
    "    # Initialize a list to store the environmental data values\n",
    "    data_values = []\n",
    "\n",
    "    # Iterate over the number of days to consider\n",
    "    for i in range(num_days):\n",
    "        # Calculate the date for the current iteration\n",
    "        current_date = date - timedelta(days=i)\n",
    "\n",
    "        # Construct the filename for the current date\n",
    "        file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "        data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "\n",
    "        # Check if the data file exists\n",
    "        if not os.path.isfile(data_file):\n",
    "            print(f\"File not found for {variable} on {file_date_str}\")\n",
    "            data_values.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Read environmental data from the file\n",
    "        nc = Dataset(data_file, 'r')\n",
    "\n",
    "        # Extract environmental data variable\n",
    "        var = None\n",
    "        for var_name in nc.variables.keys():\n",
    "            if var_name not in ['lon', 'lat', 'time']:\n",
    "                var = nc.variables[var_name]\n",
    "                break\n",
    "\n",
    "        if var is None:\n",
    "            print(f\"No environmental data variable found in {data_file}\")\n",
    "            data_values.append(np.nan)\n",
    "            nc.close()\n",
    "            continue  # Skip this variable and move to the next one\n",
    "\n",
    "        # Extract bounding box of the polygon\n",
    "        min_lon, min_lat, max_lon, max_lat = polygon.bounds\n",
    "\n",
    "        # Read the raster data and clip it to the polygon extent\n",
    "        with rasterio.open(data_file) as src:\n",
    "            out_image, out_transform = mask.mask(src, [polygon], crop=True)\n",
    "            out_image = np.squeeze(out_image)  # Remove singleton dimension\n",
    "\n",
    "            # Calculate the mean value for the current date and append to the list\n",
    "            data_values.append(np.nanmean(out_image))\n",
    "\n",
    "        # Close the netCDF file\n",
    "        nc.close()\n",
    "\n",
    "    # Calculate the average value\n",
    "    average_value = np.nanmean(data_values)\n",
    "\n",
    "    return average_value\n",
    "\n",
    "# Load the GeoJSON file into a GeoDataFrame\n",
    "spain = gpd.read_file('spain_3km_ready.geojson')\n",
    "spain = spain.to_crs('4623')\n",
    "# Define the directory containing the data files\n",
    "data_dir = \"new_data/NRT\"\n",
    "\n",
    "# Define the environmental variables you want to include\n",
    "variables = ['P', 'Pres', 'RelHum', 'SpecHum', 'Temp', 'Tmax', 'Tmin']\n",
    "\n",
    "# Create a list to store the updated data\n",
    "updated_data = []\n",
    "\n",
    "# Define a fixed date for testing\n",
    "test_date = datetime(2024, 5, 3)  # Replace with the desired date\n",
    "\n",
    "# Iterate over each polygon in the GeoDataFrame\n",
    "for index, row in spain.iterrows():\n",
    "    # Iterate over each variable\n",
    "    for variable in variables:\n",
    "        # Calculate the average environmental data for the current polygon, date, and variable\n",
    "        average_data = get_average_environmental_data(row['geometry'], test_date, data_dir, variable)\n",
    "\n",
    "        # Add the average environmental data to the row\n",
    "        row[f'{variable}_avg'] = average_data\n",
    "\n",
    "    # Append the updated row to the list\n",
    "    updated_data.append(row)\n",
    "\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_file = \"today_ready.csv\"\n",
    "\n",
    "# Write the updated data to a new CSV file\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=updated_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(updated_data)\n",
    "\n",
    "print(\"Data has been updated and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# append to geojson values (to be used in pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def sample_nc_point(file_path: str, lon: float, lat: float) -> float:\n",
    "    \"\"\"\n",
    "    Read a single NetCDF file and return the value at (lat, lon),\n",
    "    using xarray's CF decoding and spatial interpolation.\n",
    "\n",
    "    Returns float(np.nan) if the value cannot be obtained.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        return float('nan')\n",
    "\n",
    "    try:\n",
    "        # decode_cf=True applies scale_factor/add_offset and masks _FillValue/missing_value\n",
    "        with xr.open_dataset(file_path, engine=\"netcdf4\", decode_cf=True) as ds:\n",
    "            # pick the first real data variable (skip common coord names)\n",
    "            data_vars = list(ds.data_vars)\n",
    "            var_candidates = [v for v in data_vars\n",
    "                              if v.lower() not in (\"lat\", \"latitude\", \"lon\", \"longitude\", \"time\")]\n",
    "            if not var_candidates:\n",
    "                return float('nan')\n",
    "            vname = var_candidates[0]\n",
    "\n",
    "            da = ds[vname]\n",
    "\n",
    "            # Standardize spatial dim names to ('lat','lon') if needed\n",
    "            rename_map = {}\n",
    "            if \"latitude\" in da.dims: rename_map[\"latitude\"] = \"lat\"\n",
    "            if \"longitude\" in da.dims: rename_map[\"longitude\"] = \"lon\"\n",
    "            if rename_map:\n",
    "                da = da.rename(rename_map)\n",
    "\n",
    "            # If there is a time dimension, use the first (these are daily files)\n",
    "            if \"time\" in da.dims:\n",
    "                da = da.isel(time=0)\n",
    "\n",
    "            # Ensure lat/lon dims exist\n",
    "            if not ((\"lat\" in da.dims) and (\"lon\" in da.dims)):\n",
    "                return float('nan')\n",
    "\n",
    "            # Spatial interpolation (linear) with nearest fallback at edges\n",
    "            val = da.interp(lat=lat, lon=lon, method=\"linear\").values.item()\n",
    "            if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "                val = da.interp(lat=lat, lon=lon, method=\"nearest\").values.item()\n",
    "\n",
    "            # Final guard\n",
    "            if val is None:\n",
    "                return float('nan')\n",
    "            return float(val)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading/interpolating {file_path}: {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "\n",
    "def get_environmental_timeseries(polygon, date, data_dir, variables, num_days):\n",
    "    \"\"\"\n",
    "    For each variable, build a vector of daily values at the polygon centroid\n",
    "    using spatial interpolation (xarray.interp), then temporally interpolate\n",
    "    missing days (pandas), and return today's-first ordering.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    cx, cy = polygon.centroid.x, polygon.centroid.y  # lon, lat\n",
    "\n",
    "    for variable in variables:\n",
    "        values = []\n",
    "        for i in range(num_days):\n",
    "            current_date = date - timedelta(days=i)\n",
    "            file_date_str = current_date.strftime('%Y') + str(current_date.timetuple().tm_yday).zfill(3)\n",
    "            data_file = os.path.join(data_dir, variable, \"Daily\", f\"{file_date_str}.nc\")\n",
    "\n",
    "            val = sample_nc_point(data_file, cx, cy)\n",
    "            values.append(val)\n",
    "\n",
    "        # Temporal interpolation (fill gaps) while keeping Var_1 = \"today\"\n",
    "        s = pd.Series(values[::-1])  # oldest→newest for interpolation\n",
    "        s = s.interpolate(limit_direction=\"both\")\n",
    "        results[variable] = s[::-1].tolist()  # back to newest→oldest (today first)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# === Main script ===\n",
    "spain = gpd.read_file(\"crop.geojson\")\n",
    "# Ensure CRS is WGS84 lon/lat\n",
    "if spain.crs is None:\n",
    "    spain.set_crs(\"EPSG:4326\", inplace=True)\n",
    "else:\n",
    "    spain = spain.to_crs(\"EPSG:4326\")\n",
    "\n",
    "data_dir = \"new_data/NRT\"\n",
    "variables = [\"P\", \"Pres\", \"RelHum\", \"SpecHum\", \"Temp\", \"Tmax\", \"Tmin\"]\n",
    "num_days = 14\n",
    "test_date = datetime(2025, 9, 10)\n",
    "\n",
    "results = []\n",
    "for idx, row in tqdm(spain.iterrows(), total=len(spain), desc=\"Processing polygons\"):\n",
    "    ts_data = get_environmental_timeseries(row[\"geometry\"], test_date, data_dir, variables, num_days)\n",
    "    results.append((row.name, ts_data))\n",
    "\n",
    "# Merge back into GeoDataFrame\n",
    "for idx, ts_data in results:\n",
    "    for variable, values in ts_data.items():\n",
    "        # per-day values (Var_1 = today)\n",
    "        for day_index, value in enumerate(values, start=1):\n",
    "            spain.loc[idx, f\"{variable}_{day_index}\"] = value\n",
    "        # mean across the period\n",
    "        spain.loc[idx, f\"{variable}_mean\"] = float(np.nanmean(values))\n",
    "\n",
    "    # quick preview for first polygon\n",
    "    if idx == 0:\n",
    "        print(f\"--- Preview for polygon {idx} ---\")\n",
    "        for variable, values in ts_data.items():\n",
    "            print(variable, values[:5], \"...\")\n",
    "\n",
    "# Save output\n",
    "output_file = \"crop_new.geojson\"\n",
    "spain.to_file(output_file, driver=\"GeoJSON\")\n",
    "print(\"✅ Data has been updated and saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert nc to tif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Input NetCDF\n",
    "nc_file = \"new_data/NRT/Temp/Daily/2025253.nc\"\n",
    "tif_file = \"temp_2025253.tif\"\n",
    "\n",
    "# Open dataset\n",
    "ds = xr.open_dataset(nc_file)\n",
    "\n",
    "# Pick first data variable\n",
    "var_name = [v for v in ds.data_vars][0]\n",
    "\n",
    "# Select first time step\n",
    "da = ds[var_name].isel(time=0)\n",
    "\n",
    "# Tell rioxarray which dimensions are spatial\n",
    "da = da.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\")\n",
    "\n",
    "# Assign CRS\n",
    "da = da.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "# Export to GeoTIFF\n",
    "da.rio.to_raster(tif_file)\n",
    "\n",
    "print(\"✅ Saved GeoTIFF:\", tif_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
