{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_date_strings():\n",
    "    today = datetime.today()\n",
    "    date_strings = [(today - timedelta(days=i)).strftime(\"%Y%j\") for i in range(15)]\n",
    "    return date_strings\n",
    "\n",
    "def write_structure_to_file(file_path):\n",
    "    year = 2024\n",
    "    date_strings = get_date_strings()\n",
    "    \n",
    "    structure = [\n",
    "        \"+ /NRT/Wind/Daily/\"  + date + \".nc\" for date in date_strings\n",
    "    ]\n",
    "    structure.extend([\n",
    "        \"+ /NRT/P/Daily/\"  + date + \".nc\" for date in date_strings\n",
    "    ])\n",
    "    structure.extend([\n",
    "        \"+ /NRT/Pres/Daily/\"  + date + \".nc\" for date in date_strings\n",
    "    ])\n",
    "    structure.extend([\n",
    "        \"+ /NRT/RelHum/Daily/\"  + date + \".nc\" for date in date_strings\n",
    "    ])\n",
    "    structure.extend([\n",
    "        \"+ /NRT/SpecHum/Daily/\"  + date + \".nc\" for date in date_strings\n",
    "    ])\n",
    "    structure.extend([\n",
    "        \"+ /NRT/Tmin/Daily/\"  + date + \".nc\" for date in date_strings\n",
    "    ])\n",
    "    structure.extend([\n",
    "        \"+ /NRT/Tmax/Daily/\"  + date + \".nc\" for date in date_strings\n",
    "    ])\n",
    "    structure.extend([\n",
    "        \"+ /NRT/Temp/Daily/\"  + date + \".nc\" for date in date_strings\n",
    "    ])\n",
    "    structure.append(\"- *\")\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        for line in structure:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "# Specify the file path where you want to save the structure\n",
    "file_path = \"file_structure_with_14_days.txt\"\n",
    "\n",
    "# Call the function to write the structure to the file\n",
    "write_structure_to_file(file_path)\n",
    "\n",
    "print(f\"File '{file_path}' has been created with the specified structure.\")\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "dest_folder = \"new_data\"\n",
    "\n",
    "def run_rclone_sync(filter_file_path):\n",
    "    # Define the command to run\n",
    "    command = [\n",
    "        \"docker//rclone\", \"sync\", \"-v\", \"--filter-from\", filter_file_path, \"--drive-shared-with-me\",\n",
    "        \"google:/MSWX_V100\", dest_folder\n",
    "    ]\n",
    "    \n",
    "    # Run the command\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(\"rclone sync command executed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Path to the filter file generated before\n",
    "filter_file_path = \"file_structure_with_14_days.txt\"\n",
    "\n",
    "# Call the function to run the rclone sync command\n",
    "run_rclone_sync(filter_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "dest_folder = \"new_data\"\n",
    "\n",
    "def run_rclone_sync(filter_file_path):\n",
    "    # Define the command to run\n",
    "    command = [\n",
    "        \"data/rclone\", \"sync\", \"-v\", \"--filter-from\", filter_file_path, \"--drive-shared-with-me\",\n",
    "        \"google:/MSWX_V100\", dest_folder\n",
    "    ]\n",
    "    \n",
    "    # Run the command\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(\"rclone sync command executed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Path to the filter file generated before\n",
    "filter_file_path = \"file_structure_with_14_days.txt\"\n",
    "\n",
    "# Call the function to run the rclone sync command\n",
    "run_rclone_sync(filter_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_unique_dates_from_csv(csv_file_path):\n",
    "    \"\"\"Extract unique observation dates from the CSV file\"\"\"\n",
    "    unique_dates = set()\n",
    "    \n",
    "    with open(csv_file_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if 'observed_on' in row and row['observed_on']:\n",
    "                # Parse the date part (ignore timezone)\n",
    "                date_str = row['observed_on'].split(\" \")[0]\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "                    unique_dates.add(date_obj)\n",
    "                except ValueError:\n",
    "                    print(f\"Could not parse date: {row['observed_on']}\")\n",
    "                    continue\n",
    "    \n",
    "    return sorted(list(unique_dates))\n",
    "\n",
    "\n",
    "def generate_file_structure_from_csv(csv_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Generate file structure for all dates in CSV with 15 days before each observation,\n",
    "    for both MSWX_V100/PAST and MSWX_V100/NRT.\n",
    "    \"\"\"\n",
    "    unique_dates = get_unique_dates_from_csv(csv_file_path)\n",
    "    print(f\"Found {len(unique_dates)} unique observation dates\")\n",
    "    \n",
    "    all_files = set()\n",
    "    variables = ['Wind', 'P', 'Pres', 'RelHum', 'SpecHum', 'Tmin', 'Tmax', 'Temp']\n",
    "    \n",
    "    for obs_date in unique_dates:\n",
    "        # For each observation date, get 15 days before (including the observation day)\n",
    "        for i in range(15):\n",
    "            target_date = obs_date - timedelta(days=i)\n",
    "            date_string = target_date.strftime(\"%Y%j\")\n",
    "            \n",
    "            for variable in variables:\n",
    "                # Add entries for both NRT and PAST\n",
    "                all_files.add(f\"+ /NRT/{variable}/Daily/{date_string}.nc\")\n",
    "                all_files.add(f\"+ /PAST/{variable}/Daily/{date_string}.nc\")\n",
    "    \n",
    "    # Convert to sorted list\n",
    "    file_list = sorted(list(all_files))\n",
    "    file_list.append(\"- *\")  # Exclude everything else\n",
    "    \n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for line in file_list:\n",
    "            file.write(line + '\\n')\n",
    "    \n",
    "    print(f\"Generated {len(file_list)-1} unique file entries\")\n",
    "    print(f\"File structure saved to: {output_file_path}\")\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "\n",
    "# Test\n",
    "csv_file_path = \"data/negative_samples_within_land_10k_with_coords.csv\"\n",
    "output_file_path = \"climate_files_from_csv_neg.txt\"\n",
    "file_structure = generate_file_structure_from_csv(csv_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_unique_dates_from_csv(csv_file_path, limit_rows=None):\n",
    "    \"\"\"Extract unique observation dates from the CSV file, with optional row limit.\"\"\"\n",
    "    unique_dates = set()\n",
    "\n",
    "    with open(csv_file_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        total_rows = sum(1 for _ in open(csv_file_path)) - 1  # minus header\n",
    "        csvfile.seek(0)\n",
    "\n",
    "        for i, row in enumerate(tqdm(reader, total=limit_rows or total_rows, desc=\"Reading CSV\")):\n",
    "            if limit_rows is not None and i >= limit_rows:\n",
    "                break\n",
    "\n",
    "            if 'observed_on' in row and row['observed_on']:\n",
    "                date_str = row['observed_on'].split(\" \")[0]\n",
    "                try:\n",
    "                    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "                    unique_dates.add(date_obj)\n",
    "                except ValueError:\n",
    "                    print(f\"‚ö†Ô∏è Could not parse date: {row['observed_on']}\")\n",
    "                    continue\n",
    "    return sorted(list(unique_dates))\n",
    "\n",
    "\n",
    "def generate_file_structure_from_csv(csv_file_path, output_file_path, limit_rows=None):\n",
    "    \"\"\"\n",
    "    Generate file structure for all dates in CSV with 15 days before each observation,\n",
    "    for both MSWX_V100/PAST and MSWX_V100/NRT.\n",
    "    \"\"\"\n",
    "    unique_dates = get_unique_dates_from_csv(csv_file_path, limit_rows=limit_rows)\n",
    "    print(f\"üìÖ Found {len(unique_dates)} unique observation dates\")\n",
    "\n",
    "    all_files = set()\n",
    "    variables = ['Wind', 'P', 'Pres', 'RelHum', 'SpecHum', 'Tmin', 'Tmax', 'Temp']\n",
    "\n",
    "    for obs_date in tqdm(unique_dates, desc=\"Building file list\"):\n",
    "        for i in range(15):\n",
    "            target_date = obs_date - timedelta(days=i)\n",
    "            date_string = target_date.strftime(\"%Y%j\")\n",
    "\n",
    "            for variable in variables:\n",
    "                all_files.add(f\"+ /Past/{variable}/Daily/{date_string}.nc\")\n",
    "                all_files.add(f\"+ /NRT/{variable}/Daily/{date_string}.nc\")\n",
    "\n",
    "    file_list = sorted(list(all_files))\n",
    "    file_list.append(\"- *\")  # exclude everything else\n",
    "\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for line in file_list:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "    print(f\"üìù Generated {len(file_list)-1} unique file entries\")\n",
    "    print(f\"‚úÖ File structure saved to: {output_file_path}\")\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def run_rclone_sync_fixed(filter_file_path, dest_folder=\"climate_data\"):\n",
    "    \"\"\"Run rclone sync for both PAST and NRT using one filter file\"\"\"\n",
    "    rclone_path = os.path.join(\"docker\", \"rclone.exe\")\n",
    "\n",
    "    if not os.path.exists(rclone_path):\n",
    "        print(f\"‚ùå Error: rclone not found at {rclone_path}\")\n",
    "        return\n",
    "\n",
    "    command = [\n",
    "        rclone_path, \"sync\", \"-v\",\n",
    "        \"--filter-from\", filter_file_path,\n",
    "        \"--drive-shared-with-me\",\n",
    "        \"google:/MSWX_V100\",  # root folder\n",
    "        dest_folder\n",
    "    ]\n",
    "\n",
    "    print(f\"‚ñ∂Ô∏è Running command: {' '.join(command)}\")\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(\"‚úÖ rclone sync command executed successfully.\")\n",
    "        print(f\"üìÇ Climate data downloaded to: {dest_folder}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error running rclone: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: rclone executable not found.\")\n",
    "\n",
    "\n",
    "def download_climate_data_from_csv_fixed(csv_file_path, dest_folder=\"climate_data\", limit_rows=None):\n",
    "    \"\"\"Complete workflow with filter file + rclone sync\"\"\"\n",
    "    filter_file_path = \"climate_files_from_csv.txt\"\n",
    "\n",
    "    print(\"Step 1: Generating file structure from CSV...\")\n",
    "    generate_file_structure_from_csv(csv_file_path, filter_file_path, limit_rows=limit_rows)\n",
    "\n",
    "    print(\"Step 2: Downloading climate data files...\")\n",
    "    run_rclone_sync_fixed(filter_file_path, dest_folder)\n",
    "\n",
    "    return filter_file_path\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = \"data/negative_samples_within_land_10k_with_coords.csv\"\n",
    "    # Pass limit_rows=N to only scan the first N rows\n",
    "    download_climate_data_from_csv_fixed(csv_file_path, limit_rows=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
